{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-database-helper","title":"Welcome to Database Helper","text":""},{"location":"#blogs-and-websites","title":"Blogs and websites","text":"<ul> <li>w3schools</li> </ul>"},{"location":"#youtube","title":"Youtube","text":""},{"location":"#short-videos","title":"Short Videos","text":"<ul> <li>How do Databases Work? | System Design</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<ul> <li>7 Database Paradigms</li> <li>How To Choose The Right Database?</li> <li>Database Design Tips | Choosing the Best Database in a System Design Interview</li> <li>15 futuristic databases you've never heard of</li> <li>What is Database Sharding?</li> <li>When should you shard your database?</li> <li>Which Database Model to Choose?</li> <li>Secret To Optimizing SQL Queries - Understand The SQL Execution Order</li> <li>The fascinating history of Databases</li> </ul>"},{"location":"#comparisions","title":"Comparisions","text":""},{"location":"#sql-vs-nosql","title":"SQL vs NoSQL","text":"<ul> <li>SQL vs NoSQL or MySQL vs MongoDB</li> <li>Which Is Better? SQL vs NoSQL</li> <li>10. SQL vs NoSQL | Difference between SQL &amp; NoSQL | SQL Vs NoSQL Tutorial | SQL, NoSQL system design</li> </ul>"},{"location":"#database","title":"Database","text":"<ul> <li>Complete DBMS Course</li> <li>SQL Course</li> <li>Databases in Depth</li> <li>Database Programming from scratch</li> <li>DBMS Placements Series</li> <li>Database Tutorials</li> <li>Database in kubernetes</li> <li>Database Engineering</li> <li>Relational database (RDBMS) by Decomplexify</li> </ul>"},{"location":"#sql","title":"SQL","text":"<ul> <li>SQL Tutorial - Full Database Course for Beginners</li> </ul>"},{"location":"#postgresql","title":"PostgreSQL","text":""},{"location":"#nosql","title":"NOSQL","text":"<ul> <li>How do NoSQL databases work? Simply Explained!</li> <li>The Secret Sauce Behind NoSQL: LSM Tree</li> <li>Cassandra vs MongoDB vs HBase | Difference Between Popular NoSQL Databases | Edureka</li> </ul>"},{"location":"#graph-database","title":"Graph database","text":"<ul> <li>Graph Databases Will Change Your Freakin' Life (Best Intro Into Graph Databases)</li> </ul>"},{"location":"#vector-database","title":"Vector database","text":"<ul> <li>The Power of Vector Databases For Knowledge Search</li> <li>Vector databases are so hot right now. WTF are they?</li> </ul>"},{"location":"#dbmsiit","title":"DBMS(IIT)","text":"<ul> <li>Data Base Management System | IIT-KGP</li> <li>Database Management Systems | IIT-MADRAS</li> </ul>"},{"location":"#udemy","title":"Udemy","text":""},{"location":"#introduction_1","title":"Introduction","text":"<ul> <li>Cloud Computing for Beginners - Database Technologies</li> <li>Relational Database Design</li> </ul>"},{"location":"#dbms","title":"DBMS","text":"<ul> <li>Fundamentals of Database Engineering</li> <li>Database Management System from scratch in parts<ul> <li>Database Management System from scratch - Part 1</li> <li>Database Management System from scratch - Part 2</li> <li>Database Management Systems Part 3 : SQL Interview Course</li> <li>Database Management Systems Part 4 : Transactions</li> <li>Database Management Final Part (5): Indexing,B Trees,B+Trees</li> </ul> </li> <li>Complete SQL and Databases Bootcamp</li> </ul>"},{"location":"#blogs","title":"Blogs","text":""},{"location":"#introduction_2","title":"Introduction","text":"<ul> <li>Strong Consistency vs Eventual Consistency</li> <li>6 Reasons Why PostgreSQL is Not So Popular, Yet!</li> <li>Why MongoDB is Still Popular?</li> <li>LSM Trees: the Go-To Data Structure for Databases, Search Engines, and More</li> <li>Optimistic Locking vs Pessimistic Locking: Managing Concurrent Access</li> <li>Redis Pub-Sub or Kafka: Choosing the Right Tool for Your Use Case</li> <li>Understand the basic Kafka architecture before you go crazy with it!</li> <li>The Simplified Introduction of Vector Databases</li> <li>SSTables and LSM Trees</li> <li>B-Trees</li> <li>Concurrency Challenges in Database Transactions: Isolation Levels and Locking Mechanisms</li> <li>Hash Indexing</li> <li>Leaderless Replication In Distributed System</li> <li>Understanding Database Partitioning in Distributed Systems : Rebalancing Partitions</li> <li>Introduction To Database Indexing</li> <li>Design Metrics Aggregation System | LSM Tree | Storage Engine</li> </ul>"},{"location":"#sql_1","title":"SQL","text":"<ul> <li>SQL Fundamentals<ul> <li>20 Advanced SQL Techniques</li> <li>9 Advanced SQL Queries for Data Mastery</li> <li>Top 10 Advanced SQL Queries</li> </ul> </li> <li>Learning SQL<ul> <li>Writing SQL Like a Pro: Advanced Techniques Showcased in a Real-Life Scenario</li> <li>12 Tips for Optimizing SQL Queries for Faster Performance</li> </ul> </li> <li>Dev Genius<ul> <li>9 SQL Mistakes to Avoid for Effective Queries </li> </ul> </li> </ul>"},{"location":"cassandra/links/","title":"Apache Cassandra","text":""},{"location":"cassandra/links/#apache-cassandra","title":"Apache Cassandra","text":""},{"location":"cassandra/links/#udemy","title":"Udemy","text":"<ul> <li>Learn Apache Cassandra in 2 hours | NoSQL Database</li> <li>Learn Apache Cassandra from Scratch</li> <li>Apache Cassandra : Everything You Need To Know</li> </ul>"},{"location":"elastic-search/links/","title":"Elastic search","text":""},{"location":"elastic-search/links/#elastic-search","title":"Elastic search","text":""},{"location":"elastic-search/links/#introduction","title":"Introduction","text":"<ul> <li>How Elastic Search Work ? System Design Deep Dives Podcast</li> </ul>"},{"location":"kafka/deploy-on-docker/","title":"Kafka Cluster Docker Compose Documentation","text":""},{"location":"kafka/deploy-on-docker/#kafka-cluster-docker-compose-documentation","title":"Kafka Cluster Docker Compose Documentation","text":"<p>This document provides a comprehensive explanation of the Kafka cluster deployment using Docker Compose. The setup includes a 3-node Kafka cluster running in KRaft mode (without Zookeeper), along with supporting services for monitoring, management, and data integration.</p>"},{"location":"kafka/deploy-on-docker/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Architecture Overview</li> <li>Networks</li> <li>Volumes</li> <li>Services</li> <li>init-kafka</li> <li>kafka1, kafka2, kafka3</li> <li>busybox</li> <li>kafka-init-topics</li> <li>kafka-ui</li> <li>schema-registry</li> <li>kafka-connect</li> <li>prometheus</li> <li>filebrowser</li> <li>Testing the Cluster</li> </ul>"},{"location":"kafka/deploy-on-docker/#architecture-overview","title":"Architecture Overview","text":"<p>This setup creates a production-ready Kafka cluster with the following components: - 3-node Kafka cluster in KRaft mode (Kafka without Zookeeper) - Schema Registry for managing Avro/JSON schemas - Kafka Connect for data integration - Kafka UI for cluster management and monitoring - Prometheus for metrics collection - FileBrowser for browsing cluster data and logs</p>"},{"location":"kafka/deploy-on-docker/#networks","title":"Networks","text":""},{"location":"kafka/deploy-on-docker/#kafka-net","title":"kafka-net","text":"<pre><code>networks:\n  kafka-net:\n    driver: bridge\n</code></pre> <p>Purpose: Creates a custom bridge network for all Kafka-related services to communicate with each other.</p> <p>Why it's used:  - Provides network isolation for the Kafka cluster - Enables service discovery using container names as hostnames - Allows containers to communicate using internal DNS resolution</p>"},{"location":"kafka/deploy-on-docker/#volumes","title":"Volumes","text":""},{"location":"kafka/deploy-on-docker/#kafka-cluster","title":"kafka-cluster","text":"<pre><code>volumes:\n  kafka-cluster:\n</code></pre> <p>Purpose: A named Docker volume that persists Kafka data, logs, and shared files across container restarts.</p> <p>Why it's used: - Ensures data persistence even if containers are removed - Shared across all Kafka brokers and utility containers - Stores topic data, broker logs, and application logs</p> <p>Directory Structure: </p><pre><code>/mnt/shared/\n\u251c\u2500\u2500 kafka1/\n\u2502   \u251c\u2500\u2500 data/  (topic partitions and logs)\n\u2502   \u2514\u2500\u2500 logs/  (application logs)\n\u251c\u2500\u2500 kafka2/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 logs/\n\u2514\u2500\u2500 kafka3/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 logs/\n</code></pre>"},{"location":"kafka/deploy-on-docker/#services","title":"Services","text":""},{"location":"kafka/deploy-on-docker/#init-kafka","title":"init-kafka","text":"<pre><code>init-kafka:\n  image: alpine:latest\n  container_name: init-kafka\n</code></pre> <p>What it is: A one-time initialization service that prepares the filesystem structure before Kafka brokers start.</p> <p>Image: <code>alpine:latest</code> - Lightweight Linux distribution (only ~5MB)</p> <p>Command: </p><pre><code>sh -c \"\n  echo 'Creating directory structure...';\n  mkdir -p /mnt/shared/kafka1/data /mnt/shared/kafka1/logs;\n  mkdir -p /mnt/shared/kafka2/data /mnt/shared/kafka2/logs;\n  mkdir -p /mnt/shared/kafka3/data /mnt/shared/kafka3/logs;\n  echo 'Setting permissions...';\n  chmod -R 777 /mnt/shared;\n  echo 'Initialization complete.';\n\"\n</code></pre> <p>Purpose: - Creates required directory structure for each Kafka broker - Sets proper permissions (777) to avoid permission issues - Runs once before Kafka brokers start (via <code>depends_on</code>)</p> <p>Volumes: - <code>kafka-cluster:/mnt/shared</code> - Mounts the shared volume to create directories</p> <p>Network: <code>kafka-net</code></p> <p>Why it's needed: Kafka brokers need pre-existing directories with proper permissions to store data and logs. This service ensures these directories exist before brokers attempt to write to them.</p>"},{"location":"kafka/deploy-on-docker/#kafka1-kafka2-kafka3","title":"kafka1, kafka2, kafka3","text":"<p>What they are: Three Kafka broker nodes forming a high-availability cluster using KRaft mode (Kafka's built-in consensus protocol, replacing Zookeeper).</p> <p>Image: <code>confluentinc/cp-kafka:7.8.0</code> - Confluent's Kafka distribution version 7.8.0</p>"},{"location":"kafka/deploy-on-docker/#kafka1-configuration","title":"kafka1 Configuration","text":"<p>Ports: - <code>9092:9092</code> - Client connections (producers/consumers connect here) - <code>9093:9093</code> - Controller communication (KRaft consensus protocol) - <code>9997:9997</code> - JMX metrics port for monitoring</p> <p>Environment Variables:</p>"},{"location":"kafka/deploy-on-docker/#node-identity","title":"Node Identity","text":"<ul> <li>KAFKA_NODE_ID: <code>1</code></li> <li>Unique identifier for this node in the KRaft cluster</li> <li> <p>Must be unique across all brokers</p> </li> <li> <p>KAFKA_BROKER_ID: <code>1</code></p> </li> <li>Legacy broker ID (kept for compatibility)</li> <li> <p>Should match NODE_ID</p> </li> <li> <p>CLUSTER_ID: <code>EmptNWtoR4GGWx-BH6nGLQ</code></p> </li> <li>Unique cluster identifier</li> <li>Critical: Must be identical across all brokers in the cluster</li> <li>Generated once and reused for cluster formation</li> </ul>"},{"location":"kafka/deploy-on-docker/#process-roles","title":"Process Roles","text":"<ul> <li>KAFKA_PROCESS_ROLES: <code>broker,controller</code></li> <li>Defines what roles this node plays</li> <li><code>broker</code>: Handles client requests and stores data</li> <li><code>controller</code>: Participates in cluster metadata management (KRaft mode)</li> <li>Combined mode means each node is both a broker and controller</li> </ul>"},{"location":"kafka/deploy-on-docker/#kraft-quorum-configuration","title":"KRaft Quorum Configuration","text":"<ul> <li>KAFKA_CONTROLLER_QUORUM_VOTERS: <code>1@kafka1:9093,2@kafka2:9093,3@kafka3:9093</code></li> <li>Lists all controller nodes in the cluster</li> <li>Format: <code>{node-id}@{hostname}:{controller-port}</code></li> <li>Used for leader election and metadata replication</li> <li>All 3 nodes participate in the quorum for fault tolerance</li> </ul>"},{"location":"kafka/deploy-on-docker/#listeners-configuration","title":"Listeners Configuration","text":"<ul> <li>KAFKA_LISTENERS: <code>PLAINTEXT://kafka1:9092,CONTROLLER://kafka1:9093</code></li> <li>Defines the addresses Kafka binds to</li> <li><code>PLAINTEXT://kafka1:9092</code>: Listens for client connections</li> <li> <p><code>CONTROLLER://kafka1:9093</code>: Listens for controller-to-controller communication</p> </li> <li> <p>KAFKA_ADVERTISED_LISTENERS: <code>PLAINTEXT://kafka1:9092</code></p> </li> <li>The address Kafka advertises to clients</li> <li>Clients use this to connect to the broker</li> <li> <p>Only advertises the client listener (not the controller listener)</p> </li> <li> <p>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: <code>CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT</code></p> </li> <li>Maps listener names to security protocols</li> <li><code>CONTROLLER:PLAINTEXT</code>: Controller communication uses plaintext (no encryption)</li> <li> <p><code>PLAINTEXT:PLAINTEXT</code>: Client communication uses plaintext</p> </li> <li> <p>KAFKA_CONTROLLER_LISTENER_NAMES: <code>CONTROLLER</code></p> </li> <li>Specifies which listener is used for controller communication</li> <li> <p>Must match one of the listener names in KAFKA_LISTENERS</p> </li> <li> <p>KAFKA_INTER_BROKER_LISTENER_NAME: <code>PLAINTEXT</code></p> </li> <li>Specifies which listener brokers use to communicate with each other</li> <li>Used for replication and cluster coordination</li> </ul>"},{"location":"kafka/deploy-on-docker/#replication-configuration","title":"Replication Configuration","text":"<ul> <li>KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: <code>3</code></li> <li>Replication factor for the <code>__consumer_offsets</code> topic</li> <li>Set to 3 for high availability (one copy on each broker)</li> <li> <p>This topic stores consumer group offsets</p> </li> <li> <p>KAFKA_DEFAULT_REPLICATION_FACTOR: <code>3</code></p> </li> <li>Default replication factor for new topics (if not specified)</li> <li> <p>Ensures all data is replicated across all 3 brokers</p> </li> <li> <p>KAFKA_MIN_INSYNC_REPLICAS: <code>2</code></p> </li> <li>Minimum number of replicas that must acknowledge a write</li> <li>Set to 2 for a balance between availability and durability</li> <li>With 3 replicas, can tolerate 1 broker failure while maintaining writes</li> </ul>"},{"location":"kafka/deploy-on-docker/#consumer-group-configuration","title":"Consumer Group Configuration","text":"<ul> <li>KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: <code>0</code></li> <li>Delay before first consumer group rebalance</li> <li>Set to 0 for development (no delay)</li> <li>In production, typically set higher (e.g., 3000ms) to allow all consumers to join</li> </ul>"},{"location":"kafka/deploy-on-docker/#transaction-configuration","title":"Transaction Configuration","text":"<ul> <li>KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: <code>1</code></li> <li>Minimum in-sync replicas for transaction log</li> <li> <p>Set to 1 for development (allows transactions even with broker failures)</p> </li> <li> <p>KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: <code>1</code></p> </li> <li>Replication factor for transaction state log</li> <li>Set to 1 for development (typically 3 in production)</li> </ul>"},{"location":"kafka/deploy-on-docker/#jmx-monitoring-configuration","title":"JMX Monitoring Configuration","text":"<ul> <li>KAFKA_JMX_PORT: <code>9997</code></li> <li>Port for JMX (Java Management Extensions) metrics</li> <li> <p>Used by monitoring tools like Prometheus to scrape metrics</p> </li> <li> <p>KAFKA_JMX_OPTS: Complex JVM options string   </p><pre><code>-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.authenticate=false \n-Dcom.sun.management.jmxremote.ssl=false \n-Djava.rmi.server.hostname=kafka1 \n-Dcom.sun.management.jmxremote.rmi.port=9997\n</code></pre> </li> <li>Enables JMX remote monitoring</li> <li>Disables authentication and SSL (for development only)</li> <li>Sets RMI hostname to <code>kafka1</code> for proper connectivity</li> <li>Binds RMI to port 9997</li> </ul>"},{"location":"kafka/deploy-on-docker/#storage-configuration","title":"Storage Configuration","text":"<ul> <li>KAFKA_LOG_DIRS: <code>/mnt/shared/kafka1/data</code></li> <li>Directory where Kafka stores topic data (log segments)</li> <li>Each partition is stored as a subdirectory</li> <li> <p>Critical for data persistence</p> </li> <li> <p>LOG_DIR: <code>/mnt/shared/kafka1/logs</code></p> </li> <li>Directory for application logs (server.log, gc.log, etc.)</li> <li>Separate from data storage for better organization</li> </ul> <p>Volumes: - <code>kafka-cluster:/mnt/shared</code> - Mounts shared volume for data and logs</p> <p>Networks: <code>kafka-net</code></p> <p>Dependencies: <code>init-kafka</code> - Waits for directory initialization</p>"},{"location":"kafka/deploy-on-docker/#kafka2-configuration","title":"kafka2 Configuration","text":"<p>Identical to kafka1 with the following differences:</p> <p>Ports: - <code>9094:9092</code> - Client connections (mapped to host port 9094) - <code>9095:9093</code> - Controller communication (mapped to host port 9095)</p> <p>Node-specific Environment Variables: - <code>KAFKA_NODE_ID: 2</code> - <code>KAFKA_BROKER_ID: 2</code> - <code>KAFKA_LISTENERS: PLAINTEXT://kafka2:9092,CONTROLLER://kafka2:9093</code> - <code>KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9092</code> - <code>KAFKA_JMX_OPTS</code>: Uses <code>kafka2</code> as hostname - <code>KAFKA_LOG_DIRS: /mnt/shared/kafka2/data</code> - <code>LOG_DIR: /mnt/shared/kafka2/logs</code></p>"},{"location":"kafka/deploy-on-docker/#kafka3-configuration","title":"kafka3 Configuration","text":"<p>Identical to kafka1 with the following differences:</p> <p>Ports: - <code>9096:9092</code> - Client connections (mapped to host port 9096) - <code>9097:9093</code> - Controller communication (mapped to host port 9097)</p> <p>Node-specific Environment Variables: - <code>KAFKA_NODE_ID: 3</code> - <code>KAFKA_BROKER_ID: 3</code> - <code>KAFKA_LISTENERS: PLAINTEXT://kafka3:9092,CONTROLLER://kafka3:9093</code> - <code>KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9092</code> - <code>KAFKA_JMX_OPTS</code>: Uses <code>kafka3</code> as hostname - <code>KAFKA_LOG_DIRS: /mnt/shared/kafka3/data</code> - <code>LOG_DIR: /mnt/shared/kafka3/logs</code></p> <p>Why 3 brokers?: - Provides high availability and fault tolerance - Allows replication factor of 3 - Can survive 1 broker failure while maintaining data availability - Enables proper quorum-based leader election in KRaft mode</p>"},{"location":"kafka/deploy-on-docker/#busybox","title":"busybox","text":"<pre><code>busybox:\n  image: confluentinc/cp-kafka:7.8.0\n  container_name: busybox\n  hostname: busybox\n  command: /scripts/init-script.sh\n</code></pre> <p>What it is: A utility container that runs custom initialization scripts after the Kafka cluster is ready.</p> <p>Image: <code>confluentinc/cp-kafka:7.8.0</code> - Uses the same Kafka image to have access to Kafka CLI tools</p> <p>Command: <code>/scripts/init-script.sh</code> - Executes a custom shell script</p> <p>Environment Variables: - TZ: <code>America/New_York</code>   - Sets the timezone for the container   - Useful for consistent timestamp logging</p> <ul> <li>KAFKA_BROKER_1: <code>kafka1:9092</code></li> <li>KAFKA_BROKER_2: <code>kafka2:9092</code></li> <li>KAFKA_BROKER_3: <code>kafka3:9092</code></li> <li>Environment variables for easy access to broker addresses</li> <li>Can be used in scripts without hardcoding broker URLs</li> </ul> <p>Volumes: - <code>kafka-cluster:/mnt/shared</code> - Access to shared volume - <code>./scripts:/scripts</code> - Mounts local scripts directory into container</p> <p>Working Directory: <code>/scripts</code> - Scripts are executed from this directory</p> <p>Dependencies: Depends on all three Kafka brokers being up</p> <p>Network: <code>kafka-net</code></p> <p>Why it's used:  - Runs custom initialization tasks after cluster is ready - Can create topics, configure ACLs, or perform health checks - Provides a container with Kafka tools for debugging</p>"},{"location":"kafka/deploy-on-docker/#kafka-init-topics","title":"kafka-init-topics","text":"<pre><code>kafka-init-topics:\n  image: confluentinc/cp-kafka:7.8.0\n  container_name: kafka-init-topics\n</code></pre> <p>What it is: An initialization container that creates topics and loads sample data when the cluster starts.</p> <p>Image: <code>confluentinc/cp-kafka:7.8.0</code> - Contains Kafka command-line tools</p> <p>Volumes: - <code>./data/message.json:/data/message.json</code> - Mounts sample data file</p> <p>Command Breakdown: </p><pre><code>sh -c 'echo Waiting for Kafka to be ready... &amp;&amp; \\\n       cub kafka-ready -b kafka1:9092 1 30 &amp;&amp; \\\n       kafka-topics --create --topic users --partitions 3 --replication-factor 3 --if-not-exists --bootstrap-server kafka1:9092 &amp;&amp; \\\n       kafka-topics --create --topic messages --partitions 3 --replication-factor 3 --if-not-exists --bootstrap-server kafka1:9092 &amp;&amp; \\\n       kafka-console-producer --bootstrap-server kafka1:9092 --topic users &lt; /data/message.json'\n</code></pre> <p>Command Steps: 1. Wait for Kafka: <code>cub kafka-ready -b kafka1:9092 1 30</code>    - <code>cub</code> (Confluent Utility Belt) checks if Kafka is ready    - <code>-b kafka1:9092</code>: Bootstrap server to check    - <code>1</code>: Minimum number of brokers required    - <code>30</code>: Timeout in seconds</p> <ol> <li>Create 'users' topic:     <pre><code>kafka-topics --create --topic users --partitions 3 --replication-factor 3 --if-not-exists --bootstrap-server kafka1:9092\n</code></pre></li> <li>Creates a topic named <code>users</code></li> <li><code>--partitions 3</code>: Splits topic into 3 partitions for parallel processing</li> <li><code>--replication-factor 3</code>: Replicates each partition to all 3 brokers</li> <li> <p><code>--if-not-exists</code>: Prevents errors if topic already exists</p> </li> <li> <p>Create 'messages' topic: Same as above for <code>messages</code> topic</p> </li> <li> <p>Load sample data:    </p><pre><code>kafka-console-producer --bootstrap-server kafka1:9092 --topic users &lt; /data/message.json\n</code></pre> </li> <li>Produces messages from <code>message.json</code> to the <code>users</code> topic</li> <li>Uses stdin redirection to read from file</li> </ol> <p>Dependencies: All three Kafka brokers must be running</p> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Automates topic creation on cluster startup - Pre-loads sample data for testing - Ensures topics have proper partitioning and replication</p>"},{"location":"kafka/deploy-on-docker/#kafka-ui","title":"kafka-ui","text":"<pre><code>kafka-ui:\n  image: ghcr.io/kafbat/kafka-ui:latest\n  container_name: kafka-ui\n</code></pre> <p>What it is: A modern web-based UI for managing and monitoring Kafka clusters (formerly known as kafka-ui by Provectus).</p> <p>Image: <code>ghcr.io/kafbat/kafka-ui:latest</code> - Latest version of Kafbat UI</p> <p>Ports: - <code>8080:8080</code> - Web interface accessible at http://localhost:8080</p> <p>Environment Variables:</p>"},{"location":"kafka/deploy-on-docker/#cluster-configuration","title":"Cluster Configuration","text":"<ul> <li>KAFKA_CLUSTERS_0_NAME: <code>local</code></li> <li>Display name for the cluster in the UI</li> <li> <p>The <code>0</code> indicates this is the first (and only) cluster</p> </li> <li> <p>KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: <code>kafka1:9092,kafka2:9092,kafka3:9092</code></p> </li> <li>Comma-separated list of broker addresses</li> <li>UI connects to these brokers to retrieve cluster information</li> </ul>"},{"location":"kafka/deploy-on-docker/#metrics-configuration","title":"Metrics Configuration","text":"<ul> <li>KAFKA_CLUSTERS_0_METRICS_PORT: <code>9997</code></li> <li>JMX port to scrape metrics from brokers</li> <li> <p>Enables performance monitoring in the UI</p> </li> <li> <p>KAFKA_CLUSTERS_0_METRICS_STORE_PROMETHEUS_URL: <code>http://prometheus:9090</code></p> </li> <li>URL of Prometheus server for historical metrics</li> <li> <p>Allows viewing metrics trends over time</p> </li> <li> <p>KAFKA_CLUSTERS_0_METRICS_STORE_PROMETHEUS_REMOTEWRITE: <code>true</code></p> </li> <li>Enables remote write to Prometheus</li> <li> <p>Pushes metrics to Prometheus for storage</p> </li> <li> <p>KAFKA_CLUSTERS_0_METRICS_STORE_KAFKA_TOPIC: <code>kafka_metrics</code></p> </li> <li>Kafka topic for storing metrics data</li> <li>Allows metrics to be persisted in Kafka itself</li> </ul>"},{"location":"kafka/deploy-on-docker/#kafka-connect-integration","title":"Kafka Connect Integration","text":"<ul> <li>KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: <code>http://kafka-connect:8083</code></li> <li>URL of Kafka Connect cluster</li> <li> <p>Enables managing connectors from the UI</p> </li> <li> <p>KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: <code>first</code></p> </li> <li>Display name for this Connect cluster</li> </ul>"},{"location":"kafka/deploy-on-docker/#schema-registry-integration","title":"Schema Registry Integration","text":"<ul> <li>KAFKA_CLUSTERS_0_SCHEMAREGISTRY: <code>http://schema-registry:8085</code></li> <li>URL of Schema Registry service</li> <li>Allows viewing and managing schemas from the UI</li> </ul>"},{"location":"kafka/deploy-on-docker/#ui-features","title":"UI Features","text":"<ul> <li>DYNAMIC_CONFIG_ENABLED: <code>true</code></li> <li>Allows modifying cluster configuration through the UI</li> <li>Enables dynamic updates without restarting</li> </ul> <p>Dependencies:  - Kafka brokers (kafka1, kafka2, kafka3) - schema-registry - kafka-connect</p> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Provides a user-friendly interface for cluster management - Monitors topics, partitions, consumer groups, and brokers - Manages schemas and connectors - Visualizes metrics and cluster health - Allows producing/consuming messages for testing</p>"},{"location":"kafka/deploy-on-docker/#schema-registry","title":"schema-registry","text":"<pre><code>schema-registry:\n  image: confluentinc/cp-schema-registry:7.8.0\n  container_name: schema-registry\n  hostname: schema-registry\n</code></pre> <p>What it is: A centralized service for managing and enforcing schemas for Kafka messages (Avro, JSON Schema, Protobuf).</p> <p>Image: <code>confluentinc/cp-schema-registry:7.8.0</code> - Confluent's Schema Registry</p> <p>Ports: - <code>8081:8081</code> - Primary REST API endpoint - <code>8085:8085</code> - Secondary listener (used by Kafka UI)</p> <p>Environment Variables:</p>"},{"location":"kafka/deploy-on-docker/#basic-configuration","title":"Basic Configuration","text":"<ul> <li>SCHEMA_REGISTRY_HOST_NAME: <code>schema-registry</code></li> <li>Hostname for this Schema Registry instance</li> <li> <p>Used for service identification</p> </li> <li> <p>SCHEMA_REGISTRY_LISTENERS: <code>http://schema-registry:8081,http://schema-registry:8085</code></p> </li> <li>Comma-separated list of HTTP endpoints to listen on</li> <li>Multiple listeners allow different services to connect on different ports</li> </ul>"},{"location":"kafka/deploy-on-docker/#kafka-connection","title":"Kafka Connection","text":"<ul> <li>SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: <code>PLAINTEXT://kafka1:9092,PLAINTEXT://kafka2:9092,PLAINTEXT://kafka3:9092</code></li> <li>Kafka brokers where schemas are stored</li> <li>Schema Registry uses Kafka as its storage backend</li> <li> <p>Schemas are stored in the <code>_schemas</code> topic</p> </li> <li> <p>SCHEMA_REGISTRY_KAFKASTORE_TOPIC: <code>_schemas</code></p> </li> <li>Kafka topic where schemas are persisted</li> <li>Single-partition topic for consistency</li> </ul>"},{"location":"kafka/deploy-on-docker/#security-configuration","title":"Security Configuration","text":"<ul> <li>SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: <code>PLAINTEXT</code></li> <li>Security protocol for connecting to Kafka</li> <li> <p><code>PLAINTEXT</code> means no encryption (for development)</p> </li> <li> <p>SCHEMA_REGISTRY_SCHEMA_REGISTRY_INTER_INSTANCE_PROTOCOL: <code>http</code></p> </li> <li>Protocol for communication between Schema Registry instances</li> <li>Would use <code>https</code> in production for security</li> </ul>"},{"location":"kafka/deploy-on-docker/#logging","title":"Logging","text":"<ul> <li>SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: <code>INFO</code></li> <li>Sets the logging level for Schema Registry</li> <li><code>INFO</code> provides standard operational logs</li> <li> <p>Other options: <code>DEBUG</code>, <code>WARN</code>, <code>ERROR</code></p> </li> <li> <p>SCHEMA_REGISTRY_DEBUG: <code>true</code></p> </li> <li>Enables debug mode for more verbose logging</li> <li>Useful for troubleshooting</li> </ul> <p>Dependencies: All three Kafka brokers</p> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Ensures data consistency by enforcing schemas - Provides schema evolution and compatibility checking - Enables versioning of data schemas - Reduces message size by storing schemas separately - Prevents incompatible data from being produced - Central registry for all data contracts</p> <p>How it works: 1. Producers register schemas before sending data 2. Schema Registry validates compatibility with existing versions 3. Consumers retrieve schemas to deserialize messages 4. All schemas are versioned and stored in Kafka's <code>_schemas</code> topic</p>"},{"location":"kafka/deploy-on-docker/#kafka-connect","title":"kafka-connect","text":"<pre><code>kafka-connect:\n  image: confluentinc/cp-kafka-connect:7.8.0\n  container_name: kafka-connect\n  hostname: kafka-connect\n</code></pre> <p>What it is: A distributed framework for streaming data between Kafka and external systems (databases, file systems, cloud services, etc.).</p> <p>Image: <code>confluentinc/cp-kafka-connect:7.8.0</code> - Confluent's Kafka Connect</p> <p>Ports: - <code>8083:8083</code> - REST API for managing connectors</p> <p>Environment Variables:</p>"},{"location":"kafka/deploy-on-docker/#bootstrap-configuration","title":"Bootstrap Configuration","text":"<ul> <li>CONNECT_BOOTSTRAP_SERVERS: <code>kafka1:9092,kafka2:9092,kafka3:9092</code></li> <li>Kafka brokers to connect to</li> <li>Connect uses Kafka to coordinate distributed work and store configurations</li> </ul>"},{"location":"kafka/deploy-on-docker/#worker-configuration","title":"Worker Configuration","text":"<ul> <li>CONNECT_GROUP_ID: <code>compose-connect-group</code></li> <li>Unique identifier for this Connect cluster</li> <li>Multiple Connect workers with the same group ID form a cluster</li> <li>Enables distributed processing and fault tolerance</li> </ul>"},{"location":"kafka/deploy-on-docker/#internal-topics-configuration","title":"Internal Topics Configuration","text":"<p>Kafka Connect uses three internal topics to coordinate work and store state:</p> <ol> <li>Config Storage:</li> <li>CONNECT_CONFIG_STORAGE_TOPIC: <code>_connect_configs</code><ul> <li>Stores connector and task configurations</li> </ul> </li> <li> <p>CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: <code>3</code></p> <ul> <li>Replicates configs across all 3 brokers for high availability</li> </ul> </li> <li> <p>Offset Storage:</p> </li> <li>CONNECT_OFFSET_STORAGE_TOPIC: <code>_connect_offset</code><ul> <li>Stores source connector offsets (position in source data)</li> </ul> </li> <li> <p>CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: <code>3</code></p> <ul> <li>Ensures offset data is not lost if a broker fails</li> </ul> </li> <li> <p>Status Storage:</p> </li> <li>CONNECT_STATUS_STORAGE_TOPIC: <code>_connect_status</code><ul> <li>Stores connector and task status information</li> </ul> </li> <li>CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: <code>3</code><ul> <li>Maintains status information across broker failures</li> </ul> </li> </ol>"},{"location":"kafka/deploy-on-docker/#converter-configuration","title":"Converter Configuration","text":"<p>Converters control how data is serialized when writing to Kafka and deserialized when reading from Kafka:</p> <ul> <li>CONNECT_KEY_CONVERTER: <code>org.apache.kafka.connect.storage.StringConverter</code></li> <li>Converts message keys to/from strings</li> <li> <p>Simple format for most use cases</p> </li> <li> <p>CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: <code>http://schema-registry:8085</code></p> </li> <li>Schema Registry URL for key conversion (if needed)</li> <li> <p>Not used with StringConverter but available for schema-aware converters</p> </li> <li> <p>CONNECT_VALUE_CONVERTER: <code>org.apache.kafka.connect.storage.StringConverter</code></p> </li> <li>Converts message values to/from strings</li> <li> <p>Alternative converters: AvroConverter, JsonConverter, etc.</p> </li> <li> <p>CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: <code>http://schema-registry:8085</code></p> </li> <li>Schema Registry URL for value conversion</li> </ul>"},{"location":"kafka/deploy-on-docker/#internal-converters","title":"Internal Converters","text":"<ul> <li>CONNECT_INTERNAL_KEY_CONVERTER: <code>org.apache.kafka.connect.json.JsonConverter</code></li> <li>Format for keys in internal topics</li> <li> <p>Uses JSON for human readability</p> </li> <li> <p>CONNECT_INTERNAL_VALUE_CONVERTER: <code>org.apache.kafka.connect.json.JsonConverter</code></p> </li> <li>Format for values in internal topics</li> </ul>"},{"location":"kafka/deploy-on-docker/#rest-api-configuration","title":"REST API Configuration","text":"<ul> <li>CONNECT_REST_ADVERTISED_HOST_NAME: <code>kafka-connect</code></li> <li>Hostname advertised to other Connect workers</li> <li>Used for distributed coordination</li> </ul>"},{"location":"kafka/deploy-on-docker/#plugin-configuration","title":"Plugin Configuration","text":"<ul> <li>CONNECT_PLUGIN_PATH: <code>\"/usr/share/java,/usr/share/confluent-hub-components\"</code></li> <li>Directories where Connect looks for connector plugins</li> <li><code>/usr/share/java</code>: Built-in connectors</li> <li><code>/usr/share/confluent-hub-components</code>: Connectors installed via Confluent Hub</li> </ul> <p>Dependencies:  - All three Kafka brokers - schema-registry (for schema-aware converters)</p> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Integrates Kafka with external systems without writing custom code - Provides pre-built connectors for common integrations:   - Source Connectors: Pull data into Kafka (e.g., JDBC, MongoDB, S3)   - Sink Connectors: Push data from Kafka (e.g., Elasticsearch, HDFS) - Distributed and scalable architecture - Manages offset tracking and fault tolerance automatically - Supports transformations and schema evolution</p> <p>Common Use Cases: - Stream database changes to Kafka (CDC - Change Data Capture) - Export Kafka topics to data warehouses - Sync data between microservices - Real-time ETL pipelines</p>"},{"location":"kafka/deploy-on-docker/#prometheus","title":"prometheus","text":"<pre><code>prometheus:\n  image: prom/prometheus:latest\n  container_name: prometheus\n  hostname: prometheus\n</code></pre> <p>What it is: A time-series database and monitoring system for collecting and querying metrics from Kafka brokers and other services.</p> <p>Image: <code>prom/prometheus:latest</code> - Official Prometheus image</p> <p>Ports: - <code>9090:9090</code> - Prometheus web UI and query interface</p> <p>Volumes: - <code>./scripts:/etc/prometheus</code> - Mounts local scripts directory containing Prometheus configuration</p> <p>Command: <code>--web.enable-remote-write-receiver --config.file=/etc/prometheus/prometheus.yaml</code></p>"},{"location":"kafka/deploy-on-docker/#command-options","title":"Command Options:","text":"<ul> <li>--web.enable-remote-write-receiver</li> <li>Enables the remote write receiver endpoint</li> <li>Allows Kafka UI and other services to push metrics to Prometheus</li> <li> <p>Without this, Prometheus only scrapes (pulls) metrics</p> </li> <li> <p>--config.file=/etc/prometheus/prometheus.yaml</p> </li> <li>Specifies the configuration file location</li> <li>Points to the mounted prometheus.yaml in the scripts directory</li> <li>Configuration defines what targets to scrape</li> </ul> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Collects JMX metrics from Kafka brokers (via port 9997) - Stores time-series data for historical analysis - Provides a query language (PromQL) for analyzing metrics - Integrates with Kafka UI for metrics visualization - Monitors broker health, throughput, latency, consumer lag, etc.</p> <p>Typical Metrics Collected: - Broker CPU and memory usage - Message throughput (messages/sec) - Request latency - Partition counts - Consumer group lag - Replication status</p> <p>Configuration File (prometheus.yaml): Should contain scrape configs for Kafka brokers, e.g.: </p><pre><code>scrape_configs:\n  - job_name: 'kafka'\n    static_configs:\n      - targets: ['kafka1:9997', 'kafka2:9997', 'kafka3:9997']\n</code></pre>"},{"location":"kafka/deploy-on-docker/#filebrowser","title":"filebrowser","text":"<pre><code>filebrowser:\n  image: filebrowser/filebrowser:latest\n  container_name: filebrowser\n  hostname: filebrowser\n</code></pre> <p>What it is: A web-based file manager that provides a UI for browsing and managing files in Docker volumes.</p> <p>Image: <code>filebrowser/filebrowser:latest</code> - Official FileBrowser image</p> <p>Ports: - <code>9999:80</code> - Web interface accessible at http://localhost:9999</p> <p>Volumes: 1. Data Volume: <code>kafka-cluster:/srv</code>    - Mounts the shared Kafka volume to <code>/srv</code> (FileBrowser's default data path)    - Provides access to all Kafka data and logs    - Allows browsing: <code>/srv/kafka1/data</code>, <code>/srv/kafka1/logs</code>, etc.</p> <ol> <li>Database File: <code>./filebrowser/filebrowser.db:/database/filebrowser.db</code></li> <li>Persists FileBrowser's user database locally</li> <li>Stores user accounts, permissions, and settings</li> <li> <p>Survives container restarts</p> </li> <li> <p>Configuration File: <code>./filebrowser/settings.json:/.filebrowser.json</code></p> </li> <li>Mounts custom configuration</li> <li>Defines settings like authentication, branding, etc.</li> </ol> <p>Environment Variables: - FB_BASEURL: <code>/</code>   - Base URL for the application   - Set to <code>/</code> means it's accessible at the root path</p> <ul> <li>FB_LOG: <code>stdout</code></li> <li>Sends logs to standard output</li> <li>Makes logs visible via <code>docker logs filebrowser</code></li> </ul> <p>Network: <code>kafka-net</code></p> <p>Why it's used: - Browse Kafka data files and log segments without SSH/exec into containers - Inspect partition data and log files - Download logs for debugging - View file sizes and disk usage - Useful for development and troubleshooting - Provides a visual interface to the shared volume</p> <p>Default Access: - URL: http://localhost:9999 - Default credentials (if not configured): admin/admin - Change credentials in settings.json for security</p>"},{"location":"kafka/deploy-on-docker/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"kafka/deploy-on-docker/#step-1-start-the-cluster","title":"Step 1: Start the Cluster","text":"<p>Launch the entire stack: </p><pre><code>docker-compose -f docker-compose.yml up -d\n</code></pre>"},{"location":"kafka/deploy-on-docker/#step-2-check-service-health","title":"Step 2: Check Service Health","text":"<p>Check all containers are running: </p><pre><code>docker-compose ps\n</code></pre> <p>You should see all services in \"Up\" state.</p>"},{"location":"kafka/deploy-on-docker/#step-3-view-logs","title":"Step 3: View Logs","text":"<pre><code># All services\ndocker-compose logs -f\n\n# Specific service\ndocker-compose logs -f kafka1\ndocker-compose logs -f kafka-ui\n</code></pre>"},{"location":"kafka/deploy-on-docker/#step-4-access-web-uis","title":"Step 4: Access Web UIs","text":"<ul> <li>Kafka UI: http://localhost:8080 - Cluster management and monitoring</li> <li>Prometheus: http://localhost:9090 - Metrics and monitoring</li> <li>FileBrowser: http://localhost:9999 - Browse volume data and logs</li> <li>Schema Registry: http://localhost:8081 - Schema management API</li> <li>Kafka Connect: http://localhost:8083 - Connector management API</li> </ul>"},{"location":"kafka/deploy-on-docker/#testing-the-cluster","title":"Testing the Cluster","text":""},{"location":"kafka/deploy-on-docker/#open-the-kafka-sandbox-environment","title":"Open the kafka sandbox environment","text":"<pre><code>docker exec -it busybox  bash\n\n\n# Sample producer directly on console to test\ndocker exec -it busybox  bash /scripts/producer.sh\n\n# Sample consumer directly on console to test\ndocker exec -it busybox  bash /scripts/consumer.sh\n</code></pre>"},{"location":"kafka/deploy-on-docker/#create-a-topic","title":"Create a Topic","text":"<p>Create a test topic with 3 partitions and replication factor of 3:</p> <pre><code>kafka-topics \\\n    --create \\\n    --topic test-topic \\\n    --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092 \\\n    --replication-factor 3 \\\n    --partitions 3\n</code></pre>"},{"location":"kafka/deploy-on-docker/#list-all-topics","title":"List All Topics","text":"<p>View all topics in the cluster:</p> <pre><code>kafka-topics \\\n    --list \\\n    --bootstrap-server kafka1:9092\n</code></pre>"},{"location":"kafka/deploy-on-docker/#describe-a-topic","title":"Describe a Topic","text":"<p>Get detailed information about a topic (partitions, replicas, leaders):</p> <pre><code>kafka-topics \\\n    --describe \\\n    --topic test-topic \\\n    --bootstrap-server kafka1:9092\n</code></pre>"},{"location":"kafka/deploy-on-docker/#working-with-producers-and-consumers","title":"Working with Producers and Consumers","text":""},{"location":"kafka/deploy-on-docker/#basic-producer","title":"Basic Producer","text":"<p>Produce messages to your topic using the Kafka CLI:</p> <pre><code>kafka-console-producer \\\n    --broker-list kafka1:9092 \\\n    --topic test-topic\n</code></pre> <p>Type your messages and press Enter to send them. Press Ctrl+C to exit.</p>"},{"location":"kafka/deploy-on-docker/#basic-consumer","title":"Basic Consumer","text":"<p>Consume messages from the beginning of the topic:</p> <pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka1:9092 \\\n    --topic test-topic \\\n    --from-beginning\n</code></pre> <p>This will display all messages from offset 0. Press Ctrl+C to exit.</p>"},{"location":"kafka/deploy-on-docker/#advanced-producerconsumer-scenarios","title":"Advanced Producer/Consumer Scenarios","text":""},{"location":"kafka/deploy-on-docker/#produce-messages-with-keys","title":"Produce Messages with Keys","text":"<p>Produce messages with keys for partition routing:</p> <pre><code>kafka-console-producer \\\n    --broker-list kafka1:9092 \\\n    --topic test-topic \\\n    --property \"parse.key=true\" \\\n    --property \"key.separator=:\"\n</code></pre> <p>Type messages in the format <code>key1:value1</code>, <code>key2:value2</code>, etc.</p> <p>Example messages: </p><pre><code>user1:{\"name\":\"Alice\",\"age\":30}\nuser2:{\"name\":\"Bob\",\"age\":25}\nuser1:{\"name\":\"Alice\",\"age\":31}\n</code></pre> <p>Messages with the same key will go to the same partition.</p>"},{"location":"kafka/deploy-on-docker/#consumer-groups-with-load-balancing","title":"Consumer Groups with Load Balancing","text":"<p>Create two consumers in the same consumer group to enable partition assignment and load balancing.</p> <p>Consumer 1: </p><pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka1:9092 \\\n    --topic test-topic \\\n    --group my-consumer-group\n</code></pre> <p>Consumer 2 (in a separate terminal): </p><pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka2:9092 \\\n    --topic test-topic \\\n    --group my-consumer-group\n</code></pre> <p>How it works: - Both consumers share the same group ID (<code>my-consumer-group</code>) - Kafka automatically assigns partitions to each consumer - With 3 partitions and 2 consumers, one consumer gets 2 partitions, the other gets 1 - Messages are load-balanced across consumers - Each message is delivered to only one consumer in the group</p>"},{"location":"kafka/deploy-on-docker/#cluster-management-commands","title":"Cluster Management Commands","text":""},{"location":"kafka/deploy-on-docker/#stop-the-stack","title":"Stop the Stack","text":"<p>Stop all services (data persists in volumes): </p><pre><code>docker-compose down\n</code></pre>"},{"location":"kafka/deploy-on-docker/#stop-and-remove-all-data","title":"Stop and Remove All Data","text":"<p>Stop services and delete all volumes (removes all data): </p><pre><code>docker-compose down -v\n</code></pre> <p>Warning: This will permanently delete all Kafka data, logs, and configurations.</p>"},{"location":"kafka/deploy-on-docker/#restart-a-specific-service","title":"Restart a Specific Service","text":"<pre><code>docker-compose restart kafka1\n</code></pre>"},{"location":"kafka/deploy-on-docker/#view-resource-usage","title":"View Resource Usage","text":"<pre><code>docker stats\n</code></pre>"},{"location":"kafka/deploy-on-docker/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Docker Network: kafka-net                \u2502\n\u2502                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502\n\u2502  \u2502  kafka1  \u2502  \u2502  kafka2  \u2502  \u2502  kafka3  \u2502  (KRaft Cluster)      \u2502\n\u2502  \u2502  :9092   \u2502  \u2502  :9094   \u2502  \u2502  :9096   \u2502                       \u2502\n\u2502  \u2502  :9093   \u2502  \u2502  :9095   \u2502  \u2502  :9097   \u2502                       \u2502\n\u2502  \u2502  :9997   \u2502  \u2502          \u2502  \u2502          \u2502                       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502       \u2502             \u2502             \u2502                              \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                     \u2502                                             \u2502\n\u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502       \u2502             \u2502                         \u2502                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 Schema  \u2502  \u2502  Kafka   \u2502  \u2502  Kafka UI   \u2502 \u2502Prometheus \u2502      \u2502\n\u2502  \u2502Registry \u2502  \u2502 Connect  \u2502  \u2502   :8080     \u2502 \u2502  :9090    \u2502      \u2502\n\u2502  \u2502  :8081  \u2502  \u2502  :8083   \u2502  \u2502             \u2502 \u2502           \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502  \u2502 FileBrowser  \u2502  \u2502    busybox      \u2502                          \u2502\n\u2502  \u2502    :9999     \u2502  \u2502  (init helper)  \u2502                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                                                                   \u2502\n\u2502  Volume: kafka-cluster                                           \u2502\n\u2502  \u251c\u2500\u2500 kafka1/                                                     \u2502\n\u2502  \u2502   \u251c\u2500\u2500 data/  (topic partitions)                              \u2502\n\u2502  \u2502   \u2514\u2500\u2500 logs/  (application logs)                              \u2502\n\u2502  \u251c\u2500\u2500 kafka2/                                                     \u2502\n\u2502  \u2514\u2500\u2500 kafka3/                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"kafka/deploy-on-docker/#key-features","title":"Key Features","text":""},{"location":"kafka/deploy-on-docker/#high-availability","title":"High Availability","text":"<ul> <li>3-node cluster with replication factor of 3</li> <li>Survives single broker failure</li> <li>Automatic leader election via KRaft</li> </ul>"},{"location":"kafka/deploy-on-docker/#monitoring-management","title":"Monitoring &amp; Management","text":"<ul> <li>Kafka UI for visual management</li> <li>Prometheus for metrics collection</li> <li>JMX endpoints on all brokers</li> <li>FileBrowser for data inspection</li> </ul>"},{"location":"kafka/deploy-on-docker/#data-integration","title":"Data Integration","text":"<ul> <li>Schema Registry for schema management</li> <li>Kafka Connect for external system integration</li> <li>Pre-configured connectors</li> </ul>"},{"location":"kafka/deploy-on-docker/#development-friendly","title":"Development-Friendly","text":"<ul> <li>Automatic topic creation</li> <li>Sample data loading</li> <li>Easy access to all services</li> <li>Persistent data storage</li> </ul>"},{"location":"kafka/deploy-on-docker/#production-considerations","title":"Production Considerations","text":"<p>For production deployments, consider these changes:</p> <ol> <li>Security:</li> <li>Enable SSL/TLS for all listeners</li> <li>Configure SASL authentication</li> <li>Secure JMX endpoints</li> <li> <p>Use secrets management for credentials</p> </li> <li> <p>Replication:</p> </li> <li>Keep <code>KAFKA_MIN_INSYNC_REPLICAS: 2</code></li> <li>Set transaction log replication to 3</li> <li> <p>Increase <code>KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS</code></p> </li> <li> <p>Resources:</p> </li> <li>Set JVM heap sizes</li> <li>Configure resource limits in Docker</li> <li> <p>Allocate sufficient disk space</p> </li> <li> <p>Networking:</p> </li> <li>Use proper DNS names</li> <li>Configure external listeners for client access</li> <li> <p>Set up load balancers</p> </li> <li> <p>Monitoring:</p> </li> <li>Enable authentication on Prometheus</li> <li>Set up alerting rules</li> <li>Configure log aggregation</li> </ol>"},{"location":"kafka/deploy-on-docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kafka/deploy-on-docker/#cluster-wont-start","title":"Cluster won't start","text":"<ul> <li>Check that CLUSTER_ID is identical across all brokers</li> <li>Verify KAFKA_CONTROLLER_QUORUM_VOTERS lists all 3 nodes</li> <li>Check logs: <code>docker-compose logs kafka1</code></li> </ul>"},{"location":"kafka/deploy-on-docker/#cant-connect-from-host","title":"Can't connect from host","text":"<ul> <li>Verify port mappings are correct</li> <li>Check KAFKA_ADVERTISED_LISTENERS</li> <li>Test with: <code>telnet localhost 9092</code></li> </ul>"},{"location":"kafka/deploy-on-docker/#topics-not-created","title":"Topics not created","text":"<ul> <li>Check kafka-init-topics logs</li> <li>Verify brokers are ready before topic creation</li> <li>Manually create: <code>kafka-topics --create ...</code></li> </ul>"},{"location":"kafka/deploy-on-docker/#permissions-errors","title":"Permissions errors","text":"<ul> <li>Ensure init-kafka completed successfully</li> <li>Check volume permissions: <code>docker exec kafka1 ls -la /mnt/shared</code></li> </ul>"},{"location":"kafka/deploy-on-docker/#conclusion","title":"Conclusion","text":"<p>This Docker Compose setup provides a complete, production-like Kafka environment for development and testing. It includes all necessary components for a modern Kafka deployment with monitoring, management, and integration capabilities.</p>"},{"location":"kafka/introduction/","title":"introduction","text":""},{"location":"kafka/introduction/#apache-kafka-complete-cheat-sheet","title":"Apache Kafka - Complete Cheat Sheet","text":""},{"location":"kafka/introduction/#1-topic","title":"1\ufe0f\u20e3 Topic","text":"<ul> <li>A topic is a logical channel or category where producers send records and consumers read them.</li> <li>Topics are the main way Kafka organizes data streams.</li> <li>Each topic can have multiple partitions for scalability.</li> <li>Records in a topic consist of:<ul> <li>Key (optional): Used for partitioning and message grouping.</li> <li>Value (payload): The actual data/message.</li> <li>Offset: Unique position of the record within a partition.</li> </ul> </li> <li>Topics can be configured for retention (how long data is kept) and compaction (keeping only the latest value per key).</li> <li>Topics are append-only; records cannot be updated or deleted after being written.</li> </ul> <p>Example: An <code>orders</code> topic collects all order events from an e-commerce platform.</p>"},{"location":"kafka/introduction/#2-consumer-group","title":"2\ufe0f\u20e3 Consumer Group","text":"<ul> <li>A consumer group is a set of consumers that work together to read data from a topic.</li> <li>Kafka ensures that each partition in a topic is consumed by only one consumer within a group, enabling parallel processing and load balancing.</li> <li>Multiple consumer groups can read the same topic independently, allowing different applications to process the same data in their own way.</li> <li>Consumer groups provide fault tolerance: if one consumer fails, another in the group can take over.</li> <li>Consumers in a group coordinate their progress using offsets.</li> </ul> <p>Example: A group of analytics services consuming the <code>orders</code> topic, each processing a subset of the data.</p>"},{"location":"kafka/introduction/#3-partition","title":"3\ufe0f\u20e3 Partition","text":"<ul> <li>Topics are split into partitions, which are the basic unit of parallelism and scalability in Kafka.</li> <li>Each partition is:<ul> <li>Ordered: Records are stored in the order they arrive.</li> <li>Immutable: Once written, records cannot be changed.</li> <li>Identified: By <code>&lt;topic&gt;-&lt;partition_number&gt;</code>.</li> </ul> </li> <li>Partitions allow Kafka to distribute data across multiple brokers and scale horizontally.</li> <li>More partitions mean higher throughput, but also more overhead for management and coordination.</li> </ul> <p>Example: The <code>orders</code> topic might have 6 partitions, allowing 6 consumers to process data in parallel.</p>"},{"location":"kafka/introduction/#4-replication","title":"4\ufe0f\u20e3 Replication","text":"<ul> <li>Kafka replicates each partition across multiple brokers for high availability and durability.</li> <li>One broker acts as the leader for a partition; others are followers.</li> <li>Producers and consumers interact only with the leader.</li> <li>If the leader fails, a follower is automatically promoted to leader, ensuring no data loss.</li> <li>Replication factor is configurable per topic.</li> </ul> <p>Example: A partition with replication factor 3 is stored on three brokers.</p>"},{"location":"kafka/introduction/#offset","title":"Offset","text":"<ul> <li>An offset is a unique, sequential number assigned to each record within a partition.</li> <li>Offsets allow consumers to track their progress and resume processing after failures.</li> <li>Offsets are managed per partition and per consumer group.</li> <li>Consumers can commit offsets manually or automatically.</li> <li>Offsets enable features like replay (reprocessing old data) and exactly-once or at-least-once delivery semantics.</li> </ul> <p>Example: Consumer group A has processed up to offset 100 in partition 2.</p>"},{"location":"kafka/introduction/#5-routing-records-oddeven-example","title":"5\ufe0f\u20e3 Routing Records (Odd/Even Example)","text":"<ul> <li>Producers can control which partition a record goes to by specifying a key or using a custom partitioner.</li> <li>This enables routing logic, such as sending odd numbers to one partition and even numbers to another.</li> <li>Consumers can be assigned to specific partitions to process only the relevant data.</li> </ul> <p>Producer Example: </p><pre><code>int partition = (number % 2 == 0) ? 1 : 0;\nProducerRecord&lt;String, String&gt; record =\n        new ProducerRecord&lt;&gt;(\"numbers\", partition, null, String.valueOf(number));\nproducer.send(record);\n</code></pre> <p>Consumer Example: </p><pre><code>KafkaConsumer&lt;String, String&gt; oddConsumer = new KafkaConsumer&lt;&gt;(props);\noddConsumer.assign(List.of(new TopicPartition(\"numbers\", 0))); // Odd partition\n\nKafkaConsumer&lt;String, String&gt; evenConsumer = new KafkaConsumer&lt;&gt;(props);\nevenConsumer.assign(List.of(new TopicPartition(\"numbers\", 1))); // Even partition\n</code></pre> <p>Use case: Targeted processing, filtering, or sharding of data streams.</p>"},{"location":"kafka/introduction/#6-change-data-capture-cdc","title":"6\ufe0f\u20e3 Change Data Capture (CDC)","text":"<ul> <li>CDC is a technique for capturing changes (INSERT, UPDATE, DELETE) in databases and streaming them into Kafka topics.</li> <li>Enables real-time data synchronization between databases and other systems.</li> <li>Common CDC tools:<ul> <li>Debezium: Open-source, supports MySQL, PostgreSQL, MongoDB, SQL Server, Oracle, etc.</li> <li>Kafka Connect CDC connectors.</li> </ul> </li> <li>Use cases:<ul> <li>Replicating database changes to analytics platforms.</li> <li>Building event-driven architectures.</li> <li>Maintaining audit logs and data lineage.</li> </ul> </li> </ul>"},{"location":"kafka/introduction/#7-kafka-connect","title":"7\ufe0f\u20e3 Kafka Connect","text":"<ul> <li>Kafka Connect is a framework for integrating Kafka with external systems (databases, files, cloud storage, etc.).</li> <li>Provides source connectors (import data into Kafka) and sink connectors (export data from Kafka).</li> <li>Connectors are configurable and scalable, supporting distributed deployments.</li> <li>Enables building ETL pipelines without custom code.</li> <li>Supports transformations and error handling.</li> </ul> <p>Example: Use Kafka Connect to stream data from MySQL into Kafka, then from Kafka to Elasticsearch.</p>"},{"location":"kafka/introduction/#8-schema-registry","title":"8\ufe0f\u20e3 Schema Registry","text":"<ul> <li>The Schema Registry manages schemas for Kafka messages (Avro, JSON, Protobuf).</li> <li>Ensures producers and consumers agree on message structure, preventing data corruption.</li> <li>Supports schema evolution (backward/forward compatibility).</li> <li>Enforces rules to prevent breaking changes.</li> <li>Integrates with Kafka clients for automatic serialization/deserialization.</li> </ul> <p>Example: Enforce Avro schema for all messages in the <code>orders</code> topic.</p>"},{"location":"kafka/introduction/#9-zookeeper-kraft","title":"9\ufe0f\u20e3 ZooKeeper &amp; KRaft","text":""},{"location":"kafka/introduction/#zookeeper-legacy","title":"ZooKeeper (legacy)","text":"<ul> <li>Kafka originally used ZooKeeper for cluster metadata, broker management, and controller election.</li> <li>Required a separate ZooKeeper cluster, adding operational complexity.</li> </ul>"},{"location":"kafka/introduction/#kraft-kafka-raft-mode","title":"KRaft (Kafka Raft mode)","text":"<ul> <li>Modern Kafka (2.8+) uses KRaft (Kafka Raft) mode, eliminating ZooKeeper.</li> <li>Stores metadata in internal Kafka topics.</li> <li>Simplifies deployment, improves scalability and resilience.</li> <li>New Kafka clusters should use KRaft mode.</li> </ul>"},{"location":"kafka/introduction/#additional-features","title":"\ud83d\udd1f Additional Features","text":"<ul> <li>Log Compaction: Retains only the latest value per key, useful for stateful applications.</li> <li>Exactly-Once Semantics (EOS): Guarantees that each message is processed only once, even in failure scenarios.</li> <li>Streams API: Enables building real-time, stateful stream processing applications directly on Kafka.</li> <li>Kafka Admin API: Programmatically manage topics, configurations, and access controls (ACLs).</li> <li>Security: Supports SSL, SASL, and ACLs for authentication and authorization.</li> <li>Monitoring: Exposes metrics via JMX and integrates with monitoring tools.</li> </ul>"},{"location":"kafka/introduction/#kafka-use-cases","title":"\ud83d\udd25 Kafka Use Cases","text":"<ul> <li>Real-time analytics: Process and analyze data streams instantly for dashboards and alerts.</li> <li>Event sourcing: Store all changes as a sequence of events for audit and recovery.</li> <li>Log aggregation: Centralize logs from multiple services for troubleshooting and monitoring.</li> <li>Messaging: Decouple producers and consumers for scalable, resilient architectures.</li> <li>Data integration: Move data between databases, caches, search engines, and cloud services.</li> <li>Microservices communication: Enable asynchronous, reliable communication between services.</li> </ul>"},{"location":"kafka/introduction/#useful-kafka-cli-commands","title":"\ud83d\udee0\ufe0f Useful Kafka CLI Commands","text":"<pre><code># List topics\nkafka-topics.sh --list --bootstrap-server localhost:9092\n\n# Create a topic\nkafka-topics.sh --create --topic my-topic --partitions 3 --replication-factor 2 --bootstrap-server localhost:9092\n\n# Describe a topic\nkafka-topics.sh --describe --topic my-topic --bootstrap-server localhost:9092\n\n# Produce messages\nkafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092\n\n# Consume messages\nkafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092\n\n# List consumer groups\nkafka-consumer-groups.sh --list --bootstrap-server localhost:9092\n\n# Describe consumer group offsets\nkafka-consumer-groups.sh --describe --group my-group --bootstrap-server localhost:9092\n</code></pre>"},{"location":"kafka/introduction/#references","title":"\ud83d\udcda References","text":"<ul> <li>Apache Kafka Documentation</li> <li>Kafka Quickstart Guide</li> <li>Debezium CDC</li> <li>Confluent Schema Registry</li> <li>Kafka Connectors Hub</li> </ul> <p>Tip: Monitor Kafka cluster health, tune configurations, and secure your deployment for optimal performance and reliability.</p>"},{"location":"kafka/kafbat-ui/","title":"Kafbat UI","text":""},{"location":"kafka/kafbat-ui/#kafbat-ui","title":"Kafbat UI","text":"<p>Kafbat UI (formerly <code>provectus/kafka-ui</code>) is the active, community-maintained open-source web UI for Apache Kafka. It serves as the direct successor to the now-inactive Provectus project.</p>"},{"location":"kafka/kafbat-ui/#project-status","title":"Project Status","text":"Feature Provectus (<code>provectus/kafka-ui</code>) Kafbat (<code>kafbat/kafka-ui</code>) Status \ud83d\udd34 Inactive / Abandoned \ud83d\udfe2 Active / Maintained Github link provectus/kafka-ui kafbat/kafka-ui Security Unpatched Vulnerabilities (CVEs) Actively Patched Version Stalled at v0.7.x v1.0+ Docker Image <code>provectuslabs/kafka-ui</code> <code>ghcr.io/kafbat/kafka-ui</code>"},{"location":"kafka/kafbat-ui/#why-the-split","title":"Why the Split?","text":"<p>The original creator (Provectus) paused development on the project around late 2023. To prevent the tool from dying, the core open-source maintainers and community forked the repository. They rebranded it as Kafbat to facilitate continued security updates, bug fixes, and feature development.</p>"},{"location":"kafka/kafbat-ui/#key-improvements-in-kafbat","title":"Key Improvements in Kafbat","text":"<ul> <li>Security: Critical CVEs found in old dependencies have been resolved.</li> <li>RBAC: Enhanced Role-Based Access Control and ACL management.</li> <li>Smart Filters: Better filtering capabilities for topic messages.</li> <li>Bug Fixes: Resolution of long-standing issues from the original repo.</li> </ul>"},{"location":"kafka/kafbat-ui/#migration-guide","title":"Migration Guide","text":"<p>Since Kafbat is a direct fork, it is largely backward compatible. In most cases, migration only requires changing the Docker image source.</p>"},{"location":"kafka/kafbat-ui/#docker-compose","title":"Docker Compose","text":"<p>Update your <code>docker-compose.yaml</code>:</p> <pre><code>services:\n  kafka-ui:\n    # OLD: image: provectuslabs/kafka-ui:latest\n    # NEW:\n    image: ghcr.io/kafbat/kafka-ui:latest\n    container_name: kafka-ui\n    ports:\n      - \"8080:8080\"\n    environment:\n      - KAFKA_CLUSTERS_0_NAME=local\n      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092\n</code></pre>"},{"location":"kafka/kafka-deploy-on-docker/","title":"Apache Kafka Cluster with Docker Compose (KRaft Mode)","text":""},{"location":"kafka/kafka-deploy-on-docker/#apache-kafka-cluster-with-docker-compose-kraft-mode","title":"Apache Kafka Cluster with Docker Compose (KRaft Mode)","text":""},{"location":"kafka/kafka-deploy-on-docker/#introduction","title":"Introduction","text":"<p>Apache Kafka is the standard for real-time data pipelines and streaming applications. This guide walks you through setting up a Kafka cluster using Docker Compose in KRaft mode (no Zookeeper required).</p>"},{"location":"kafka/kafka-deploy-on-docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose (tested with Docker 27.0.3)</li> <li>Kafka basics (Kafka 3.8+)</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#architecture-overview","title":"Architecture Overview","text":"<ul> <li>3 Kafka brokers (KRaft mode)</li> <li>Kafka UI for management</li> <li>Docker network for communication</li> <li>Persistent volumes for data</li> </ul> <p>KRaft mode replaces Zookeeper with a built-in consensus mechanism, simplifying the architecture.</p>"},{"location":"kafka/kafka-deploy-on-docker/#step-1-project-setup","title":"Step 1: Project Setup","text":"<pre><code>mkdir kafka-cluster\ncd kafka-cluster\nmkdir -p kafka1/data kafka2/data kafka3/data\n</code></pre>"},{"location":"kafka/kafka-deploy-on-docker/#step-2-docker-compose-configuration","title":"Step 2: Docker Compose Configuration","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nnetworks:\n    kafka-net:\n        driver: bridge\n\nservices:\n    kafka1:\n        image: confluentinc/cp-kafka:7.8.0\n        hostname: kafka1\n        container_name: kafka1\n        ports:\n            - \"9092:9092\"\n            - \"9093:9093\"\n        environment:\n            KAFKA_NODE_ID: 1\n            KAFKA_BROKER_ID: 1\n            KAFKA_PROCESS_ROLES: 'broker,controller'\n            KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'\n            KAFKA_LISTENERS: 'PLAINTEXT://kafka1:9092,CONTROLLER://kafka1:9093'\n            KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka1:9092'\n            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'\n            KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n            KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'\n            CLUSTER_ID: 'EmptNWtoR4GGWx-BH6nGLQ'\n            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n            KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n            KAFKA_MIN_INSYNC_REPLICAS: 2\n        volumes:\n            - ./kafka1/data:/var/lib/kafka/data\n        networks:\n            - kafka-net\n\n    kafka2:\n        image: confluentinc/cp-kafka:7.8.0\n        hostname: kafka2\n        container_name: kafka2\n        ports:\n            - \"9094:9092\"\n            - \"9095:9093\"\n        environment:\n            KAFKA_NODE_ID: 2\n            KAFKA_BROKER_ID: 2\n            KAFKA_PROCESS_ROLES: 'broker,controller'\n            KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'\n            KAFKA_LISTENERS: 'PLAINTEXT://kafka2:9092,CONTROLLER://kafka2:9093'\n            KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka2:9092'\n            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'\n            KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n            KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'\n            CLUSTER_ID: 'EmptNWtoR4GGWx-BH6nGLQ'\n            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n            KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n            KAFKA_MIN_INSYNC_REPLICAS: 2\n        volumes:\n            - ./kafka2/data:/var/lib/kafka/data\n        networks:\n            - kafka-net\n\n    kafka3:\n        image: confluentinc/cp-kafka:7.8.0\n        hostname: kafka3\n        container_name: kafka3\n        ports:\n            - \"9096:9092\"\n            - \"9097:9093\"\n        environment:\n            KAFKA_NODE_ID: 3\n            KAFKA_BROKER_ID: 3\n            KAFKA_PROCESS_ROLES: 'broker,controller'\n            KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'\n            KAFKA_LISTENERS: 'PLAINTEXT://kafka3:9092,CONTROLLER://kafka3:9093'\n            KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka3:9092'\n            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'\n            KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'\n            KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'\n            CLUSTER_ID: 'EmptNWtoR4GGWx-BH6nGLQ'\n            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3\n            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n            KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n            KAFKA_MIN_INSYNC_REPLICAS: 2\n        volumes:\n            - ./kafka3/data:/var/lib/kafka/data\n        networks:\n            - kafka-net\n\n    kafka-ui:\n        image: provectuslabs/kafka-ui:latest\n        container_name: kafka-cluster-ui\n        ports:\n            - \"8080:8080\"\n        environment:\n            KAFKA_CLUSTERS_0_NAME: local\n            KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092,kafka2:9092,kafka3:9092\n        depends_on:\n            - kafka1\n            - kafka2\n            - kafka3\n        networks:\n            - kafka-net\n</code></pre>"},{"location":"kafka/kafka-deploy-on-docker/#step-3-configuration-breakdown","title":"Step 3: Configuration Breakdown","text":""},{"location":"kafka/kafka-deploy-on-docker/#network","title":"Network","text":"<pre><code>networks:\n    kafka-net:\n        driver: bridge\n</code></pre> Creates an isolated network for secure container communication."},{"location":"kafka/kafka-deploy-on-docker/#brokers","title":"Brokers","text":"<ul> <li>KRaft mode: <code>KAFKA_PROCESS_ROLES: 'broker,controller'</code></li> <li>Listeners: <code>KAFKA_LISTENERS</code> and <code>KAFKA_ADVERTISED_LISTENERS</code> define how brokers and clients connect.</li> <li>Replication: <ul> <li><code>KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR</code>: Replication for <code>__consumer_offsets</code> topic.</li> <li><code>KAFKA_DEFAULT_REPLICATION_FACTOR</code>: Default partition replication.</li> <li><code>KAFKA_MIN_INSYNC_REPLICAS</code>: Minimum replicas for write acknowledgment.</li> </ul> </li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#step-4-launch-the-cluster","title":"Step 4: Launch the Cluster","text":"<pre><code>docker compose -f docker-compose.yml up -d\n</code></pre> <p>Check containers:</p> <pre><code>docker compose ps\n</code></pre>"},{"location":"kafka/kafka-deploy-on-docker/#step-5-test-the-cluster","title":"Step 5: Test the Cluster","text":"<p>Create a topic:</p> <pre><code>kafka-topics \\\n    --create \\\n    --topic test-topic \\\n    --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092 \\\n    --replication-factor 3 \\\n    --partitions 3\n</code></pre> <p>List topics:</p> <pre><code>kafka-topics \\\n    --list \\\n    --bootstrap-server kafka1:9092\n</code></pre> <p>Describe topic:</p> <pre><code>kafka-topics \\\n    --describe \\\n    --topic test-topic \\\n    --bootstrap-server kafka1:9092\n</code></pre>"},{"location":"kafka/kafka-deploy-on-docker/#create-a-producer","title":"Create a Producer","text":"<p>You can produce messages to your topic using the Kafka CLI:</p> <pre><code>kafka-console-producer \\\n    --broker-list kafka1:9092 \\\n    --topic test-topic\n</code></pre> <p>Type your messages and press Enter to send them.</p>"},{"location":"kafka/kafka-deploy-on-docker/#create-a-consumer-from-offset-0","title":"Create a Consumer from Offset 0","text":"<p>Consume messages from the beginning of the topic:</p> <pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka1:9092 \\\n    --topic test-topic \\\n    --from-beginning\n</code></pre> <p>This will display all messages from offset 0.</p>"},{"location":"kafka/kafka-deploy-on-docker/#produce-messages-with-keys-and-consumer-groups","title":"Produce Messages with Keys and Consumer Groups","text":"<p>You can produce messages with a key using the Kafka CLI:</p> <pre><code>kafka-console-producer \\\n    --broker-list kafka1:9092 \\\n    --topic test-topic \\\n    --property \"parse.key=true\" \\\n    --property \"key.separator=:\"\n</code></pre> <p>Type messages in the format <code>key1:value1</code>, <code>key2:value2</code>, etc.</p>"},{"location":"kafka/kafka-deploy-on-docker/#create-two-consumers-in-the-same-consumer-group","title":"Create Two Consumers in the Same Consumer Group","text":"<p>Start two consumers with the same group ID to enable partition assignment and load balancing:</p> <p>Consumer 1:</p> <pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka1:9092 \\\n    --topic test-topic \\\n    --group my-consumer-group\n</code></pre> <p>Consumer 2:</p> <pre><code>kafka-console-consumer \\\n    --bootstrap-server kafka2:9092 \\\n    --topic test-topic \\\n    --group my-consumer-group\n</code></pre> <p>Both consumers will share the workload and receive messages according to partition assignment within the group.</p>"},{"location":"kafka/kafka-deploy-on-docker/#step-6-kafka-ui","title":"Step 6: Kafka UI","text":"<p>Access http://localhost:8080 for:</p> <ul> <li>Broker health</li> <li>Topic management</li> <li>Consumer groups</li> <li>Message browsing</li> <li>Metrics</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#production-considerations","title":"Production Considerations","text":""},{"location":"kafka/kafka-deploy-on-docker/#security","title":"Security","text":"<ul> <li>Enable SSL/TLS</li> <li>Use SASL authentication</li> <li>Set ACLs</li> <li>Secure Kafka UI</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#performance","title":"Performance","text":"<ul> <li>Tune replica fetch/batch sizes</li> <li>Set retention policies</li> <li>Adjust JVM heap</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#monitoring","title":"Monitoring","text":"<ul> <li>Prometheus metrics</li> <li>Grafana dashboards</li> <li>Alerts/log aggregation</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#conclusion","title":"Conclusion","text":"<p>You now have a production-ready Kafka cluster in Docker with:</p> <ul> <li>Three-node redundancy</li> <li>KRaft consensus (no Zookeeper)</li> <li>Web UI</li> <li>Persistent storage</li> <li>Network isolation</li> </ul> <p>This setup is a solid foundation for scalable streaming applications.</p>"},{"location":"kafka/kafka-deploy-on-docker/#resources","title":"Resources","text":"<ul> <li>Apache Kafka Documentation</li> <li>Confluent Docker Images</li> <li>Kafka UI GitHub</li> </ul>"},{"location":"kafka/kafka-deploy-on-docker/#faq","title":"FAQ","text":""},{"location":"kafka/kafka-deploy-on-docker/#how-can-i-define-my-cluster_id","title":"How can I define my <code>cluster_id</code>?","text":"<p>Kafka provides a helper command to generate a random cluster ID:</p> <pre><code>kafka-storage.sh random-uuid\n</code></pre> <p>You can use the output of this command as your <code>CLUSTER_ID</code> in the Docker Compose configuration.</p> <p>Thank you, it's very helpful for me.</p>"},{"location":"kafka/kafka/","title":"A Complete Comparison: Confluent vs. Apache Kafka\u00ae","text":""},{"location":"kafka/kafka/#a-complete-comparison-confluent-vs-apache-kafka","title":"A Complete Comparison: Confluent vs. Apache Kafka\u00ae","text":"<p>This guide breaks down the differences between the open-source Apache Kafka project and the commercial Confluent offering (Platform &amp; Cloud).</p>"},{"location":"kafka/kafka/#1-executive-summary","title":"1. Executive Summary","text":"Apache Kafka\u00ae Confluent The Engine The Complete Car Free, open-source software (OSS). You download the code and build the infrastructure yourself. It requires significant engineering expertise to manage, secure, and scale. A commercial data streaming platform built on top of Apache Kafka. It includes the Kafka engine plus a massive suite of enterprise tools (GUI, security, connectors, disaster recovery). Best for: Tech-heavy teams who want full control, zero licensing costs, and are willing to handle all operations manually. Best for: Enterprises needing speed-to-market, strict SLAs, advanced security (RBAC), and reduced operational overhead."},{"location":"kafka/kafka/#2-detailed-feature-comparison","title":"2. Detailed Feature Comparison","text":"Feature Category Feature Apache Kafka (Open Source) Confluent (Platform &amp; Cloud) Core License Apache 2.0 (Free) Commercial / Community License Management UI None (CLI only) Control Center (Web GUI) Architecture Brokers + Zookeeper (or KRaft) Brokers + KRaft + Kora Engine (Cloud) Updates 3 releases/year (manual upgrade) Rolling updates / Managed (Cloud) Development Schema Management \u274c None (Third-party required) \u2705 Schema Registry (Avro, Protobuf, JSON) Stream Processing Kafka Streams (Java Library) ksqlDB (SQL-based) &amp; Flink (Managed) Connectors Framework only (Build your own) 120+ Pre-built Connectors (S3, Oracle, etc.) Clients Java / Scala C, C++, Python, Go, .NET, Java Operations Storage Local Disk (limited by broker size) Tiered Storage (Offload to S3/GCS) Rebalancing Manual (Risk of performance hit) Self-Balancing Clusters (Automated) Multi-DC / DR MirrorMaker 2 (Manual setup) Cluster Linking &amp; Multi-Region Clusters Kubernetes Manual manifests / Strimzi Confluent for Kubernetes (CFK) Operator Security Authentication SSL / SASL (Plain, SCRAM, Kerberos) OAuth, OIDC, LDAP, AD Integration Authorization ACLs (Simple Allow/Deny) RBAC (Role-Based Access Control) Auditing \u274c None \u2705 Structured Audit Logs Encryption Over the wire (TLS) At rest + Bring Your Own Key (BYOK)"},{"location":"kafka/kafka/#3-deep-dive-key-confluent-only-features","title":"3. Deep Dive: Key \"Confluent-Only\" Features","text":"<p>While Apache Kafka provides the messaging backbone, Confluent adds layers that are critical for production enterprise environments.</p>"},{"location":"kafka/kafka/#schema-registry","title":"\ud83e\udde0 Schema Registry","text":"<ul> <li>Apache Kafka: Does not understand the data inside your messages. If a producer changes the data format, downstream consumers will crash.</li> <li>Confluent: Provides a centralized Schema Registry that enforces data contracts (Avro, Protobuf, JSON). It prevents \"bad\" data from being produced and breaks the tightness between producers and consumers.</li> </ul>"},{"location":"kafka/kafka/#tiered-storage","title":"\ud83d\udcbe Tiered Storage","text":"<ul> <li>Apache Kafka: You are limited by the physical disk space on your brokers. Storing 1 year of data is expensive and slows down recovery.</li> <li>Confluent: Offloads older data to cheap object storage (AWS S3, Google GCS, Azure Blob) seamlessly. Brokers stay lightweight and fast, while you can retain infinite historical data cheaply.</li> </ul>"},{"location":"kafka/kafka/#ksqldb-flink","title":"\u26a1 ksqlDB &amp; Flink","text":"<ul> <li>Apache Kafka: You must write Java/Scala code using the Kafka Streams library to process data.</li> <li>Confluent: Offers ksqlDB, which allows you to build stream processing apps using simple SQL commands (e.g., <code>CREATE STREAM fraudulent_payments AS SELECT * FROM payments WHERE amount &gt; 10000</code>). Confluent Cloud also offers managed Apache Flink.</li> </ul>"},{"location":"kafka/kafka/#cluster-linking","title":"\ud83d\udd17 Cluster Linking","text":"<ul> <li>Apache Kafka: To replicate data between regions (e.g., New York to London), you must run a separate cluster of \"MirrorMaker 2\" workers. This is complex to manage and monitor.</li> <li>Confluent: Cluster Linking is built into the brokers. You can \"link\" two clusters and mirror topics with a simple configuration, preserving offsets and reducing operational complexity.</li> </ul>"},{"location":"kafka/kafka/#4-deployment-models","title":"4. Deployment Models","text":""},{"location":"kafka/kafka/#apache-kafka","title":"Apache Kafka","text":"<ul> <li>Self-Managed: You provision VMs (EC2, etc.) or bare metal.</li> <li>Docker/K8s: You write your own Helm charts or use the Strimzi operator (Open Source).</li> <li>Responsibility: You are on the hook for disk balancing, OS patching, Zookeeper management, and 2am outages.</li> </ul>"},{"location":"kafka/kafka/#confluent-platform-self-hosted-software","title":"Confluent Platform (Self-Hosted Software)","text":"<ul> <li>Enterprise Software: You download the Confluent distribution and run it on your own servers (On-prem or Cloud).</li> <li>Automation: Includes Confluent for Kubernetes (CFK) and Ansible playbooks to automate upgrades and scaling.</li> </ul>"},{"location":"kafka/kafka/#confluent-cloud-saas","title":"Confluent Cloud (SaaS)","text":"<ul> <li>Serverless: You don't see brokers. You just create topics and produce data.</li> <li>Kora Engine: A rewritten Kafka engine (10x faster) that is elastic. It scales up/down automatically based on traffic.</li> <li>SLA: 99.99% uptime guarantee.</li> </ul>"},{"location":"kafka/kafka/#5-which-one-should-you-choose","title":"5. Which one should you choose?","text":""},{"location":"kafka/kafka/#choose-apache-kafka-if","title":"Choose Apache Kafka if:","text":"<ul> <li>\u2705 You have a strong team of DevOps/Java engineers who know Kafka internals.</li> <li>\u2705 You have a strict $0 software budget (CapEx) but have budget for engineering hours (OpEx).</li> <li>\u2705 You are building a non-critical internal tool or a small-scale POC.</li> <li>\u2705 You need complete control over every configuration parameter.</li> </ul>"},{"location":"kafka/kafka/#choose-confluent-if","title":"Choose Confluent if:","text":"<ul> <li>\u2705 You are handling critical data (Payments, Customer 360, Fraud Detection).</li> <li>\u2705 You need \"batteries included\" (Connectors to S3, Snowflake, Mongo without coding).</li> <li>\u2705 You need strict security (RBAC, Audit Logs) for compliance (GDPR, HIPAA, SOC2).</li> <li>\u2705 You want to focus on using data, not managing Zookeeper and broker failures.</li> </ul>"},{"location":"kafka/kafka/#confluent-kafka-cp-kafka780-ports-monitoring-guide","title":"Confluent Kafka (cp-kafka:7.8.0) Ports &amp; Monitoring Guide","text":"<p>This document summarizes the exported ports for the <code>confluentinc/cp-kafka:7.8.0</code> Docker image, with a deep dive into the specific monitoring ports (9997 vs 9101).</p>"},{"location":"kafka/kafka/#1-standard-exported-ports","title":"1. Standard Exported Ports","text":"<p>The following are the standard ports used when deploying Confluent Platform 7.8.0 via Docker.</p> Port Protocol Description 9092 PLAINTEXT Client Port. Standard internal client connections. 29092 EXTERNAL External Port. (Convention) Used for host machine access. 9093 CONTROLLER KRaft Controller. Used for cluster orchestration (no Zookeeper). 9101 HTTP Prometheus Metrics. (JMX Exporter) Scrape endpoint. 9997 JMX/RMI Raw JMX. Direct Java management connection. 8090 HTTP MDS/REST. Metadata Service / Confluent Server API."},{"location":"kafka/kafka/#verification-command","title":"Verification Command","text":"<p>To see exactly what your container is exposing, run: </p><pre><code>docker run --rm confluentinc/cp-kafka:7.8.0 cat /etc/kafka/server.properties | grep port\n</code></pre>"},{"location":"kafka/kafka/#comprehensive-explanation-of-kafka-docker-configuration","title":"Comprehensive Explanation of Kafka Docker Configuration","text":"<p>This document explains every line and environment variable in your <code>docker-compose</code> snippet for <code>kafka1</code>.</p>"},{"location":"kafka/kafka/#1-container-basics","title":"1. Container Basics","text":"<p>These settings define how Docker runs the container itself.</p> <ul> <li><code>image: confluentinc/cp-kafka:7.8.0</code><ul> <li>Uses Confluent's distribution of Apache Kafka, version 7.8.0.</li> </ul> </li> <li><code>hostname: kafka1</code><ul> <li>Sets the internal network hostname of the container to <code>kafka1</code>. Other containers in the same network can reach it using this name.</li> </ul> </li> <li><code>container_name: kafka1</code><ul> <li>A static name for the container, making it easier to manage via CLI (e.g., <code>docker logs kafka1</code>).</li> </ul> </li> <li><code>ports</code><ul> <li><code>\"9092:9092\"</code>: The standard port for client traffic (Producers/Consumers).</li> <li><code>\"9093:9093\"</code>: The port used for the KRaft Controller (internal cluster voting).</li> <li><code>\"9997:9997\"</code>: The port for JMX monitoring (metrics).</li> </ul> </li> <li><code>networks: - kafka-net</code><ul> <li>Attaches this container to a custom bridge network called <code>kafka-net</code>, allowing it to resolve <code>kafka2</code> and <code>kafka3</code> by name.</li> </ul> </li> </ul>"},{"location":"kafka/kafka/#2-core-identity-roles-kraft-mode","title":"2. Core Identity &amp; Roles (KRaft Mode)","text":"<p>In KRaft mode, Kafka does not use ZooKeeper. Instead, brokers manage their own metadata using a Raft quorum.</p> <ul> <li><code>KAFKA_NODE_ID: 1</code><ul> <li>A unique integer identifier for this specific node in the cluster. Every node must have a different ID.</li> </ul> </li> <li><code>KAFKA_BROKER_ID: 1</code><ul> <li>Legacy synonym for <code>NODE_ID</code>. In newer KRaft versions, <code>NODE_ID</code> is preferred, but keeping both ensures compatibility.</li> </ul> </li> <li><code>KAFKA_PROCESS_ROLES: 'broker,controller'</code><ul> <li>Broker: This node handles data (produces/consumes).</li> <li>Controller: This node participates in the metadata consensus (voting on leader election).</li> <li>Note: In large production clusters, these roles are often separated. Here, the node does both (hyper-converged).</li> </ul> </li> <li><code>KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:9093,2@kafka2:9093,3@kafka3:9093'</code><ul> <li>Defines the entire \"voting board\" for the cluster.</li> <li>Format: <code>nodeID@host:port</code>.</li> <li>It tells <code>kafka1</code> that there are 3 voters total (itself, kafka2, and kafka3) communicating on port 9093.</li> </ul> </li> <li><code>CLUSTER_ID: 'EmptNWtoR4GGWx-BH6nGLQ'</code><ul> <li>A unique UUID string that acts as a password for the cluster.</li> <li>All nodes must have the exact same Cluster ID to successfully join and form a cluster.</li> </ul> </li> </ul>"},{"location":"kafka/kafka/#3-networking-listeners","title":"3. Networking &amp; Listeners","text":"<p>This is often the most complex part of Kafka configuration. It defines \"where I listen\" and \"how people find me.\"</p> <ul> <li><code>KAFKA_LISTENERS: 'PLAINTEXT://kafka1:9092,CONTROLLER://kafka1:9093'</code><ul> <li>Binds ports. It tells the process: \"Open a socket on port 9092 for standard traffic and 9093 for controller traffic.\"</li> </ul> </li> <li><code>KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka1:9092'</code><ul> <li>The \"Business Card\". This is the address the broker sends back to clients (producers/consumers).</li> <li>When a client connects, the broker says: \"If you want to write data to me, contact <code>kafka1</code> on port <code>9092</code>.\"</li> <li>Important: This requires clients to be able to resolve the hostname <code>kafka1</code> (i.e., they must be inside the same Docker network).</li> </ul> </li> <li><code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'</code><ul> <li>Maps the listener names to security protocols.</li> <li><code>CONTROLLER</code>: Uses <code>PLAINTEXT</code> (unencrypted) for internal voting.</li> <li><code>PLAINTEXT</code>: Uses <code>PLAINTEXT</code> (unencrypted) for client data.</li> </ul> </li> <li><code>KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'</code><ul> <li>Explicitly tells Kafka which listener name from the list above is reserved strictly for the KRaft controller metadata traffic.</li> </ul> </li> <li><code>KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'</code><ul> <li>Tells the broker: \"When you need to replicate data to other brokers (kafka2, kafka3), use the listener named <code>PLAINTEXT</code>.\"</li> </ul> </li> </ul>"},{"location":"kafka/kafka/#4-replication-reliability","title":"4. Replication &amp; Reliability","text":"<p>These settings control data safety and availability.</p> <ul> <li><code>KAFKA_DEFAULT_REPLICATION_FACTOR: 3</code><ul> <li>When a user creates a new topic without specifying details, create 3 copies of the data (one on each node). This ensures High Availability.</li> </ul> </li> <li><code>KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3</code><ul> <li>The <code>__consumer_offsets</code> topic stores where every consumer group is currently reading. Setting this to 3 ensures that if a node dies, consumer progress is not lost.</li> </ul> </li> <li><code>KAFKA_MIN_INSYNC_REPLICAS: 2</code><ul> <li>Safety Gate. If a producer sends a message with <code>acks=all</code>, at least 2 replicas (e.g., the leader + 1 follower) must acknowledge receipt before the write is considered successful.</li> <li>Prevents data loss if only 1 node is alive.</li> </ul> </li> <li><code>KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0</code><ul> <li>Optimization for development. It tells the Group Coordinator strictly not to wait before rebalancing consumer groups. In production, a small delay (e.g., 3000ms) prevents \"rebalance storms\" when brokers restart.</li> </ul> </li> <li><code>KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1</code></li> <li><code>KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1</code><ul> <li>Configuration Note: These settings define the reliability of Kafka Transactions (Exactly-Once Semantics).</li> <li>Warning: Setting this to <code>1</code> is risky for production but acceptable for local testing to save resources. In production, these should usually match your default replication (e.g., 3).</li> </ul> </li> </ul>"},{"location":"kafka/kafka/#5-monitoring-jmx","title":"5. Monitoring (JMX)","text":"<p>These settings enable tools like VisualVM, Datadog, or Prometheus to monitor the broker's health.</p> <ul> <li><code>KAFKA_JMX_PORT: 9997</code><ul> <li>Opens port 9997 for JMX connections.</li> </ul> </li> <li><code>KAFKA_JMX_OPTS: ...</code><ul> <li>Standard Java options to configure the JMX remote agent:<ul> <li><code>-Dcom.sun.management.jmxremote</code>: Enable remote JMX.</li> <li><code>authenticate=false</code> / <code>ssl=false</code>: Security Warning. Disables login and encryption for metrics. Safe for local Docker, unsafe for public internet.</li> <li><code>java.rmi.server.hostname=kafka1</code>: Crucial. Tells the JMX registry to advertise the hostname <code>kafka1</code> so remote tools can find the return path.</li> </ul> </li> </ul> </li> </ul>"},{"location":"kafka/kafka/#6-storage-volumes","title":"6. Storage &amp; Volumes","text":"<ul> <li><code>KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'</code><ul> <li>The internal path inside the container where Kafka writes its data segments.</li> <li>Note: The naming \"kraft-combined-logs\" implies it stores both Metadata logs (Controller) and Data logs (Broker) in the same directory structure.</li> </ul> </li> <li><code>volumes: - kafka-cluster:/var/lib/kafka/data</code><ul> <li>Correction/Conflict: In the environment variable above, you set the log dir to <code>/tmp/kraft-combined-logs</code>, but here you are mounting a volume to <code>/var/lib/kafka/data</code>.</li> <li>Fix: If you want your data to persist after the container restarts, you must ensure <code>KAFKA_LOG_DIRS</code> points to the mounted volume path.</li> <li>Recommended Change: Set <code>KAFKA_LOG_DIRS: '/var/lib/kafka/data'</code> so data is actually stored in the Docker volume <code>kafka-cluster</code>.</li> </ul> </li> </ul>"},{"location":"kafka/links/","title":"links","text":""},{"location":"kafka/links/#kafka-basics","title":"Kafka basics","text":""},{"location":"kafka/links/#youtube","title":"Youtube","text":""},{"location":"kafka/links/#introduction","title":"Introduction","text":"<ul> <li>Apache Kafka: What it is and where it's going.</li> <li>What makes Kafka special? | System Design</li> <li>3 Important Use Cases Why The Industry Is Using Kafka</li> <li>MASTER Apache Kafka in just 8 minutes to Build Real-Time Apps</li> </ul>"},{"location":"kafka/links/#playlists","title":"Playlists","text":"<ul> <li>ByteByteGo</li> <li>Apache Kafka Fundamentals You Should Know</li> <li>System Design: Apache Kafka In 3 Minutes</li> <li>System Design: Why is Kafka fast?</li> <li> <p>Top Kafka Use Cases You Should Know</p> </li> <li> <p>Piyush Garg</p> </li> <li>Why do we need Kafka?</li> <li> <p>Apache Kafka Crash Course | What is Kafka? | Piyush Garg</p> </li> <li> <p>Kafka for beginners | Java Techie</p> </li> <li> <p>DataCouch</p> </li> <li>Kafka Architecture in Depth | Apache Kafka Architecture | Understanding Kafka Architecture | DataCouch</li> <li> <p>Exploring Kafka Internals | Kafka | DataCouch</p> </li> <li> <p>Apache Kafka Tutorials by Anton Putra</p> </li> <li> <p>Confluent</p> </li> <li>Data Architecture Basics with Adam Bellemare</li> <li>Event Sourcing and Event Storage with Apache Kafka\u00ae | Event Sourcing 101</li> <li>Course | Apache Kafka Fundamentals</li> <li>Apache Kafka 101: Confluent's Flagship Course on Apache Kafka\u00ae Fundamentals ft. Tim Berglund</li> <li>Designing Event-Driven Microservices</li> <li>Apache Kafka\u00ae Architecture | Kafka's Internal Components and How They Work</li> <li> <p>Schema Registry 101 | Introduction to Schema Registry in Apache Kafka and How it Works</p> </li> <li> <p>Event-Driven Microservices in Banking: Best practices for building reliable distributed systems and optimizing customer satisfaction</p> </li> <li> <p>Spring Framework and Apache Kafka\u00ae Tutorials | Spring for Apache Kafka 101</p> </li> <li> <p>Designing Events and Event Streams | Apache Kafka\u00ae Event Streaming</p> </li> <li>Kafka Streams Tutorials | Kafka Streams 101 (2023)</li> <li>Kafka Connect Tutorials | Kafka Connect 101 (2023)</li> <li>Data Mesh and Data Domains Tutorials | Data Mesh 101</li> <li>Building Data Pipelines with Apache Kafka\u00ae and Confluent | Data Pipelines Tutorials</li> <li> <p>ksqlDB and Stream Processing Tutorials | ksqlDB 101</p> </li> <li> <p>Apache Flink 101</p> </li> <li>Getting Started with Apache Flink\u00ae</li> <li>Building Apache Flink Applications in Java</li> </ul>"},{"location":"kafka/links/#comparision","title":"Comparision","text":"<ul> <li>Apache Kafka vs message queue explained</li> <li>What's Stream Processing + When Do We Use It? | Systems Design Interview 0 to 1 with Ex-Google SWE</li> <li>What is Apache Flink\u00ae?</li> <li>Apache Kafka Vs. Apache Flink</li> <li>Apache Flink - A Must-Have For Your Streams | Systems Design Interview 0 to 1 With Ex-Google SWE</li> <li>Flink - Exactly Once Processing? | Distributed Systems Deep Dives With Ex-Google SWE</li> <li>Is Kafka OUTDATED? Could RedPanda Be the Future of Event Streaming?</li> <li>Kafka vs Redpanda Performance - Do the claims add up?</li> </ul>"},{"location":"kafka/links/#udemy","title":"Udemy","text":"<ul> <li>Apache Kafka for absolute beginners</li> <li>Java Microservices: CQRS &amp; Event Sourcing with Kafka</li> <li>Apache Kafka Series - Learn Apache Kafka for Beginners v3</li> </ul>"},{"location":"kafka/links/#blogs","title":"Blogs","text":"<ul> <li>Apache Kafka vs Confluent Kafka</li> </ul>"},{"location":"kafka/links/#freecodecamp","title":"FreeCodeCamp","text":"<ul> <li>How to Build a Real-Time Notification System with Go and Kafka</li> </ul>"},{"location":"kafka/links/#medium","title":"Medium","text":""},{"location":"kafka/links/#interview-question","title":"Interview question","text":"<ul> <li>Comprehensive List of Kafka Interview Questions(200+)</li> <li>Mastering Apache Kafka: Top 15 Key Interview Questions, Answers, and Practical Examples</li> <li>Top Kafka Interview Questions for Experienced Professionals in 2025 to 2026| Part 1</li> <li>From Scratch: Kafka Producer and Consumer Example with Spring Boot| Part 2</li> <li>In Depth Explanation : Apache Kafka Key Terminologies and Architecture Internal Working | Features | Part 3</li> <li>Interview Prep: Kafka vs ActiveMQ vs RabbitMQ \u2014 Features, Use Cases &amp; Q&amp;A| Part 4</li> <li>Kafka Scenario-Based Interview Questions: 10 Real-World Challenges You Must Be Ready For</li> </ul>"},{"location":"kafka/links/#apache-kafka","title":"Apache Kafka","text":"<ul> <li>Apache Kafka: The Complete Guide to Distributed Event Streaming</li> <li>Kafka Is Hard Because You Keep Ignoring These Patterns</li> <li>The Kafka Rebalancing Issue That Taught Me Everything About Distributed Systems</li> <li>Apache Kafka: The Beating Heart of Real-Time Systems (with Uber &amp; Netflix Examples)</li> <li>Kafka</li> <li>Managing Kafka Consumer Lag and Offsets</li> <li> <p>Extending Kafka's Exactly-Once Semantics to External Systems</p> </li> <li> <p>Building the Foundation \u2014 Initial Project Setup for Our Kafka Event Pipeline \u2014 Day 1</p> </li> </ul>"},{"location":"kafka/links/#kafka-consumer","title":"Kafka consumer","text":"<ul> <li>Kafka Consumers \u2014 Full Deep Dive (basic \u2192 advanced)</li> </ul>"},{"location":"kafka/links/#kafka-with-golang","title":"Kafka with Golang","text":"<ul> <li>Kafka with confluent-kafka-go: A Go Developer's Playbook</li> <li>Kafka Producers and Consumers With Golang</li> <li>Go &amp; Kafka: From Theory to Practice</li> <li>Kafka Integration in Go Using Sarama: Producer and Consumer Setup</li> <li>Delaying and Aggregating Kafka Messages with Goroutines and Caching</li> </ul>"},{"location":"kafka/links/#kafka-with-spring-boot","title":"Kafka with Spring boot","text":"<ul> <li>Kafka Consumer Strategies: Spring Boot Kafka Listener vs. Kafka Streams Topology</li> <li>Scaling Spring Boot Microservices with Kafka: A Banking Case Study</li> <li>How I Implemented Kafka Delayed Retries \u2014 and Silenced Our New Relic Alerts Once and for All</li> <li>Kafka Error Handling with Spring Boot: A Complete Guide to Robust Message Processing</li> <li> <p>Kafka Retry &amp; DLT in Spring Boot: A Practical Guide</p> </li> <li> <p>Kafka, Schema Registry, JUnit and Test Containers \u2014 Part I</p> </li> <li>Kafka, Schema Registry, JUnit and Test Containers \u2014 Part II: Creating a Kafka Cluster Test Extension</li> <li>Kafka, Schema Registry, JUnit and Test Containers \u2014 Part III: Reducing test time by removing all topics between tests</li> </ul>"},{"location":"kafka/links/#kafka-kraft","title":"Kafka Kraft","text":"<ul> <li>Understanding Kafka KRaft: How Controllers and Brokers Talk in the Zookeeper-less World</li> <li>Docker Compose for Running Kafka in Kraft Mode<ul> <li>katyagorshkova/kafka-kraft</li> </ul> </li> </ul>"},{"location":"kafka/links/#kafka-schema-registry","title":"kafka schema registry","text":"<ul> <li>Introduction to Schema Registry in Kafka</li> <li>Schema Registry for Kafka</li> <li>Handling Schema Evolution in Kafka Connect: Patterns, Pitfalls, and Practices</li> <li>Schema Registry Demystified: Why It Matters and How to Use Avro with Spring Boot + Kafka</li> <li>Kafka Schema Registry in Distributed Systems</li> <li>The Schema Registry API is not how you use schema with Kafka!</li> <li>Schema Registry and Confluent Kafka</li> <li>A Guide to Kafka Schema Registry: Building with Gradle and Avro</li> </ul>"},{"location":"kafka/links/#kafka-ui","title":"Kafka UI","text":"<ul> <li>Tool \u2699\ufe0f: UI for Apache Kafka \u2014 Clean, Modern, and Easy to Use</li> <li>you should switch to Kafbat UI, basically a fork of Kafka UI after it was deprecated:</li> <li>provectus/kafka-ui</li> <li>kafbat/kafka-ui</li> </ul>"},{"location":"kafka/links/#kafka-connect","title":"Kafka Connect","text":"<ul> <li>Kafka Connect: Understanding Workers and Converters</li> </ul>"},{"location":"kafka/links/#kafka-streams","title":"Kafka Streams","text":"<ul> <li>Developer Guide to Achieve Transactional Processing in Kafka Streams</li> </ul>"},{"location":"kafka/links/#kafka-on-docker","title":"Kafka on docker","text":"<ul> <li>Setting Up a Kafka Cluster Using Docker Compose(Kraft Mode): A Step-by-Step Guide</li> <li>Introduction to Apache Kafka: A Hands-On Guide with Docker</li> <li>Running Kafka in a Docker Container,with KafkaUI and Minio</li> <li>experientlabs-data-engg/streaming-data-dp-201</li> <li>Docker Compose example for Kafka, Zookeeper, and Schema Registry</li> </ul>"},{"location":"kafka/links/#kafka-cluster-on-kubernetes","title":"Kafka cluster on kubernetes","text":"<ul> <li>Deploying Apache Kafka on Kubernetes with KRaft Mode: A Complete Guide</li> <li>Kafka Cluster Deployment on Kubernetes</li> <li>Simplifying Streaming Architectures: Apache Kafka, KRaft, Kafka Connect and MySQL on Kubernetes</li> <li>soumeng03/kafka-kraft-mysql-connect</li> <li>Kafka on Kubernetes</li> </ul>"},{"location":"kafka/links/#kafka-on-aws","title":"Kafka on AWS","text":"<ul> <li>Self-Managed Kafka Cluster on AWS EC2 with Zookeeper and Multi-Broker Setup</li> </ul>"},{"location":"kafka/links/#github-links","title":"Github links","text":""},{"location":"kafka/links/#kafka-ui_1","title":"Kafka UI","text":"<ul> <li>About Kafbat UI (UI for Apache Kafka)</li> <li>kafbat/kafka-ui/blob/main/documentation/compose/DOCKER_COMPOSE.md</li> <li>kafbat/kafka-ui/blob/main/documentation/compose/kafbat-ui.yaml</li> </ul>"},{"location":"mongodb/","title":"index","text":""},{"location":"mongodb/#introduction","title":"Introduction","text":"<p>Why Indexes?</p> <p>An index can speed up our find update and delete query. If our query is like <code>db.products.find({ seller : \"Max\" })</code> then MongoDB will search for the entire collection for the seller name <code>\"Max\"</code>, which is also called as <code>COLLSCAN</code> and this can take a while if there is million record. </p> <p>So, in that case we can create a <code>Index</code> on <code>Selle</code>r field. MongoDB will create an <code>Ordered</code> list with all the values of the <code>Seller</code>s and all the items of this list will have a pointer to the actual document in the collection.  Now if we run the exact query then Mongodb will see that there is an <code>Index</code> on Seller so MongoDB will run <code>IXSCAN</code> and directly jump to <code>\"M\"</code> which will speed up the querying.</p> <p>But we should not overdo the indexes. If we can index on all fields, then it will certainly improve the performance for the find query but for the <code>insert</code> query it will slow down. As now it will again have to update the Ordered list for every field index for every insert and update.</p> <p>To see the all the index present on the collection: </p><pre><code>&gt; db.infos.getIndexes()\n[ { \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\" } ]\n</code></pre> <p>By default, mongodb will create an index on <code>_id</code> field. To create an index on specific fields: </p><pre><code>db.infos.createIndex( { \"dob.age\" : 1} } )\n\ndb.infos.createIndex( { \"dob.age\" : -1} } )\n</code></pre> <code>1</code> means increasing and <code>-1</code> means decreasing Though that does not matter as mongoDB can traverse both ways.  <p>We can also create index with more than on field. The order matters here. </p><pre><code>db.infos.createIndex( { \"email\" : 1, \"dob.age\" : 1} } )\n</code></pre> This means that mongoDb will create a <code>compound index</code> and first the <code>index</code> with email then <code>dob.age</code> Example: (a@test.com,23) will come before (a.@test.com,24)  <p>To drop the index use: </p><pre><code>&gt;db.infos.dropIndex({ \"dob.age\": 1 })\n{ \"nIndexesWas\" : 2, \"ok\" : 1 }\n</code></pre> <p>We can also drop index by name.  </p><pre><code>&gt; db.infos.dropIndex(\"dob.age_1\")\n{ \"nIndexesWas\" : 2, \"ok\" : 1 }\n</code></pre>"},{"location":"mongodb/#query-explain","title":"Query Explain","text":"<p>To analyse how a query will execute mongodb has a unique method that is explain(). </p><pre><code>&gt; db.infos.explain().find( { \"dob.age\" : { $gt : 60 }} )\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"queryHash\" : \"FC9E47D2\",\n                \"planCacheKey\" : \"A5FF588D\",\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"COLLSCAN\",\n                        \"filter\" : {\n                                \"dob.age\" : {\n                                        \"$gt\" : 60\n                                }\n                        },\n                        \"direction\" : \"forward\"\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"$db\" : \"persons\"\n        },\n\n        \"ok\" : 1\n}\n</code></pre> <p>In the winning plan we can see <code>COLLSCAN</code> as mongodb searched the entire collection for this query. There is also a rejected plans array but currently it is empty as mongodb has no other option than searching the entire array.</p> <p>We can also add additional properties in explain(). It will print some additional information </p><pre><code>&gt; db.infos.explain(\"executionStats\").find( { \"dob.age\" : { $gt : 60 }} )\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"COLLSCAN\",\n                        \"filter\" : {\n                                \"dob.age\" : {\n                                        \"$gt\" : 60\n                                }\n                        },\n                        \"direction\" : \"forward\"\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"executionStats\" : {\n                \"executionSuccess\" : true,\n                \"nReturned\" : 1222,\n                \"executionTimeMillis\" : 3,\n                \"totalKeysExamined\" : 0,\n                \"totalDocsExamined\" : 5000,\n                \"executionStages\" : {\n                        \"stage\" : \"COLLSCAN\",\n                        \"filter\" : {\n                                \"dob.age\" : {\n                                        \"$gt\" : 60\n                                }\n                        },\n                        \"nReturned\" : 1222,\n                        \"executionTimeMillisEstimate\" : 0,\n                        \"works\" : 5002,\n                        \"advanced\" : 1222,\n                        \"needTime\" : 3779,\n                        \"needYield\" : 0,\n                        \"saveState\" : 5,\n                        \"restoreState\" : 5,\n                        \"isEOF\" : 1,\n                        \"direction\" : \"forward\",\n                        \"docsExamined\" : 5000\n                }\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"$db\" : \"persons\"\n        },\n\n        \"ok\" : 1\n}\n</code></pre> <p>Here we can see some other additional informations like totalDocumentScan, totalDocumentReturn, executionTimeMillis.</p> <p>Now if we do the indexing on dob.age and run the same query with explain </p><pre><code>&gt; db.infos.createIndex( { \"dob.age\" : 1} )\n{\n        \"numIndexesBefore\" : 1,\n        \"numIndexesAfter\" : 2,\n        \"createdCollectionAutomatically\" : false,\n        \"ok\" : 1\n}\n\n&gt; db.infos.explain(\"executionStats\").find( { \"dob.age\" : { $gt : 60 }} )\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"FETCH\",\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"(60.0, inf.0]\"\n                                        ]\n                                }\n                        }\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"executionStats\" : {\n                \"executionSuccess\" : true,\n                \"nReturned\" : 1222,\n                \"executionTimeMillis\" : 50,\n                \"totalKeysExamined\" : 1222,\n                \"totalDocsExamined\" : 1222,\n                \"executionStages\" : {\n                        \"stage\" : \"FETCH\",\n                        \"nReturned\" : 1222,\n                        \"executionTimeMillisEstimate\" : 0,\n                        \"works\" : 1223,\n                        \"advanced\" : 1222,\n                        \"needTime\" : 0,\n                        \"needYield\" : 0,\n                        \"saveState\" : 1,\n                        \"restoreState\" : 1,\n                        \"isEOF\" : 1,\n                        \"docsExamined\" : 1222,\n                        \"alreadyHasObj\" : 0,\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"nReturned\" : 1222,\n                                \"executionTimeMillisEstimate\" : 0,\n                                \"works\" : 1223,\n                                \"advanced\" : 1222,\n                                \"needTime\" : 0,\n                                \"needYield\" : 0,\n                                \"saveState\" : 1,\n                                \"restoreState\" : 1,\n                                \"isEOF\" : 1,\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"(60.0, inf.0]\"\n                                        ]\n                                },\n                                \"keysExamined\" : 1222,\n                                \"seeks\" : 1,\n                                \"dupsTested\" : 0,\n                                \"dupsDropped\" : 0\n                        }\n                }\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"dob.age\" : {\n                                \"$gt\" : 60\n                        }\n                },\n                \"$db\" : \"persons\"\n        },\n\n        \"ok\" : 1\n}\n</code></pre> Now the query did not search for the entire collection it has done an <code>IXSCAN</code>."},{"location":"mongodb/#indexes-behind-the-scenes","title":"Indexes Behind the Scenes","text":"<p>What does createIndex() do in detail?</p> <p>Whilst we can't really see the index, you can think of the index as a simple list of values + pointers to the original document. Something like this (for the \"age\" field):</p> <ul> <li>(29, \"address in memory/ collection a1\")</li> <li>(30, \"address in memory/ collection a2\")</li> <li>(33, \"address in memory/ collection a3\")</li> </ul> <p>The documents in the collection would be at the \"addresses\" a1, a2 and a3. The order does not have to match the order in the index (and most likely, it indeed won't).</p> <p>The important thing is that the index items are ordered (ascending or descending - depending on how you created the index). </p> <p><code>createIndex({age: 1})</code> creates an index with ascending sorting, <code>createIndex({age: -1})</code> creates one with descending sorting.</p> <p>MongoDB is now able to quickly find a fitting document when you filter for its age as it has a sorted list. Sorted lists are way quicker to search because you can skip entire ranges (and don't have to look at every single document).</p> <p>Additionally, sorting (via sort(...)) will also be sped up because you already have a sorted list. Of course, this is only true when sorting for the age.</p> <p>Let's say all our document has <code>age</code> greater than 50 and in query <code>[db.infos.find({\"dob.age\": { $gt: 20 }})]</code> we are trying to find the documents greater than <code>20</code> so it will return all the documents. So, in this case IXSCAN has the less performance as it will introduce an extra step. As at first the mongodb will scan the entire index then it will go to the actual mongodb collection. If we delete the index, then it will again search with COLSCAN and eventually that will have a better performance. So, it is recommended that only to use index when the query will return a <code>small subset</code> of the actual collection. </p> <p>Index on Boolean value does not make much sense.</p>"},{"location":"mongodb/#compound-index","title":"Compound index","text":"<p>first let's create a compound index </p><pre><code>&gt; db.infos.createIndex({ \"dob.age\" : 1, \"gender\" : 1})\n{\n    \"numIndexesBefore\" : 1,\n    \"numIndexesAfter\" : 2,\n    \"createdCollectionAutomatically\" : false,\n    \"ok\" : 1\n}\n</code></pre> <p>If we search with dob.age and gender then mongodb will use this compound index. </p><pre><code>&gt; db.infos.explain(\"executionStats\").find({\"dob.age\" : 35, \"gender\" : \"male\"})\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"$and\" : [\n                                {\n                                        \"dob.age\" : {\n                                                \"$eq\" : 35\n                                        }\n                                },\n                                {\n                                        \"gender\" : {\n                                                \"$eq\" : \"male\"\n                                        }\n                                }\n                        ]\n                },\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"FETCH\",\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1,\n                                        \"gender\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1_gender_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ],\n                                        \"gender\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"[35.0, 35.0]\"\n                                        ],\n                                        \"gender\" : [\n                                                \"[\\\"male\\\", \\\"male\\\"]\"\n                                        ]\n                                }\n                        }\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"executionStats\" : {\n                \"executionSuccess\" : true,\n                \"nReturned\" : 43,\n                \"executionTimeMillis\" : 19,\n                \"totalKeysExamined\" : 43,\n                \"totalDocsExamined\" : 43,\n                \"executionStages\" : {\n                        \"stage\" : \"FETCH\",\n                        \"nReturned\" : 43,\n                        \"executionTimeMillisEstimate\" : 11,\n                        \"works\" : 44,\n                        \"advanced\" : 43,\n                        \"needTime\" : 0,\n                        \"needYield\" : 0,\n                        \"saveState\" : 1,\n                        \"restoreState\" : 1,\n                        \"isEOF\" : 1,\n                        \"docsExamined\" : 43,\n                        \"alreadyHasObj\" : 0,\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"nReturned\" : 43,\n                                \"executionTimeMillisEstimate\" : 11,\n                                \"works\" : 44,\n                                \"advanced\" : 43,\n                                \"needTime\" : 0,\n                                \"needYield\" : 0,\n                                \"saveState\" : 1,\n                                \"restoreState\" : 1,\n                                \"isEOF\" : 1,\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1,\n                                        \"gender\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1_gender_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ],\n                                        \"gender\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"[35.0, 35.0]\"\n                                        ],\n                                        \"gender\" : [\n                                                \"[\\\"male\\\", \\\"male\\\"]\"\n                                        ]\n                                },\n                                \"keysExamined\" : 43,\n                                \"seeks\" : 1,\n                                \"dupsTested\" : 0,\n                                \"dupsDropped\" : 0\n                        }\n                }\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"dob.age\" : 35,\n                        \"gender\" : \"male\"\n                },\n                \"$db\" : \"persons\"\n        },\n}\n</code></pre> <p>If we just look for the age, then also mongodb will use this index as \"dob.age\" comes first in the index order.</p> <pre><code>&gt; db.infos.explain(\"executionStats\").find({\"dob.age\" : 35})\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"dob.age\" : {\n                                \"$eq\" : 35\n                        }\n                },\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"FETCH\",\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1,\n                                        \"gender\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1_gender_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ],\n                                        \"gender\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"[35.0, 35.0]\"\n                                        ],\n                                        \"gender\" : [\n                                                \"[MinKey, MaxKey]\"\n                                        ]\n                                }\n                        }\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"executionStats\" : {\n                \"executionSuccess\" : true,\n                \"nReturned\" : 95,\n                \"executionTimeMillis\" : 0,\n                \"totalKeysExamined\" : 95,\n                \"totalDocsExamined\" : 95,\n                \"executionStages\" : {\n                        \"stage\" : \"FETCH\",\n                        \"nReturned\" : 95,\n                        \"executionTimeMillisEstimate\" : 0,\n                        \"works\" : 96,\n                        \"advanced\" : 95,\n                        \"needTime\" : 0,\n                        \"needYield\" : 0,\n                        \"saveState\" : 0,\n                        \"restoreState\" : 0,\n                        \"isEOF\" : 1,\n                        \"docsExamined\" : 95,\n                        \"alreadyHasObj\" : 0,\n                        \"inputStage\" : {\n                                \"stage\" : \"IXSCAN\",\n                                \"nReturned\" : 95,\n                                \"executionTimeMillisEstimate\" : 0,\n                                \"works\" : 96,\n                                \"advanced\" : 95,\n                                \"needTime\" : 0,\n                                \"needYield\" : 0,\n                                \"saveState\" : 0,\n                                \"restoreState\" : 0,\n                                \"isEOF\" : 1,\n                                \"keyPattern\" : {\n                                        \"dob.age\" : 1,\n                                        \"gender\" : 1\n                                },\n                                \"indexName\" : \"dob.age_1_gender_1\",\n                                \"isMultiKey\" : false,\n                                \"multiKeyPaths\" : {\n                                        \"dob.age\" : [ ],\n                                        \"gender\" : [ ]\n                                },\n                                \"isUnique\" : false,\n                                \"isSparse\" : false,\n                                \"isPartial\" : false,\n                                \"indexVersion\" : 2,\n                                \"direction\" : \"forward\",\n                                \"indexBounds\" : {\n                                        \"dob.age\" : [\n                                                \"[35.0, 35.0]\"\n                                        ],\n                                        \"gender\" : [\n                                                \"[MinKey, MaxKey]\"\n                                        ]\n                                },\n                                \"keysExamined\" : 95,\n                                \"seeks\" : 1,\n                                \"dupsTested\" : 0,\n                                \"dupsDropped\" : 0\n                        }\n                }\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"dob.age\" : 35\n                },\n                \"$db\" : \"persons\"\n        }\n}\n</code></pre> <p>But if we only search will the gender then index has no use because gender is not sorted primarily. It is secondary sort on the dob.age. Here mongodb will use the full COLLSCAN. </p><pre><code>&gt; db.infos.explain(\"executionStats\").find({\"gender\" : \"male\"})\n{\n        \"explainVersion\" : \"1\",\n        \"queryPlanner\" : {\n                \"namespace\" : \"persons.infos\",\n                \"indexFilterSet\" : false,\n                \"parsedQuery\" : {\n                        \"gender\" : {\n                                \"$eq\" : \"male\"\n                        }\n                },\n                \"maxIndexedOrSolutionsReached\" : false,\n                \"maxIndexedAndSolutionsReached\" : false,\n                \"maxScansToExplodeReached\" : false,\n                \"winningPlan\" : {\n                        \"stage\" : \"COLLSCAN\",\n                        \"filter\" : {\n                                \"gender\" : {\n                                        \"$eq\" : \"male\"\n                                }\n                        },\n                        \"direction\" : \"forward\"\n                },\n                \"rejectedPlans\" : [ ]\n        },\n        \"executionStats\" : {\n                \"executionSuccess\" : true,\n                \"nReturned\" : 2435,\n                \"executionTimeMillis\" : 4,\n                \"totalKeysExamined\" : 0,\n                \"totalDocsExamined\" : 5000,\n                \"executionStages\" : {\n                        \"stage\" : \"COLLSCAN\",\n                        \"filter\" : {\n                                \"gender\" : {\n                                        \"$eq\" : \"male\"\n                                }\n                        },\n                        \"nReturned\" : 2435,\n                        \"executionTimeMillisEstimate\" : 0,\n                        \"works\" : 5002,\n                        \"advanced\" : 2435,\n                        \"needTime\" : 2566,\n                        \"needYield\" : 0,\n                        \"saveState\" : 5,\n                        \"restoreState\" : 5,\n                        \"isEOF\" : 1,\n                        \"direction\" : \"forward\",\n                        \"docsExamined\" : 5000\n                }\n        },\n        \"command\" : {\n                \"find\" : \"infos\",\n                \"filter\" : {\n                        \"gender\" : \"male\"\n                },\n                \"$db\" : \"persons\"\n        }\n}\n</code></pre> <p>Sorting with indexing:</p> <p>If we are sorting on any field and that field has an indexing, then mongodb will not sort it will directly use the indexed records as mongodb already has a sorted list on that field.</p> <p>If we are trying to sort on a large number of documents, then it will time out. MongoDB has a memory of <code>32 megabytes</code> of memory of sorting. By default, mongodb loads all the documents on its memory then it sorts on them. So, without indexing sometimes it is not possible to get the sorted documents.</p> <p>When we are creating any index on that time, we can specify that the index will be <code>unique</code> or not. By default, the indexing on <code>$id</code> holds unique criteria. </p><pre><code>&gt; db.infos.createIndex({ email : 1 }, { unique : true })\n</code></pre> <p>Before creating index if there is already any duplicate email available then it will throw an error. </p><pre><code>&gt; db.infos.createIndex({ email : 1 }, { unique : true })\n{\n        \"ok\" : 0,\n        \"errmsg\" : \"Index build failed: 8aff9b57-7fce-4ff9-8631-4f22c63ddaff: Collection persons.infos ( c6d8709f-2a51-4bda-ac9e-343a639304d6 ) :: caused by :: E11000 duplicate key error collection: persons.infos index: email_1 dup key: { email: \\\"abigail.clark@example.com\\\" }\",\n        \"code\" : 11000,\n        \"codeName\" : \"DuplicateKey\",\n        \"keyPattern\" : {\n                \"email\" : 1\n        },\n        \"keyValue\" : {\n                \"email\" : \"abigail.clark@example.com\"\n        }\n}\n</code></pre>"},{"location":"mongodb/#partial-filterindexing","title":"Partial filter/Indexing","text":"<p>We can always use compound indexing but the problem with the compound indexing is that it takes much space in discs. So, in that case we can use partial filter like if we know that gender male is frequently queried rather than female. So, we can create a partial index with gender male.</p> <p>Creating a partial index on gender <code>male</code> </p><pre><code>&gt; db.infos.createIndex({\"dob.age\" : 1}, {partialFilterExpression : {\"gender\" : 1}} )\n{\n        \"numIndexesBefore\" : 1,\n        \"numIndexesAfter\" : 2,\n        \"createdCollectionAutomatically\" : false,\n        \"ok\" : 1\n}\n</code></pre> <p>Getting all the index information </p><pre><code>&gt; db.infos.getIndexes()\n[\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"_id\" : 1\n        },\n        \"name\" : \"_id_\"\n    },\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"dob.age\" : 1\n        },\n        \"name\" : \"dob.age_1\",\n        \"partialFilterExpression\" : {\n            \"gender\" : 1\n        }\n    }\n]\n</code></pre> <p>Drawback of this partial filter is that now when we just query for the <code>\"dob.age\"</code> it will not use <code>IXSCAN</code> it will use the <code>COLLSCAN</code>. But if we also mention gender male then it will use the <code>IXSCAN</code>.</p> <p>Advantage of partial filter is that now the write query is more efficient as the size of the ordered list is small.</p> <p>If we have an index on <code>email</code> and <code>unique true</code> and if we enter document <code>without email</code> then mongodb will treat that document as email equal to <code>null</code>. Again, if we try to <code>insert</code> any document <code>without</code> email then mongoDB will <code>throw an exception</code> as email <code>null</code> is already stored in ordered list. We cannot add null value again.</p> <p>To allow this condition we can use unique true with partial filter expression. </p><pre><code>&gt; db.infos.createIndex({\"dob.age\" : 1}, {unique : true, partialFilterExpression : {\"email\" : {exists : true}}} )\n{\n        \"numIndexesBefore\" : 1,\n        \"numIndexesAfter\" : 2,\n        \"createdCollectionAutomatically\" : false,\n        \"ok\" : 1\n}\n</code></pre>"},{"location":"mongodb/#time-to-live-indexttl","title":"Time to live index(TTL)","text":"<p>It is only applicable for <code>date or timestamp</code>. With this indexing after certain time the document will automatically be <code>deleted</code>.</p> <p>If there is already some document and then we are adding this <code>index</code>, then at the time of index creation it will not check the existing documents. When we insert any new data then it will evaluate all the documents again and then it will use <code>TTL</code> index.</p> <pre><code>&gt; db.sessions.createIndex({ createdAt : 1} , {expireAfterSeconds : 10})\n\n&gt; db.sessions.insertOne({data : \"I am Abhishek\", createdAt : new Date()})\n{\n    \"acknowledged\" : true,\n    \"insertedId\" : ObjectId(\"62da72b385a6e4bfe5a374cb\")\n}\n\n&gt; db.sessions.findOne()\n{\n    \"_id\" : ObjectId(\"62da72b385a6e4bfe5a374cb\"),\n    \"data\" : \"I am Abhishek\",\n    \"createdAt\" : ISODate(\"2022-07-22T09:49:39.459Z\")\n}\n\n&gt; db.sessions.createIndex({ createdAt : 1} , {expireAfterSeconds : 10})\n{\n    \"numIndexesBefore\" : 1,\n    \"numIndexesAfter\" : 2,\n    \"createdCollectionAutomatically\" : false,\n    \"ok\" : 1\n}\n</code></pre> <p>Now with this index the documents will be delete after <code>10</code> seconds. This can be useful for session or carts in online shopping where the cart item automatically deletes after one day.</p>"},{"location":"mongodb/#query-diagnosis-and-query-planning","title":"Query Diagnosis and &amp; Query Planning","text":"<p>explain() method takes three type of string:</p> <ul> <li>\"queryPlanner\": Show summary for executed query and winning plan</li> <li>\"executionStats\": Show detailed summary for executed query and winning plan and rejected plans.</li> <li>\"allPlansExecution\": Show detailed summary for executed query and winning plan and winning plan decision process.</li> </ul> <p>For determining the query is efficient or not we must check following things:</p> <p>Processing time in milliseconds, no of keys examined (if index scan happened), No of documents examined, no of documents returned.</p> <p>The keys and documents examined should be close together and documents examined and returned should be closed or documents should be zero so that it looked at zero documents.  In a so-called covered query, it will be happening.</p> <p>Covered query:</p> <p>If we have an indexing on name and we are only querying for name on that time mongodb will not even look to the documents, instead it will directly return the name from the indexed ordered list.</p> <p>Example of this type of query is like: </p><pre><code>db.infos.findOne({ \"name\" : \"Abhishek\"}, { _id: 0, name : 1})\n</code></pre> <p>Suppose we have an index on <code>name</code> and another index one <code>age and name</code> (the ordering is important here). Now if we search for any document with <code>name and age</code> then mongodb will use the <code>compound index</code>, it will use the single index on <code>name</code>. If we do an <code>explain(\"executionStats\")</code> then <code>age_1_name_1</code> will fall under <code>winning plan</code> and <code>name_1</code> will fall under rejected plans.</p> <p>To find the <code>winning plan</code> mongodb check the query and available index then it will choose among them. So every time there is a query mongodb tries to find a <code>winning plan</code>, but again it will be having the extra step to find among all the plans. So mongodb save the winning plan in the caches for the query. This cache is not for forever. Mongodb resets the cache after db restarts, after few inserts or there is any rebuilt of index or changes in index.</p>"},{"location":"mongodb/#multikey-index","title":"Multikey index","text":"<p>We can also create indexes on array values. Let's say we are adding one document like this. </p><pre><code>&gt; db.infos.insertOne({\"name\" : \"Abhishek\", \"gender\" : \"male\", \"hobbies\" : [\"Sports\", \"Coding\"]})\n{\n        \"acknowledged\" : true,\n        \"insertedId\" : ObjectId(\"62dcaf7185a6e4bfe5a374cc\")\n}\n&gt; db.infos.createIndex({hobbies: 1})\n{\n        \"numIndexesBefore\" : 1,\n        \"numIndexesAfter\" : 2,\n        \"createdCollectionAutomatically\" : false,\n        \"ok\" : 1\n}\n&gt; db.infos.explain().find({hobbies : \"Coding\"})\n{\n    \"explainVersion\" : \"1\",\n    \"queryPlanner\" : {\n        \"namespace\" : \"persons.infos\",\n        \"indexFilterSet\" : false,\n        \"parsedQuery\" : {\n            \"hobbies\" : {\n                \"$eq\" : \"Coding\"\n            }\n        },\n        \"queryHash\" : \"895C9692\",\n        \"planCacheKey\" : \"439794C9\",\n        \"maxIndexedOrSolutionsReached\" : false,\n        \"maxIndexedAndSolutionsReached\" : false,\n        \"maxScansToExplodeReached\" : false,\n        \"winningPlan\" : {\n            \"stage\" : \"FETCH\",\n            \"inputStage\" : {\n                \"stage\" : \"IXSCAN\",\n                \"keyPattern\" : {\n                    \"hobbies\" : 1\n                },\n                \"indexName\" : \"hobbies_1\",\n                \"isMultiKey\" : true,\n                \"multiKeyPaths\" : {\n                    \"hobbies\" : [\n                        \"hobbies\"\n                    ]\n                },\n                \"isUnique\" : false,\n                \"isSparse\" : false,\n                \"isPartial\" : false,\n                \"indexVersion\" : 2,\n                \"direction\" : \"forward\",\n                \"indexBounds\" : {\n                    \"hobbies\" : [\n                        \"[\\\"Coding\\\", \\\"Coding\\\"]\"\n                    ]\n                }\n            }\n        },\n        \"rejectedPlans\" : [ ]\n    },\n    \"command\" : {\n        \"find\" : \"infos\",\n        \"filter\" : {\n            \"hobbies\" : \"Coding\"\n        },\n        \"$db\" : \"persons\"\n    },\n}\n</code></pre> <p>Here <code>multikey</code> is true.</p> <p>When we are creating index on array values. On that time there will be a ordered list with all the elements with array. It polls out all the elements of the array and stores as a separate element. So, it is larger than the size of the document.</p> <p>If the array consists of documents, then we have to query with that document otherwise it will not use <code>IXSCAN</code>. Suppose we have address array with <code>homeAddress</code> and we are creating index on arrays. </p><pre><code>{ \"address\": [\n    { \"homeAddress\" : \"18 No alep khan mahalla road\" }, \n    { \"homeAddress\" : \"Rameswara waterview block 1,4B\" }\n]}\n</code></pre> <p>Here we have to search like this: </p><pre><code>db.infos.find({\n    \"address\": { \"homeAddress\" : \"18 No alep khan mahalla road\" }\n})\n</code></pre> Otherwise indexing will not work. We can also use on <code>\"address.homeAddress\"</code>.  We can create compound indexes with multikey index like with name and address array.  It will do a <code>cartesian product</code> of the name and address values. Then it will store in the ordered list.  But we cannot create a compound index if both values are array."},{"location":"mongodb/#text-index","title":"Text index","text":"<p>If we search using regex that is very low in performance rather, we can use text indexes. Text string is just an array of words. So, mongodb stores the main keywords and removes the stop words like \"is\", \"the\", \"a\" etc. The main thing with text index it we can only create on index, which is type of text, because it is expensive to store all the keywords. If we have any criteria to use on both rather, we can create compound index of type text with the two fields. </p><pre><code>db.infos.createIndex({ \"description\" : \"text\" })\n</code></pre> We can not specify 1 or -1 while creating the index. <p>We can search like this. We cannot use regular queries. Text index is expensive, and we have to use it like this. </p><pre><code>db.infos.find({ \"$text\" : { \"$search\" : \"pretty\" }})\n</code></pre> <p>If we search for \"red book\" then the query <code>db.infos.find({ \"$text\" : { \"$search\" : \"red book\" }})</code> will not work as it will split the query string into multiple word, then it will search individually like it will search for red and it will search for book then it will combine the result. So, we have to use quotation mark around our query if we are searching for phrases. </p><pre><code>db.infos.find({ \"$text\" : { \"$search\" : \"\\\"red book\\\"\" }})\n</code></pre> <p>If we have more than one result, then behind the scenes mongodb assigns meta score to the documents. Higher the score means that the document matches with our query better. To see the score with have to project the score as well. </p><pre><code>db.infos.find(\n    { $text : { $search : \"awesome book\" }}, \n    { score : { $meta : \"textScore\" }}\n)\n</code></pre> <p>we can also show the results with sorted based on scores. </p><pre><code>db.infos.find(\n    { $text : { $search : \"awesome book\" }} , \n    { score : { $meta : \"textScore\" }}\n).sort({ \n    score : { $meta : \"textScore\" }\n})\n</code></pre> <p>It will be a <code>decreasing</code> type of sorting.  We can use more than field for text index. To drop an index of text, we must drop by name. </p><pre><code>db.infos.createIndex({ \n    title : \"text\" , \n    description : text \n})\n</code></pre> <p>it will create an index using the keywords of both fields. We can search like previous. Case does not matter for text index. </p><pre><code>db.infos.find({ \n    \"$text\" : { \"$search\" : \"pretty\" }\n})\n</code></pre> <p>We can also rule out for the specific words. </p><pre><code>db.infos.find({ \n    \"$text\" : { \"$search\" : \"pretty -books\" }\n})\n</code></pre> we have to add minus <code>(-)</code> before that word. It will for <code>pretty word</code> where book word is not present. <p>We can also use language in text index as stop words for different language is different. Default language is English though.  There is list of supported language that we can use. Default language is very important when it comes to text index. </p> <pre><code>db.infos.createIndex(\n    { title : \"text\" }, \n    { default_laguage : \"germany\"}\n)\n</code></pre> <p>We can also assign weight to the fields which will be used to create text index. </p><pre><code>db.infos.createIndex(\n    { title : \"text\" , summary : \"text\" }, \n    { weights : { title : 5 , summary : 1 }\n)\n</code></pre> <p>We also search in case sensitive way like the following. </p><pre><code>db.infos.find({ \n    \"$text\" : { \"$search\" : \"pretty\" }, \n    $caseSentitive : true \n})\n</code></pre>"},{"location":"mongodb/#building-index","title":"Building Index","text":"<p>When we are creating any index using createIndex method on that time the collection got locked. On that time if we try to insert any document then we have to wait for a certain time. The down time will depend on the size of the collection. It is adjustable in lower environment, but we cannot afford this in production. To deal with this create index in background. The time taken for creating the index is slow in background than foreground. </p><pre><code>db.infos.createIndex(\n    { \"age\" : 1 } , \n    { \"background\" : true }\n)\n</code></pre>"},{"location":"mongodb/#more-examples","title":"More Examples","text":""},{"location":"mongodb/#list-indexes","title":"List Indexes","text":"<p>To list all indexes on a collection: </p><pre><code>db.coll.getIndexes()\n</code></pre> <pre><code>db.coll.getIndexKeys()\n</code></pre>"},{"location":"mongodb/#create-indexes","title":"Create Indexes","text":"<p>To create different types of indexes:</p> <p>single field index </p><pre><code>db.coll.createIndex({\"name\": 1})\n</code></pre> <p>compound index </p><pre><code>db.coll.createIndex({\"name\": 1, \"date\": 1})\n</code></pre> <p>text index </p><pre><code>db.coll.createIndex({foo: \"text\", bar: \"text\"}) // \n</code></pre> <p>wildcard text index </p><pre><code>db.coll.createIndex({\"$**\": \"text\"})\n</code></pre> <p>wildcard index </p><pre><code>db.coll.createIndex({\"userMetadata.$**\": 1})\n</code></pre> <p>2d index </p><pre><code>db.coll.createIndex({\"loc\": \"2d\"})\n</code></pre> <p>2dsphere index </p><pre><code>db.coll.createIndex({\"loc\": \"2dsphere\"})\n</code></pre> <p>hashed index </p><pre><code>db.coll.createIndex({\"_id\": \"hashed\"})\n</code></pre> <p>Index Options</p> <p>TTL index </p><pre><code>db.coll.createIndex({\"lastModifiedDate\": 1}, {expireAfterSeconds: 3600})\n</code></pre> <p>Unique index </p><pre><code>db.coll.createIndex({\"name\": 1}, {unique: true})\n</code></pre> <p>partial index </p><pre><code>db.coll.createIndex({\"name\": 1}, {partialFilterExpression: {age: {$gt: 18}}})\n</code></pre> <p>case insensitive index with strength = 1 or 2 </p><pre><code>db.coll.createIndex({\"name\": 1}, {collation: {locale: 'en', strength: 1}})\n</code></pre> <p>Sparse index </p><pre><code>db.coll.createIndex({\"name\": 1 }, {sparse: true})\n</code></pre>"},{"location":"mongodb/#drop-indexes","title":"Drop Indexes","text":"<p>To drop an index by name: </p><pre><code>db.coll.dropIndex(\"name_1\")\n</code></pre>"},{"location":"mongodb/#hideunhide-indexes","title":"Hide/Unhide Indexes","text":"<p>To hide or unhide an index: </p><pre><code>db.coll.hideIndex(\"name_1\")\n</code></pre> <pre><code>db.coll.unhideIndex(\"name_1\")\n</code></pre>"},{"location":"mongodb/advance-commands/","title":"advance commands","text":""},{"location":"mongodb/advance-commands/#handy-commands","title":"Handy commands","text":"<p>Switch to admin database and create a user: </p><pre><code>use admin\ndb.createUser({\"user\": \"root\", \"pwd\": passwordPrompt(), \"roles\": [\"root\"]})\n</code></pre> <p>Drop a user: </p><pre><code>db.dropUser(\"root\")\n</code></pre> <p>Authenticate a user: </p><pre><code>db.auth(\"user\", passwordPrompt())\n</code></pre> <p>Switch to test database: </p><pre><code>use test\n</code></pre> <p>Get sibling database: </p><pre><code>db.getSiblingDB(\"dbname\")\n</code></pre> <p>Get current operations: </p><pre><code>db.currentOp()\n</code></pre> <p>Kill an operation: </p><pre><code>db.killOp(123) // opid\n</code></pre> <p>Lock and unlock the database: </p><pre><code>db.fsyncLock()\ndb.fsyncUnlock()\n</code></pre> <p>Get collection names and information: </p><pre><code>db.getCollectionNames()\ndb.getCollectionInfos()\ndb.printCollectionStats()\n</code></pre> <p>Get database statistics: </p><pre><code>db.stats()\n</code></pre> <p>Get replication information: </p><pre><code>db.getReplicationInfo()\ndb.printReplicationInfo()\n</code></pre> <p>Get server information: </p><pre><code>db.hello()\ndb.hostInfo()\n</code></pre> <p>Shutdown the server: </p><pre><code>db.shutdownServer()\n</code></pre> <p>Get server status: </p><pre><code>db.serverStatus()\n</code></pre> <p>Get and set profiling level: </p><pre><code>db.getProfilingStatus()\ndb.setProfilingLevel(1, 200) // 0 == OFF, 1 == ON with slowms, 2 == ON\n</code></pre> <p>Enable and disable free monitoring: </p><pre><code>db.enableFreeMonitoring()\ndb.disableFreeMonitoring()\ndb.getFreeMonitoringStatus()\n</code></pre> <p>Create a view: </p><pre><code>db.createView(\"viewName\", \"sourceColl\", [{$project:{department: 1}}])\n</code></pre>"},{"location":"mongodb/advance-commands/#change-streams","title":"Change Streams","text":"<p>Watch for changes in a collection: </p><pre><code>watchCursor = db.coll.watch([ { $match : {\"operationType\" : \"insert\" } } ])\nwhile (!watchCursor.isExhausted()){\n   if (watchCursor.hasNext()){\n      print(tojson(watchCursor.next()));\n   }\n}\n</code></pre>"},{"location":"mongodb/advance-commands/#replica-set","title":"Replica Set","text":"<p>Get replica set status: </p><pre><code>rs.status()\n</code></pre> <p>Initialize a replica set: </p><pre><code>rs.initiate({\n  \"_id\": \"RS1\",\n  members: [\n    { _id: 0, host: \"mongodb1.net:27017\" },\n    { _id: 1, host: \"mongodb2.net:27017\" },\n    { _id: 2, host: \"mongodb3.net:27017\" }\n  ]\n})\n</code></pre> <p>Add a member to the replica set: </p><pre><code>rs.add(\"mongodb4.net:27017\")\n</code></pre> <p>Add an arbiter to the replica set: </p><pre><code>rs.addArb(\"mongodb5.net:27017\")\n</code></pre> <p>Remove a member from the replica set: </p><pre><code>rs.remove(\"mongodb1.net:27017\")\n</code></pre> <p>Get replica set configuration: </p><pre><code>rs.conf()\n</code></pre> <p>Get replica set hello information: </p><pre><code>rs.hello()\n</code></pre> <p>Print replication information: </p><pre><code>rs.printReplicationInfo()\nrs.printSecondaryReplicationInfo()\n</code></pre> <p>Reconfigure the replica set: </p><pre><code>rs.reconfig(config)\nrs.reconfigForPSASet(memberIndex, config, { options })\n</code></pre> <p>Set read preference: </p><pre><code>db.getMongo().setReadPref('secondaryPreferred')\n</code></pre> <p>Step down the primary: </p><pre><code>rs.stepDown(20, 5) // (stepDownSecs, secondaryCatchUpPeriodSecs)\n</code></pre>"},{"location":"mongodb/advance-commands/#sharded-cluster","title":"Sharded Cluster","text":"<p>Print sharding status: </p><pre><code>db.printShardingStatus()\n</code></pre> <p>Get sharding status: </p><pre><code>sh.status()\n</code></pre> <p>Add a shard to the cluster: </p><pre><code>sh.addShard(\"rs1/mongodb1.example.net:27017\")\n</code></pre> <p>Shard a collection: </p><pre><code>sh.shardCollection(\"mydb.coll\", {zipcode: 1})\n</code></pre> <p>Move a chunk to a different shard: </p><pre><code>sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\")\n</code></pre> <p>Split a chunk at a specific point: </p><pre><code>sh.splitAt(\"mydb.coll\", {x: 70})\n</code></pre> <p>Split a chunk based on a query: </p><pre><code>sh.splitFind(\"mydb.coll\", {x: 70})\n</code></pre> <p>Start and stop the balancer: </p><pre><code>sh.startBalancer()\nsh.stopBalancer()\n</code></pre> <p>Enable and disable balancing for a collection: </p><pre><code>sh.disableBalancing(\"mydb.coll\")\nsh.enableBalancing(\"mydb.coll\")\n</code></pre> <p>Get and set balancer state: </p><pre><code>sh.getBalancerState()\nsh.setBalancerState(true/false)\n</code></pre> <p>Check if the balancer is running: </p><pre><code>sh.isBalancerRunning()\n</code></pre> <p>Start and stop auto-merger: </p><pre><code>sh.startAutoMerger()\nsh.stopAutoMerger()\n</code></pre> <p>Enable and disable auto-merger: </p><pre><code>sh.enableAutoMerger()\nsh.disableAutoMerger()\n</code></pre> <p>Update zone key range: </p><pre><code>sh.updateZoneKeyRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\")\n</code></pre> <p>Remove range from zone: </p><pre><code>sh.removeRangeFromZone(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey })\n</code></pre> <p>Add and remove shard from zone: </p><pre><code>sh.addShardToZone(\"shard0000\", \"NYC\")\nsh.removeShardFromZone(\"shard0000\", \"NYC\")\n</code></pre>"},{"location":"mongodb/aggregation-framework/","title":"aggregation framework","text":""},{"location":"mongodb/aggregation-framework/#introduction","title":"Introduction","text":"<p>Aggregation framework is just another find method we could say but it has some other advantages too. In aggregation framework we basically create pipeline of steps which operates on datas of that collection.</p> <p>Aggregation Pipeline </p><pre><code>db.listingsAndReviews.aggregate([\n    {\n        $match: {\n            number_of_reviews: { $gte: 100 } // Listings with more than 100 reviews\n        } \n    },\n    {\n        $group : {\n            _id : \"$property_type\",        // Group by property type\n            count: { $sum : 1 },           // Total listings\n            reviewCount: { $sum : \"$number_of_reviews\" },        // Total reviews\n            avgPrice: { $avg : \"$price\" }, // Average price\n        },\n    },\n    {\n        $project: {\n            _id: 1,\n            count: 1,\n            reviewCount: 1,\n            avgPrice: { $ceil : \"$avgPrice\" } // Round up avgPrice\n        }\n    },\n    {\n        $match: {\n            reviewCount: { $gte: 10000 } // Listings by property with more than 10000 total reviews\n        } \n    },\n    {\n        $sort : { \n            count : -1, // Sort by count descending\n            avgPrice: 1 // Sort by avgPrice ascending\n        }\n    }\n])\n</code></pre> <p><code>$lookup (Join)</code> </p><pre><code>db.accounts.aggregate([\n   {\n      $lookup:\n        {\n          from: \"transactions\",         // join with 'transactions' collection\n          localField: \"account_id\",     // field from the 'accounts' collection\n          foreignField: \"account_id\",   // field from the 'transactions' collection\n          as: \"customer_orders\"         // output array field\n        }\n   },\n   {\n      $match: { $expr: { $lt: [ {$size: \"$customer_orders\"}, 5 ] } } // filter for documents where 'customer_orders' is &lt; 5\n   },\n])\n</code></pre>"},{"location":"mongodb/create/","title":"create","text":""},{"location":"mongodb/create/#general","title":"General","text":"<p>We have three methods for inserting documents</p> <ul> <li><code>insertOne</code> </li> <li><code>insertMany</code></li> <li><code>insert</code></li> </ul> <p>Though <code>insert</code> method is flexible enough to handle one document or multiple but still it is deprecated on purpose.</p> <p>Also, we can directly import from a json file using mongoimort command</p> <p>If we are using insert many and we are inserting multiple documents in a shot then if there is a issue with any document in that list then from that onwards there will be no insertions, only the documents before the wrecked document will be inserted, it will not be rolled back.</p> <p>Like for the following code there is a issue in third document </p><pre><code>&gt; db.hobbies.insertMany([{_id: \"yoga\"}, {_id: \"sports\"} ,{_id: \"yoga\"}, {_id: \"maths\"}])\n\"errmsg\" : \"E11000 duplicate key error collection: contacts.hobbies index: _id_ dup key: { _id: \\\"yoga\\\" }\",\n\n&gt; db.hobbies.find().toArray()\n[ { \"_id\" : \"yoga\" }, { \"_id\" : \"sports\" } ]\n</code></pre> <p>But to remove this one we can pass one argument {ordered: false}. By default, it is true. It defines that the insertion will be ordered or not.</p> <p>If we again try to run the previous code in shell it will again give us the error, but it will not stop to the error document rather it will insert all the correct documents. </p><pre><code>&gt; db.hobbies.insertMany([{_id: \"yoga\"}, {_id: \"sports\"}, {_id: \"yoga\"}, {_id: \"maths\"}], {ordered: false})\n\"E11000 duplicate key error collection: contacts.hobbies index: _id_ dup key: { _id: \\\"yoga\\\" }\", \"E11000 duplicate key error collection: contacts.hobbies index: _id_ dup key: { _id: \\\"sports\\\" }\",\n\n&gt; db.hobbies.find().toArray()\n[ { \"_id\" : \"yoga\" }, { \"_id\" : \"sports\" }, { \"_id\" : \"maths\" } ]\n</code></pre> <p>Example of <code>insertOne</code> and <code>insertMany</code> </p><pre><code>&gt; use contacts\nswitched to db contacts\n\n&gt; db.persons.insertOne({name:\"Abhishek Ghosh\"})\n{\n        \"acknowledged\" : true,\n        \"insertedId\" : ObjectId(\"62aadb4256184ff0056adbd7\")\n}\n\n&gt; db.persons.insertMany([{name:\"Abhishek Pal\"},{name:\"Bishal Mukherjee\"}])\n{\n        \"acknowledged\" : true,\n        \"insertedIds\" : [\n                ObjectId(\"62aadbed56184ff0056adbd8\"),\n                ObjectId(\"62aadbed56184ff0056adbd9\")\n        ]\n}\n</code></pre>"},{"location":"mongodb/create/#writeconcern","title":"WriteConcern","text":""},{"location":"mongodb/create/#description","title":"Description","text":"<p>Write concern describes the level of acknowledgment requested from MongoDB for write operations to a standalone mongod or to replica sets or to sharded clusters. In sharded clusters, mongos instances will pass the write concern on to the shards.</p> <p>the write concern is a specification of MongoDB for write operations that determines the acknowledgement you want after a write operation has taken place. MongoDB has a default write concern of always acknowledging all writes, which means that after every write, MongoDB must always return an acknowledgement (in a form of a document), meaning that it was successful. When asking for write acknowledgement, if none isn't returned (in case of failover, crashes), the write isn't successful. This behavior is very useful specially on replica set usage, since you will have more than one mongod instance, and depending on your needs, maybe you don't want all instances to acknowledge the write, just a few, to speed up writes. Also, when to specify a write concern, you can specify journal writing, so you can guarantee that operation result and any rollbacks required if a failover happens. More information, here.</p> <p>In your case, it depends on how many mongod (if you have replica sets or just a single server) instances you have. Since \"always acknowledge\" is the default, you may want to change it if you have to manage replica sets operations and speed things up or just doesn't care about write acknowledgement in a single instance (which is not so good, since it's a single server only).</p> <p>Write concern can include the following fields: <code>{w: &lt;value&gt;, j: &lt;boolean&gt;, wtimeout: &lt;number&gt;}</code> </p><pre><code>{w: 1, j: true, wtimeout: 500}\n</code></pre> <p>the <code>w</code> option to request acknowledgment that the write operation has propagated to a specified number of mongod instances or to mongod instances with specified tags.</p> <p>the <code>j</code> option to request acknowledgment that the write operation has been written to the on-disk journal.</p> <p>the <code>wtimeout</code> option to specify a time limit to prevent write operations from blocking indefinitely.</p>"},{"location":"mongodb/create/#write-concern-levels","title":"Write Concern Levels","text":"<p>MongoDB has the following levels of conceptual write concern, listed from weakest to strongest:</p>"},{"location":"mongodb/create/#unacknowledged","title":"Unacknowledged","text":"<p>With an <code>unacknowledged</code> write concern, MongoDB does not acknowledge the receipt of write operations. <code>unacknowledged</code> is like errors ignored; however, drivers will attempt to receive and handle network errors when possible. The driver's ability to detect network errors depends on the system's networking configuration. Write operation to a <code>mongod</code> instance with write concern of <code>unacknowledged</code>. The client does not wait for any acknowledgment. </p>"},{"location":"mongodb/create/#acknowledged","title":"Acknowledged","text":"<p>With a receipt acknowledged write concern, the mongod confirms the receipt of the write operation. Acknowledged write concern allows clients to catch network, duplicate key, and other errors. This is default write concern. Write operation to a <code>mongod</code> instance with write concern of <code>acknowledged</code>. The client waits for acknowledgment of success or exception.</p>"},{"location":"mongodb/create/#journaled","title":"Journaled","text":"<p>With a journaled write concern, the MongoDB acknowledges the write operation only after committing the data to the journal. This write concern ensures that MongoDB can recover the data following a shutdown or power interruption. You must have journaling enabled to use this write concern. Write operation to a <code>mongod</code> instance with write concern of <code>journaled</code>. The <code>mongod</code> sends acknowledgment after it commits the write operation to the journal.</p>"},{"location":"mongodb/create/#replica-acknowledged","title":"Replica Acknowledged","text":"<p>Replica sets present additional considerations with regards to write concern. The default write concern only requires acknowledgement from the primary. With <code>replica acknowledged</code> write concern, you can guarantee that the write operation propagates to additional members of the replica set. Write operation to a replica set with write concern level of <code>w:2</code> or write to the primary and at least one secondary.</p>"},{"location":"mongodb/create/#reference","title":"Reference","text":"<ul> <li>Write Concern</li> <li>Journaling</li> </ul> <p>When we have millions of records inserting in seconds then on that time, we can skip the acknowledgement and use <code>w: 0</code>. By default is <code>undefined</code>.</p> <p>If <code>j: true</code>, then inserting will take some extra time as it will write on journal. By default, is <code>undefined</code>. Here it has the higher security.</p>"},{"location":"mongodb/create/#atomicity","title":"Atomicity","text":"<p>It means when we are inserting any document then either it will be saved as a whole, or it will not be saved at all if there is any issue. MongoDB provides atomic transaction guarantee.</p>"},{"location":"mongodb/create/#import-from-file","title":"Import from file","text":"<p>Lastly, we can import json file in and save it mongodb.</p> <p>if it is a single document </p><pre><code>mongoimport --db dbName --collection collectionName --file /path/fileName.json\n</code></pre> <p>if it is a array of documents. </p><pre><code>mongoimport --db dbName --collection collectionName --file /path/fileName.json --jsonArray\n</code></pre> <p>if we add <code>--drop</code> then it will delete previous data  </p>"},{"location":"mongodb/create/#more-examples","title":"More examples","text":"<p>Insert one document: </p><pre><code>db.coll.insertOne({name: \"Max\"})\n</code></pre> <p>Insert many documents (ordered bulk insert): </p><pre><code>db.coll.insertMany([{name: \"Max\"}, {name:\"Alex\"}])\n</code></pre> <p>Insert many documents (unordered bulk insert): </p><pre><code>db.coll.insertMany([{name: \"Max\"}, {name:\"Alex\"}], {ordered: false})\n</code></pre> <p>Insert a document with the current date: </p><pre><code>db.coll.insertOne({date: ISODate()})\n</code></pre> <p>Insert a document with write concern: </p><pre><code>db.coll.insertOne({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\n</code></pre>"},{"location":"mongodb/crud/","title":"crud","text":""},{"location":"mongodb/crud/#create","title":"Create","text":"<ul> <li><code>insertOne(data, options)</code> -&gt; for inserting one item</li> <li><code>insertMany(data, options)</code> -&gt; for inserting multiple items</li> </ul>"},{"location":"mongodb/crud/#read","title":"Read","text":"<ul> <li><code>find(filter, options)</code> -&gt; find all the data based on the filter</li> <li><code>findOne(filter, options)</code> -&gt; find the first matching element based on the filter</li> </ul>"},{"location":"mongodb/crud/#update","title":"Update","text":"<ul> <li><code>updateOne(filter, data, options)</code> -&gt; to update one document</li> <li><code>updateMany(filter, data, options)</code> -&gt; for updating multiple documents</li> <li><code>replaceOne(filter, data, options)</code> -&gt; for replacing the entire document</li> </ul>"},{"location":"mongodb/crud/#delete","title":"Delete","text":"<ul> <li><code>deleteOne(filter, options)</code> -&gt; delete only the first item with matching filter</li> <li><code>deleteMany(filter, options)</code> -&gt; delete all items matching with the filter</li> </ul>"},{"location":"mongodb/crud/#examples","title":"Examples","text":"<p>Delete the first element with <code>name</code> as <code>\"Abhishek Ghosh\"</code>: </p><pre><code>db.products.deleteOne({name: \"Abhishek Ghosh\"})\n</code></pre> <p>Update the <code>age</code> to <code>24</code> where <code>name</code> is <code>\"Abhishek Pal\"</code>: </p><pre><code>db.products.updateOne({name: \"Abhishek Pal\"}, {$set: {age: 24}})\n</code></pre> <p>Add a field <code>height</code> to all the documents: </p><pre><code>db.products.updateMany({}, {$set: {height: \"Unknown\"}})\n</code></pre> <code>{}</code> this means all the documents. <p>Insert two items at a time: </p><pre><code>db.products.insertMany([\n    {name: \"Nasim Molla\", age: 25},\n    {name: \"Sayan Mandal\", age: 24}\n])\n</code></pre> <p>Find all the <code>students</code> whose <code>age</code> is <code>greater than 24</code>: </p><pre><code>db.products.find({age: {$gt: 24}})\n</code></pre> <p>Print all the <code>names</code> but not <code>_id</code> for the <code>student</code> whose <code>age</code> is <code>greater than 24</code>: </p><pre><code>db.products.find({age: {$gt: 24}}, {name: 1, _id: 0})\n</code></pre> <p>If we use update without <code>$set</code> then the document will be replaced with the data we have provided. (Rather use replace than update for full replacement): </p><pre><code>db.products.insertOne({})\n// {\"acknowledged\" : true,\"insertedId\" : ObjectId(\"62a7faec7866653913689afd\")}\n\ndb.products.update({_id: ObjectId(\"62a7faec7866653913689afd\")}, {name: \"Anirban Ghosh\", age: 23})\n// WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n\ndb.products.find({_id: ObjectId(\"62a7faec7866653913689afd\")})\n// { \"_id\" : ObjectId(\"62a7faec7866653913689afd\"), \"name\" : \"Anirban Ghosh\", \"age\" : 23 }\n</code></pre> <p>Set status object for age greater than 24: </p><pre><code>db.products.updateMany(\n    {age: {$gt: 24}}, \n    {$set: {status: {married: false, single: false}}}\n)\n</code></pre> <p>If we have a list of strings like hobbies, we can search like this (It will find the first document that has a list of <code>hobbies</code> containing <code>\"Drama\"</code>): </p><pre><code>db.products.findOne({hobbies: \"Drama\"})\n</code></pre> <p>We can run a query in a nested object: </p><pre><code>db.products.findOne({\"status.single\": false})\n</code></pre> <p>To get rid of your data, you can simply load the database you want to get rid of (<code>use databaseName</code>) and then execute: </p><pre><code>db.dropDatabase()\n</code></pre> <p>Similarly, you could get rid of a single collection in a database via: </p><pre><code>db.&lt;collection-name&gt;.drop()\n</code></pre>"},{"location":"mongodb/crud/#what-is-cursor","title":"What is cursor?","text":"<ul> <li>When we find anything in the shell, rather than giving everything in one shot, it gives us the cursor of 20 elements, and to move to the next 20, we have to enter \"it\". To see it, we can use the <code>toArray</code> method on the cursor, which will exhaust the cursor and make one array with all the elements and show that.</li> <li>Cursor will fetch only the needed element.</li> <li><code>findOne</code> will not give us a cursor object as it will only give us one element.</li> <li><code>db.products.find().toArray()</code></li> <li>`db.products.find().forEach((doc) =&gt; {printjson(doc)})</li> </ul>"},{"location":"mongodb/crud/#what-is-projection","title":"What is projection?","text":"<ul> <li>Rather than showing all the fields of a document, we can choose whatever we want to show.</li> <li>It will also help us to reduce the bandwidth usage as the server will not send all the elements.</li> <li>To get all the student's <code>name</code> with <code>age</code> equals <code>24</code>:  <pre><code>db.products.find({age: 24}, {name: 1})\n</code></pre></li> <li>By default, <code>_id</code> is set to 1, so if we want to remove it as well, we have to use this type of query: <pre><code>db.products.find({age: 24}, {name: 1, _id: 0})\n</code></pre></li> <li>One Document can hold a maximum of 100 levels of nesting.</li> </ul>"},{"location":"mongodb/data-types/","title":"data types","text":""},{"location":"mongodb/data-types/#primitive-types","title":"Primitive types","text":"<ul> <li>Text -&gt; <code>\"Abhishek Ghosh\"</code></li> <li>Boolean -&gt; <code>true</code></li> <li>Number -&gt; <ul> <li><code>NimberInt()</code> -&gt; 1</li> <li><code>Integer(int32)</code> -&gt; 55</li> <li><code>NumberLong(int64)</code> -&gt; 1000000000</li> <li><code>NumberDecimal</code> -&gt; 12.0009</li> </ul> </li> <li>ObjectId -&gt; <code>ObjectId(\"62a6fddadb132197c5e8879f\")</code></li> <li>ISODate -&gt; <code>2022-06-14T05:45:29.379+00:00</code></li> <li>Timestamp </li> <li>Embedded Documents</li> <li>Arrays</li> </ul> <p><code>Db.stats()</code> will bring the statistic of the database.</p> <p>MongoDB has a couple of hard limits - most importantly, a single document in a collection (including all embedded documents it might have) must be less than equal to <code>16mb</code>. Additionally, you may only have <code>100 levels of embedded documents</code>.</p> <p>You can find all limits (in great detail) here: MongoDB Limits and Thresholds</p> <p>For the data types, MongoDB supports, you find a detailed overview on this page: BSON Types</p> <p>Important data type limits are:</p> <ul> <li>Normal integers (int32) can hold a maximum value of <code>-2,147,483,647 to +2,147,483,647</code></li> <li>Long integers (int64) can hold a maximum value of <code>-9,223,372,036,854,775,807 to +9,223,372,036,854,775,807</code></li> <li>Text can be as long as you want - the limit is the <code>16mb</code> restriction for the overall document</li> </ul> <p>It's also important to understand the difference between <code>int32 (NumberInt)</code>, <code>int64 (NumberLong)</code> and a normal number as you can enter it in the shell.</p> <p>The same goes for a <code>normal double</code> and <code>NumberDecimal</code>.</p> <p><code>NumberInt</code> creates a <code>int32</code> value =&gt; <code>NumberInt(55)</code> and <code>NumberLong</code> creates a <code>int64</code> value =&gt; <code>NumberLong(7489729384792)</code></p> <p>If you just use a number e.g. <code>insertOne({age: 1})</code>, this will get added as a <code>normal double</code> into the database. </p> <p>The reason for this is that the shell is based on <code>JS</code> which only knows <code>float/double</code> values and doesn't differ between <code>integers</code> and <code>floats</code>.</p> <p><code>NumberDecimal</code> creates a high-precision double value e.g. <code>NumberDecimal(\"12.99\")</code> This can be helpful for cases where you need (many) exact decimal places for calculations.</p> <p>When not working with the shell but a MongoDB driver for your app programming language (e.g. PHP, .NET, Node.js, ...), you can use the driver to create these specific numbers.</p> <p>Example for Node.js</p> <p>This will allow you to build a <code>NumberLong</code> value like this </p><pre><code>const Long = require('mongodb').Long;\ndb.collection('wealth')\n    .insert({ value: Long.fromString(\"121949898291\")});\n</code></pre>"},{"location":"mongodb/data-types/#embedded-documents-vs-reference-id","title":"Embedded documents vs reference id","text":""},{"location":"mongodb/data-types/#embedding-is-better-for","title":"Embedding is better for","text":"<ul> <li>Small subdocuments</li> <li>Data that does not change regularly</li> <li>When eventual consistency is acceptable</li> <li>Documents that grow by a small amount</li> <li>Data that you'll often need to perform a second query to fetch Fast reads</li> </ul>"},{"location":"mongodb/data-types/#references-are-better-for","title":"References are better for","text":"<ul> <li>Large subdocuments</li> <li>Volatile data</li> <li>When immediate consistency is necessary</li> <li>Documents that grow a large amount</li> <li>Data that youll often exclude from the results</li> <li>Fast writes</li> </ul> <p>Refference : Data Modeling</p> <p>We can also use aggregation framework for joining.</p> <p>The MongoDB <code>lookup</code> operator, by definition, <code>Performs a left outer join to an unshared collection in the same database to filter in documents from the \"joined\" collection for processing.</code> Simply put, using the MongoDB <code>lookup</code> operator makes it possible to merge data from the document you are running a query on and the document you want the data from.</p> <p>More can be found in the following links</p> <ul> <li>MongoDB Lookup Aggregations: Syntax, Usage &amp; Practical Examples 101</li> <li>$lookup (aggregation)</li> </ul>"},{"location":"mongodb/data-types/#data-validation","title":"Data validation","text":"<p>Though Mongodb is schema less but we real life scenario we must have certain type of structure. We can add validators when we are creating any collection. </p><pre><code>db.createCollection('posts', {\n    validator: {\n      $jsonSchema: {\n        bsonType: 'object',\n        required: ['title', 'text', 'creator', 'comments'],\n        properties: {\n          title: {\n            bsonType: 'string',\n            description: 'must be a string and is required'\n          },\n          text: {\n            bsonType: 'string',\n            description: 'must be a string and is required'\n          },\n          creator: {\n            bsonType: 'objectId',\n            description: 'must be an objectid and is required'\n          },\n          comments: {\n            bsonType: 'array',\n            description: 'must be an array and is required',\n            items: {\n              bsonType: 'object',\n              required: ['text', 'author'],\n              properties: {\n                text: {\n                  bsonType: 'string',\n                  description: 'must be a string and is required'\n                },\n                author: {\n                  bsonType: 'objectId',\n                  description: 'must be an objectid and is required'\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  });\n</code></pre> <p>If the collection is already created, then we can use run command to add validations and also, we can add validation level </p><pre><code>db.runCommand({\n    collMod: 'posts',\n    validator: {\n      $jsonSchema: {\n        bsonType: 'object',\n        required: ['title', 'text', 'creator', 'comments'],\n        properties: {\n          title: {\n            bsonType: 'string',\n            description: 'must be a string and is required'\n          },\n          text: {\n            bsonType: 'string',\n            description: 'must be a string and is required'\n          },\n          creator: {\n            bsonType: 'objectId',\n            description: 'must be an objectid and is required'\n          },\n          comments: {\n            bsonType: 'array',\n            description: 'must be an array and is required',\n            items: {\n              bsonType: 'object',\n              required: ['text', 'author'],\n              properties: {\n                text: {\n                  bsonType: 'string',\n                  description: 'must be a string and is required'\n                },\n                author: {\n                  bsonType: 'objectId',\n                  description: 'must be an objectid and is required'\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    validationAction: 'warn'\n  });\n</code></pre> <p>Helpful Articles/ Docs:</p> <ul> <li>MongoDB Limits and Thresholds</li> <li>BSON Types</li> <li>Schema Validation</li> </ul> <p>We can configure mongodb server in with various arguments. We can check all in mongod --help command.</p> <p>We can also use mongod.cfg to put all our configurations in a file and we can put it inside any folder and to we have use that file when we are about to start the server.</p> <p>mongod -f /path/mongod.cfg </p><pre><code>storage:\n  dbPath: \"/your/path/to/the/db/folder\"\nsystemLog:\n  destination: file\n  path: \"/your/path/to/the/logs.log\"\n</code></pre> Reference: Self-Managed Configuration File Options <p>Helpful Articles/ Docs:</p> <ul> <li>More Details about Config Files: Self-Managed Configuration File Options</li> <li>More Details about the Server (mongod) Options: mongod</li> </ul>"},{"location":"mongodb/delete/","title":"delete","text":""},{"location":"mongodb/delete/#introduction","title":"Introduction","text":"<p>To <code>delete</code> any document, we have <code>deleteOne</code> and <code>deleteMany</code> methods.  In the <code>2nd</code> parameter we can also give <code>writeConcerns</code>. </p> <p>delete the first record where name is <code>Abhishek</code> </p><pre><code>db.infos.deleteOne({ name :  \"Abhishek\" })\n</code></pre> <p>delete the all record where age is greater than equal to <code>40</code> </p><pre><code>db.infos.deleteMany({ age :  { $gte : 40} })\n</code></pre> <p>delete the all record where age don't exists </p><pre><code>db.infos.deleteMany({ age :  { $exists : false } })\n</code></pre> <p>We can <code>delete</code> all the documents in a collection by using <code>db.infos.deleteMany({})</code></p> <p>We can <code>drop</code> the collection by <code>db.infos.drop()</code></p> <p>We can <code>drop</code> the database by <code>db.dropDatabase()</code></p>"},{"location":"mongodb/delete/#more-examples","title":"More Examples","text":"<p>Delete one document: </p><pre><code>db.coll.deleteOne({name: \"Max\"})\n</code></pre> <p>Delete many documents with write concern: </p><pre><code>db.coll.deleteMany({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\n</code></pre> <p>Delete all documents in a collection: </p><pre><code>db.coll.deleteMany({}) // WARNING! Deletes all the docs but not the collection itself and its index definitions\n</code></pre> <p>Find one document and delete: </p><pre><code>db.coll.findOneAndDelete({\"name\": \"Max\"})\n</code></pre>"},{"location":"mongodb/geospatial-data/","title":"geospatial data","text":""},{"location":"mongodb/geospatial-data/#introduction","title":"Introduction","text":"<p>We can also store and retrieve geo location <code>(2D)</code> data and use indexes on that. </p> <p>It will be stored as <code>[x,y]</code> where <code>x</code> must be the longitude and <code>y</code> must be the latitude. It follows the geoJSON format only.</p> <pre><code>&gt; db.infos.insertOne({\n    name : \"Home\" , \n    location : { \n        type : \"Point\" , \n        coordinates : [24.0814946,88.2408234,13.38]\n    }\n})\n\n{\n        \"acknowledged\" : true,\n        \"insertedId\" : ObjectId(\"62dcff4c85a6e4bfe5a374cd\")\n}\n</code></pre> <p>To store the coordinates, we must follow this structure of the embed document { location : { type : \"Point\" , coordinates : [24.0814946,88.2408234]}}. We can change the name of field \u201clocation\u201d, but the structure must be same.</p> <p>We can also store area or polygon Let\u2019s create 4 points  </p><pre><code>const p1 = [24.08409, 88.24231]\nconst p2 = [24.09149, 88.24707]\nconst p3 = [24.08879, 88.25578]\nconst p4 = [24.08048, 88.24934]\n\n&gt; db.infos.insertOne({\n        name : \"Gorabazar area\" , \n        location : { \n            type : \"Polygon\" , coordinates : [[p1,p2,p3,p4,p1]]\n        }\n    })\n{\n        \"acknowledged\" : true,\n        \"insertedId\" : ObjectId(\"62dd0e1d85a6e4bfe5a374d3\")\n}\n</code></pre> <p>It is to better to create a geospatial index as most of the geospatial queries require indexing. We can check any points are near to the queried point or not. For that we have a special syntax. </p><pre><code>&gt; db.infos.find({ \n    location : { \n        $near : { $geometry : { type : \"Point\", coordinates : [24,88]}}\n    }\n}).pretty()\n\n{\n        \"_id\" : ObjectId(\"62dcff4c85a6e4bfe5a374cd\"),\n        \"name\" : \"Home\",\n        \"location\" : {\n                \"type\" : \"Point\",\n                \"coordinates\" : [\n                        24.0814946,\n                        88.2408234\n                ]\n        }\n}\n</code></pre> <p>We can also specify other things along side $geometry like $maxDistance and $minDistance. The unit will be in meters. </p><pre><code>&gt; db.infos.find({ \n    location : { \n        $near : { \n            $geometry : { type : \"Point\", coordinates : [24,88]}, \n            $minDistance : 10, $maxDistance : 26809}\n    }\n}).pretty()\n\n&gt; db.infos.find({ \n        location : { \n            $near : { \n                $geometry : { type : \"Point\", coordinates : [24,88]}, \n                $minDistance : 10, \n                $maxDistance : 26810\n            }\n        }\n    }).pretty()\n{\n      \"_id\" : ObjectId(\"62dcff4c85a6e4bfe5a374cd\"),\n        \"name\" : \"Home\",\n        \"location\" : {\n                \"type\" : \"Point\",\n                \"coordinates\" : [\n                        24.0814946,\n                        88.2408234\n                ]\n        }\n}\n</code></pre> <p>To find a place inside any special region or not we can do this. First, we can create our own map (from google maps -&gt; your places -&gt; see all your maps) and create one area or polygon.</p> <p>We can validate all the points are inside of these 4 coordinates or not. In map we will see these 4 points made a rectangle.</p> <p>We will also insert some of the points. </p><pre><code>&gt; db.infos.insertOne({\n    name : \"Murshidabad medical college\" , \n    location : { type : \"Point\" , coordinates : [24.089473,88.2513618]}\n})\n&gt; db.infos.insertOne({\n    name : \"Gorabazar ICI\" , \n    location : { type : \"Point\" , coordinates : [24.0930413,88.2483631]}\n})\n&gt; db.infos.insertOne({\n        name : \"Mary immaculate school\" , \n        location : { type : \"Point\" , coordinates : [24.0930413,88.2483631]}\n    })\n&gt; db.infos.insertOne({\n        name : \"Berhampore head post office\" , \n        location : { type : \"Point\" , coordinates : [24.0947532,88.2510873]}\n    })\n&gt; db.infos.insertOne({\n        name : \"Mohon cinema hall\" , \n        location : { type : \"Point\" , coordinates : [24.0947532,88.2510873]}\n    })\n</code></pre> <p>Again, we must follow some specific syntax for with in query. </p><pre><code>&gt; db.infos.find({ \n    location : { \n        $geoWithin : { \n            $geometry : { type : \"Polygon\", coordinates : [[p1,p2,p3,p4,p1]]}\n        }\n    }\n})\n</code></pre> <p>Keyword is $geoWithin and type is Polygon and coordinates will be in 2nd layer of nested arrays and the first and the last point should be same.</p> <p>We can also search for the opposite query. We can find an poly where a point belongs or not. </p><pre><code>&gt; db.infos.find({ \n    location : { \n        $geoIntersects : { \n            $geometry : { type : \"Point\" , coordinates : [24.089473,88.2513618] } }\n    }\n})\n</code></pre> <p>We can also search in circle within a radius. </p><pre><code>&gt; db.infos.find({ \n    location : { \n        $geoWithin : { \n            $centerSphere : [[24.089473, 88.2513618], 1/6378.1]}\n    }\n})\n</code></pre> Where 1st one is the 2d coordinate and 2nd one is radius. 1 is in kilometre. 6378.1 is the constant. Check this on official documentation."},{"location":"mongodb/introduction/","title":"introduction","text":""},{"location":"mongodb/introduction/#what-is-mongodb","title":"What is MongoDB?","text":"<p>MongoDB is a document-oriented NoSQL database used to store large amounts of data as documents. It has collections similar to tables in relational databases. It has no schema. We can use JSON objects to store data, but behind the scenes, the MongoDB server stores this JSON in binary format.</p>"},{"location":"mongodb/introduction/#what-is-mongod","title":"What is <code>mongod</code>?","text":"<p>It is an executable file used to start the MongoDB server locally.</p>"},{"location":"mongodb/introduction/#what-is-mongosh","title":"What is <code>mongosh</code>?","text":"<p>It is a MongoDB shell used to connect to MongoDB to execute queries.</p> <p>We can specify the location where we want to save our data locally. It should have <code>data</code> and <code>logs</code> folders inside it. Then start the server like the following: </p><pre><code>mongod --dbpath /path/data --logpath /path/logs/mongo.log\n</code></pre>"},{"location":"mongodb/introduction/#connect-to-mongodb-shell","title":"Connect to MongoDB Shell","text":"<pre><code>mongosh // connects to mongodb://127.0.0.1:27017 by default\nmongosh \"mongodb+srv://cluster-name.abcde.mongodb.net/&lt;dbname&gt;\" --username &lt;username&gt; // MongoDB Atlas\nmongosh --host &lt;host&gt; --port &lt;port&gt; --authenticationDatabase admin -u &lt;user&gt; -p &lt;pwd&gt; # omit the password if you want a prompt\nmongosh \"mongodb://&lt;user&gt;:&lt;password&gt;@192.168.1.1:27017\"\nmongosh \"mongodb://192.168.1.1:27017\"\nmongosh \"mongodb+srv://cluster-name.abcde.mongodb.net/&lt;dbname&gt;\" --apiVersion 1 --username &lt;username&gt; # MongoDB Atlas\n</code></pre>"},{"location":"mongodb/introduction/#install-mongodb","title":"Install MongoDB","text":"<p>Install MongoDB Community Edition</p>"},{"location":"mongodb/introduction/#windows","title":"Windows","text":"<p>How do I start/stop MongoDB from running in the background in Windows?</p> <p>In Windows, there is an option to start MongoDB as a service so it will be running all the time in the background. One-liner to start or stop MongoDB service using the command line in Windows:</p> <ul> <li>To start the service use: <code>NET START MONGODB</code></li> <li>To stop the service use: <code>NET STOP MONGODB</code></li> </ul>"},{"location":"mongodb/introduction/#macoslinux","title":"MacOS/Linux","text":"<p>How do I start/stop MongoDB from running in the background in macOS/Linux?</p> <p>The <code>--fork</code> option is used to run MongoDB in the background. </p><pre><code>mongod --port 8888 --dbpath /Users/Shared/data/db --logpath /Users/Shared/log/mongo.log --fork\n</code></pre> We can shut down MongoDB by first switching to the <code>admin</code> database, then use this command: <pre><code>db.shutdownServer()\n</code></pre>"},{"location":"mongodb/introduction/#docker","title":"Docker","text":"<p>How do I start/stop mongodb from docker </p><pre><code>docker run \\\n  --rm \\\n  --name mongodb \\\n  -v ~/mongodb-data:/data/db \\\n  -e MONGO_INITDB_ROOT_USERNAME=admin \\\n  -e MONGO_INITDB_ROOT_PASSWORD=password \\\n  -p 27017:27017 \\\n  mongo\n</code></pre>"},{"location":"mongodb/introduction/#prod-deployments","title":"Prod deployments","text":"<p>We can create production level Replica Set and Sharded Cluster</p>"},{"location":"mongodb/introduction/#common-commands","title":"Common Commands","text":""},{"location":"mongodb/introduction/#setup","title":"Setup","text":"<p>MongoDB uses <code>BSON</code> instead of <code>JSON</code> to store data. The maximum size of a document can be <code>16 MB</code>.</p> <p>Show all databases: </p><pre><code>show dbs\n</code></pre> <p>Create or use a database: </p><pre><code>use &lt;db_name&gt;\n</code></pre> <p>Remove the database: </p><pre><code>db.dropDatabase()\n</code></pre> <p>Show all collections: </p><pre><code>show collections\n</code></pre> <p>Create collections: </p><pre><code>db.createCollection(\"coll\") // creates the collection `coll`\n</code></pre> <p>Drop collections: </p><pre><code>db.coll.drop()    // removes the collection `coll`\n</code></pre> <p>Create a collection with a <code>$jsonschema</code> validator: </p><pre><code>// Create collection with a $jsonschema\ndb.createCollection(\"hosts\", {\n    validator: {$jsonSchema: {\n        bsonType: \"object\",\n        required: [\"email\"], // required fields\n        properties: {\n            // All possible fields\n            phone: {\n                bsonType: \"string\",\n                description: \"must be a string and is required\"\n            },\n            email: {\n                bsonType: \"string\",\n                pattern: \"@mongodb\\.com$\",\n                description: \"must be a string and match the regular expression pattern\"\n            },\n        }\n    }}\n})\n\ndb.createCollection(\"contacts\", {\n   validator: {$jsonSchema: {\n      bsonType: \"object\",\n      required: [\"phone\"],\n      properties: {\n         phone: {\n            bsonType: \"string\",\n            description: \"must be a string and is required\"\n         },\n         email: {\n            bsonType: \"string\",\n            pattern: \"@mongodb\\.com$\",\n            description: \"must be a string and match the regular expression pattern\"\n         },\n         status: {\n            enum: [ \"Unknown\", \"Incomplete\" ],\n            description: \"can only be one of the enum values\"\n         }\n      }\n   }}\n})\n</code></pre> <p>Run JavaScript File: </p><pre><code>load(\"script.js\")\n</code></pre> <p>Get collection statistics: </p><pre><code>db.coll.stats()\n</code></pre> <p>Get collection storage size: </p><pre><code>db.coll.storageSize()\n</code></pre> <p>Get total index size of a collection: </p><pre><code>db.coll.totalIndexSize()\n</code></pre> <p>Get total size of a collection: </p><pre><code>db.coll.totalSize()\n</code></pre> <p>Validate a collection: </p><pre><code>db.coll.validate({full: true})\n</code></pre> <p>Rename a collection: </p><pre><code>db.coll.renameCollection(\"new_coll\", true) // 2nd parameter to drop the target collection if exists\n</code></pre>"},{"location":"mongodb/introduction/#insert","title":"Insert","text":"<p>Insert one document into a collection: </p><pre><code>db.products.insertOne({\n    name: \"Abhishek Ghosh\", \n    age: 24\n})\n</code></pre> This will create a document in the <code>products</code> collection. After inserting one document, it will give an <code>id</code> and acknowledgment. We can also insert nested documents. <p>Insert many documents into a collection: </p><pre><code>db.coll.insertMany([\n    {name: \"Navi\", age: 25}, \n    {name: \"Alice\", age: 30}\n])\n</code></pre> <p>By default, MongoDB adds a unique <code>ObjectId</code> to every document, and we can search items with that. MongoDB also creates a default index with this <code>_id</code> by default. We can also add our <code>_id</code> like the following: </p><pre><code>db.products.insertOne({_id: \"abhishek-test-0001\", name: \"Abhishek Ghosh\"})\n</code></pre>"},{"location":"mongodb/introduction/#find","title":"Find","text":"<p>Show all documents in a collection: </p><pre><code>db.products.find()\n</code></pre> <p>List all documents with name \"Navi\" and age 25, and return only one document: </p><pre><code>db.coll.find({\n    name: \"Navi\", \n    age: 25\n}).limit(1)\n</code></pre> <p>Show documents in a JSON structure: </p><pre><code>db.products.find().pretty()\n</code></pre> <p>Search any document using <code>_id</code>: </p><pre><code>db.products.find({_id: ObjectId('62a6ff6edb132197c5e887a0')})\n</code></pre> <p>Count all documents in 'coll' collection: </p><pre><code>db.coll.count()\n</code></pre> <p>Count all documents with name \"Navi\": </p><pre><code>db.coll.count({name: \"Navi\"})\n</code></pre> <p>Find document and show execution stats: </p><pre><code>db.coll.find({name: \"Navi\"}).explain(\"executionStats\")\n</code></pre>"},{"location":"mongodb/introduction/#update","title":"Update","text":"<p>Update all documents with name \"Navi\" and set age to 26: </p><pre><code>db.coll.update({name: \"Navi\"}, {$set: {age: 26}})\n</code></pre> <p>Update all documents with name \"Navi\" and increment age by 1: </p><pre><code>db.coll.update({name: \"Navi\"}, {$inc: {age: 1}})\n</code></pre> <p>Update all documents with name \"Navi\" and set age to null: </p><pre><code>db.coll.update({name: \"Navi\"}, {$unset: {age: 1}})\n</code></pre> <p>Remove age field from all documents with age field: </p><pre><code>db.coll.updateMany({age: {$exists: true}}, {$unset: {age: \"\"}})\n</code></pre>"},{"location":"mongodb/introduction/#delete","title":"Delete","text":"<p>Remove all documents with name \"Navi\": </p><pre><code>db.coll.deleteMany({name: \"Navi\"})\n</code></pre> <p>Remove one document with name \"Navi\": </p><pre><code>db.coll.deleteOne({name: \"Navi\"})\n</code></pre>"},{"location":"mongodb/introduction/#indexes","title":"Indexes","text":"<p>List indexes: </p><pre><code>db.coll.getIndexes()\n</code></pre> <p>List index keys: </p><pre><code>db.coll.getIndexKeys()\n</code></pre> <p>Create index: </p><pre><code>db.coll.createIndex({\"name\": 1})\n</code></pre> <p>Create a compound index: </p><pre><code>db.coll.createIndex({\"name\": 1, \"date\": 1})\n</code></pre> <p>Drop index: </p><pre><code>db.coll.dropIndex(\"name_1\")\n</code></pre>"},{"location":"mongodb/introduction/#acid-compliance-in-mongodb","title":"ACID Compliance in MongoDB","text":"ACID Property MongoDB Implementation Atomicity MongoDB ensures atomicity at the single-document level, meaning changes to a single document are always atomic. Starting with version 4.0, MongoDB provides multi-document transactions and guarantees the atomicity of the transactions. Consistency MongoDB uses schema validation, a feature that allows you to define the specific structure of documents in each MongoDB collection. If the document structure deviates from the defined schema, MongoDB will return an error. This is how MongoDB enforces its version of consistency, however, it's optional and less rigid than in traditional SQL databases. Isolation MongoDB isolates write operations on a per-document level. By default, clients do not wait for acknowledgement of write operations. However, users can configure write concern to guarantee a desired level of isolation. Multi-document transactions in MongoDB are isolated across participating nodes for the duration of each transaction. Durability MongoDB allows you to specify the level of durability when writing documents. You can choose to wait until the data is written to a certain number of servers, or even to the disk. This is configurable by setting the write concern when writing data."},{"location":"mongodb/introduction/#tutorials","title":"Tutorials","text":""},{"location":"mongodb/introduction/#website","title":"Website","text":"<ul> <li>neetcode</li> <li>MongoDB Developer</li> </ul>"},{"location":"mongodb/introduction/#youtube","title":"YouTube","text":"<ul> <li>MongoDB Crash Course</li> </ul>"},{"location":"mongodb/introduction/#udemy","title":"Udemy","text":"<ul> <li>MongoDB - The Complete Developer's Guide</li> <li>MongoDB: A Complete Database Administration Course</li> </ul>"},{"location":"mongodb/operators/","title":"operators","text":""},{"location":"mongodb/operators/#complex-operators","title":"Complex Operators","text":""},{"location":"mongodb/operators/#query-and-projection-operators","title":"Query and Projection Operators","text":"<p>$eq: Matches values that are equal to a specified value. </p><pre><code>{ field: { $eq: value } }\n</code></pre> <p>$gt: Matches values that are greater than a specified value. </p><pre><code>{ field: { $gt: value } }\n</code></pre> <p>$gte: Matches values that are greater than or equal to a specified value. </p><pre><code>{ field: { $gte: value } }\n</code></pre> <p>$lt: Matches values that are less than a specified value. </p><pre><code>{ field: { $lt: value } }\n</code></pre> <p>$lte: Matches values that are less than or equal to a specified value. </p><pre><code>{ field: { $lte: value } }\n</code></pre> <p>$ne: Matches all values that are not equal to a specified value. </p><pre><code>{ field: { $ne: value } }\n</code></pre> <p>$in: Matches any value in the specified array. </p><pre><code>{ field: { $in: [value1, value2, ...] } }\n</code></pre> <p>$nin: Matches none of the values specified in an array. </p><pre><code>{ field: { $nin: [value1, value2, ...] } }\n</code></pre>"},{"location":"mongodb/operators/#logical-operators","title":"Logical Operators","text":"<p>$or: Joins query clauses with a logical OR, returns all documents that match the conditions of either clause. </p><pre><code>{ $or: [ { clause1 }, { clause2 } ] }\n</code></pre> <p>$and: Joins query clauses with a logical AND, returns all documents that match the conditions of both clauses. </p><pre><code>{ $and: [ { clause1 }, { clause2 } ] }\n</code></pre> <p>$not: Inverts the effect of a query expression and returns documents that do not match the query expression. </p><pre><code>{ field: { $not: { clause } } }\n</code></pre> <p>$nor: Joins query clauses with a logical NOR, returns all documents that fail to match both clauses. </p><pre><code>{ $nor: [ { clause1 }, { clause2 } ] }\n</code></pre>"},{"location":"mongodb/operators/#array-operators","title":"Array Operators","text":"<p>$all: Matches arrays that contain all elements specified in the query. </p><pre><code>{ field: { $all: [value1, value2, ...] } }\n</code></pre> <p>$elemMatch: Selects documents if element in the array field matches all the specified $elemMatch conditions. </p><pre><code>{ field: { $elemMatch: { clause1, clause2, ... } } }\n</code></pre> <p>$size: Selects documents if the array field is a specified size. </p><pre><code>{ field: { $size: size } }\n</code></pre>"},{"location":"mongodb/operators/#update-operators","title":"Update Operators","text":"<p>$set: Sets the value of a field in a document. </p><pre><code>{ $set: { field: value } }\n</code></pre> <p>$unset: Removes the specified field from a document. </p><pre><code>{ $unset: { field: \"\" } }\n</code></pre> <p>$inc: Increments the value of the field by the specified amount. </p><pre><code>{ $inc: { field: amount } }\n</code></pre> <p>$mul: Multiplies the value of the field by the specified amount. </p><pre><code>{ $mul: { field: amount } }\n</code></pre> <p>$push: Appends a specified value to an array. </p><pre><code>{ $push: { field: value } }\n</code></pre> <p>$pop: Removes the first or last element of an array. </p><pre><code>{ $pop: { field: 1 } } // Removes the last element\n{ $pop: { field: -1 } } // Removes the first element\n</code></pre> <p>$pull: Removes all array elements that match a specified query. </p><pre><code>{ $pull: { field: { clause } } }\n</code></pre>"},{"location":"mongodb/operators/#aggregation-operators","title":"Aggregation Operators","text":"<p>$match: Filters the documents to pass only documents that match the specified condition(s) to the next pipeline stage. </p><pre><code>{ $match: { clause } }\n</code></pre> <p>$group: Groups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. </p><pre><code>{ $group: { _id: \"$field\", total: { $sum: \"$amount\" } } }\n</code></pre> <p>$project: Passes along the documents with only the specified fields to the next stage in the pipeline. </p><pre><code>{ $project: { field1: 1, field2: 1, _id: 0 } }\n</code></pre> <p>$sort: Sorts all input documents and outputs them to the next stage in the specified sort order. </p><pre><code>{ $sort: { field: 1 } } // Ascending order\n{ $sort: { field: -1 } } // Descending order\n</code></pre> <p>$limit: Passes the first n documents unmodified to the pipeline where n is the specified limit. </p><pre><code>{ $limit: n }\n</code></pre>"},{"location":"mongodb/read/","title":"read","text":""},{"location":"mongodb/read/#introduction","title":"Introduction","text":"<p>Following are the components of mongodb reads</p> <ul> <li>Methods, Filters and Operators</li> <li>Query Selectors</li> <li>Projection Operators</li> </ul> <p>There are two methods</p> <ul> <li><code>find</code>: returns all the documents which satisfies the criteria (basically it returns the cursor object)</li> <li><code>findOne</code>: it returns a first document that satisfies the criteria</li> </ul> <p>find method gives a cursor of 20 objects</p> <p>Examples </p><pre><code>&gt; db.products.findOne({age:24}) -&gt;  to get the document where age is 24\n&gt; db.products.findOne({age:{$gt:24}}) -&gt; to get the document where age is greater than 24\n</code></pre> <p>Operators are reserved fields started with dollar like $gt, $gte, $lt, $lte</p> <p>Query Selectors</p> <ul> <li>Comparison</li> <li>Evaluation</li> <li>Logical</li> <li>Array</li> <li>Element</li> <li>Comments</li> <li>Geospatial</li> </ul> <p>Projection Operator</p> <ul> <li>$</li> <li>$elemMatch</li> <li>$meta</li> <li>$slice</li> </ul> <p>first it will search the document where name is \"Under the Dome\" then it only return name, type and language </p><pre><code>db.infos.find({\"name\": \"Under the Dome\"},{\"name\":1,\"type\":1,\"language\":1})\n</code></pre> <p>runtime equal to 60, both of them will work same </p><pre><code>db.infos.findOne({runtime:60})\n\ndb.infos.findOne({runtime:{$eq:60}})\n</code></pre> <p>runtime not equal to 60 </p><pre><code>db.infos.findOne({runtime:{$ne:60}}) \n</code></pre> <p>runtime greater than 60 </p><pre><code>db.infos.findOne({runtime:{$gt:60}})\n</code></pre> <p>runtime greater than equal to 60 </p><pre><code>db.infos.findOne({runtime:{$gte:60}})\n</code></pre> <p>runtime less than 60 </p><pre><code>db.infos.findOne({runtime:{$lt:60}})\n</code></pre> <p>runtime less than equal to 60 </p><pre><code>db.infos.findOne({runtime:{$lte:60}}) \n</code></pre> <p>it will find all the documents where runtime is either 30 or 42 </p><pre><code>db.infos.find({runtime: {$in: [30,42]}}) \n</code></pre> <p>it will find all the documents where runtime is neither 30 nor 42 </p><pre><code>db.infos.find({runtime: {$nin: [30,42]}}) \n</code></pre> <p>average is a field which is inside of rating, so to querying anything in average we can use something like this layer1.layer2.layer3.targetField then our query operator </p><pre><code>db.infos.findOne({\"rating.average\": {$gt: 9}}) \n</code></pre> <p>here genres is a array. If we search for this, it will not equate as a string it will check that genres contain Drama or not </p><pre><code>db.infos.findOne({\"genres\": \"Drama\"}) \n</code></pre> <p><code>$or</code> operator takes an array of queries. Here average is either greater than 8 or less than 7. We can combine more than two queries. </p><pre><code>db.infos.find({$or: [{\"rating.average\": {$gt: 8}}, {\"rating.average\": {$lt: 7}}]}) \n</code></pre> <p><code>$nor</code> operator takes an array of queries. Here average is neither greater than 8 nor less than 7. We can combine more than two queries. </p><pre><code>db.infos.find({$nor : [{\"rating.average\": {$gt: 8}}, {\"rating.average\": {$lt: 7}}]}) \n</code></pre> <p><code>$and</code> operator takes an array of queries. Here average is less than 8 and greater than 7. We can combine more than two queries. We have a short cut for and query. </p><pre><code>db.infos.find({$and : [{\"rating.average\": {$lt:8}},{\"rating.average\": {$gt:7}}]}) \n</code></pre> <p>these two queries are same as mongodb by default does the and operation and equal to operation </p><pre><code>db.infos.find({$and : [{\"rating.average\": {$lt:8}},{\"runtime\": {$gte:60}}]})\n\ndb.infos.find({\"rating.average\": {$lt:8}, \"runtime\": {$gte:60}})\n</code></pre> <p>we have also <code>$not</code> operator that we can use like this. <code>$not</code> is just like another wrapper to the existing query</p> <p>not of this query <code>db.infos.find({\"rating.average\": {$lt: 8}}).count()</code> will be <code>db.infos.find({\"rating.average\": {$not :{$lt: 8}}}).count()</code></p> <p>There are two element type operators <code>$exist</code> and <code>$type</code></p> <p>As mongodb is schemaless so sometimes there may be a case a field may or may not be exist so we can check that a field is exist or not like this:</p> <p>age field exists </p><pre><code>db.users.findOne({\"age\": {$exists: true}})\n</code></pre> <p>We can use exists with another query as well</p> <p>age field exists and greater than 30 </p><pre><code>db.users.findOne({\"age\": {$exists: true, $gte: 30}})\n</code></pre> <p>age field exists and not equal to null </p><pre><code>db.users.findOne({\"age\": {$exists: true, $ne: null}}) \n</code></pre> <p>As mongodb is schemaless so sometimes there may be a case a field may or may not have the same data type for all the document so we can check that a field has the datatype or not with <code>$type</code></p> <p>phone no is double in which document </p><pre><code>db.users.findOne({\"phoneNo\": {$type: \"double\"}}) \n</code></pre> <p>phone no is string in which document </p><pre><code>db.users.findOne({\"phoneNo\": {$type: \"string\"}}) \n</code></pre> <p>phone no is string or double in which document. We can use array. It will act as OR operator here </p><pre><code>db.users.findOne({\"phoneNo\": {$type: [\"double\", \"string\"]}})\n</code></pre> <p>It will use regex to search any document have the musical word in the summary or not. But it is not that efficient better to use text indexing </p><pre><code>db.infos.find({summary: {$regex: /musical/}}) \n</code></pre> <p>it will search all the documents where weight is greater that runtime. We can use $expr like this where it will take the query inside it.  </p><pre><code>db.infos.find({$expr: {$gt: [\"$weight\", \"$runtime\"]}}) \n</code></pre> <p>We can use if, then an inside $cond and the $expr will evaluate everything.</p>"},{"location":"mongodb/read/#querying-to-arrays","title":"Querying to Arrays","text":"<p>Let's say experience is an array having many fields like college name, company name, start date end date etc</p> <p>it will search the document where in experiences array there will be a object in which companyName field will be Kreeti </p><pre><code>db.products.find({\"experiences.companyName\": \"Kreeti\"}) \n</code></pre> <p>We can use dot operator with array and embedded documents</p> <p>find all the documents where experience is length of 3 </p><pre><code>db.products.find({\"experiences\": {$size: 3}}) \n</code></pre> <p><code>$size</code> operator takes only equality it will not work with <code>$gt</code> or <code>$lt</code> like the following query</p> <p>It will give us the exception. </p><pre><code>db.products.find({\"experiences\": {$size: {$gt: 2}}}) \n</code></pre> <p>It will only search for the documents where genres is <code>[\"Drama\", \"Crime\", \"Thriller\"]</code> particularly in this order but if the order does not matter for us then we can use <code>$all</code> </p><pre><code>db.infos.find({genres: [\"Drama\", \"Crime\", \"Thriller\"]}) \n</code></pre> <p>It will search for all the documents where these three items <code>[\"Drama\", \"Crime\", \"Thriller\"]</code> are there in the genres array. </p><pre><code>db.infos.find({genres: {$all: [\"Drama\", \"Crime\", \"Thriller\"]}}) \n</code></pre> <p>Certainly, these two queries will not give us the same result: </p><pre><code>db.infos.find({genres: {$all: [\"Drama\", \"Crime\"]}}).count() -&gt; 47\n\ndb.infos.find({genres: [\"Drama\", \"Crime\"]}).count() -&gt; 12\n</code></pre> <p>Find how many persons are working in TCS or not. Probable answers are : </p><pre><code>db.products.find({\"experiences.companyName\": \"TCS\",\"experiences.currentlyInHere\": true}).count()\n\ndb.products.find({$and: [{\"experiences.companyName\": \"TCS\"}, {\"experiences.currentlyInHere\": true}]}).count()\n</code></pre> <p>If we use this query ideally it should return <code>1</code> as there is only one document where in one <code>experience</code> item <code>companyName</code> is <code>TCS</code> and <code>currentlyHere</code> is <code>true</code> but this query does not work like that it will check in the <code>arrays</code> that if any object has the <code>companyName</code> as <code>TCS</code> and <code>currentlyHere</code> is <code>true</code>. It does not need to be the same object in the array. Here we could use the <code>$elemMatch</code>. It will search for all the queries in the same item of the array.</p> <p>We can achieve our requirement of any person who is currently working in TCS or not with the below query: <code>$elemMatch</code> will match all the queries for every element in the array. </p><pre><code>db.products.find({experiences: {$elemMatch: {companyName: \"TCS\",currentlyInHere: true}}}).count()\n</code></pre>"},{"location":"mongodb/read/#cursor","title":"Cursor","text":"<p>In <code>MongoDB</code>, the <code>find()</code> method return the <code>cursor</code>, now to access the document we need to iterate the <code>cursor</code>. In the <code>mongo shell</code>, if the <code>cursor</code> is not assigned to a <code>var</code> keyword then the <code>mongo shell</code> automatically iterates the <code>cursor</code> up to <code>20</code> documents. <code>MongoDB</code> also allows you to iterate cursor manually. So, to iterate a cursor manually simply assign the cursor return by the <code>find()</code> method to the <code>var</code> keyword or JavaScript variable.</p> <p>Note: If a <code>cursor</code> inactive for <code>10 min</code>, then <code>MongoDB</code> server will automatically close that cursor.</p> <p>It will fetch the <code>cursor</code> of first 20 elements. </p><pre><code>db.infos.find().pretty() \n</code></pre> <p>It will exhaust the <code>cursor</code> and make all the documents as <code>array</code> of objects </p><pre><code>db.infos.find().toArray()\n</code></pre> <p>It give us the <code>count</code> of all the element </p><pre><code>db.infos.find().count() \n</code></pre> <p>it will say if the <code>cursor</code> has exhausted or not </p><pre><code>db.infos.find().hasNext()\n</code></pre> <p>it will give the current <code>20</code> elements of the <code>cursor</code> </p><pre><code>db.infos.find().next() \n</code></pre> <p><code>printjson</code> is a method in <code>shell</code>. <code>forEach</code> is a function on the <code>cursor</code> </p><pre><code>db.infos.find().forEach((doc) =&gt; printjson(doc)) \n</code></pre> <p>It will <code>sort</code> all the elements on <code>average</code> element on rating. </p><pre><code>db.infos.find().sort({\"rating.average\" :1}) \n</code></pre> <p>It will <code>sort</code> all the elements on <code>average</code> element on rating and then runtime but backwards </p><pre><code>db.infos.find().sort({\"rating.average\" :1, \"runtime\": -1})\n</code></pre> <p>It will sort all the elements on <code>average</code> element on rating then skip the first 10 elements </p><pre><code>db.infos.find().sort({\"rating.average\" :1}).skip(10)  \n</code></pre> <p>It will sort all the elements on <code>average</code> element on rating then only show the first <code>2</code> elements </p><pre><code>db.infos.find().sort({\"rating.average\" :1}).limit(2) \n</code></pre> <p>It will sort all the elements on <code>average</code> element on rating then <code>skip 2 elements</code> and show only <code>2</code> elements </p><pre><code>db.infos.find().sort({\"rating.average\" :1}).skip(2).limit(2) \n</code></pre> <p>It will show only the <code>name</code> and the <code>_id</code> of first <code>20</code> documents. <code>_id</code> is shown by default. </p><pre><code>db.infos.find({},{name: 1}) \n</code></pre> <p>It will show only the name of first <code>20</code> documents. </p><pre><code>db.infos.find({},{_id: 0, name: 1}) \n</code></pre> <p>It will show only the <code>name</code> and <code>schedule</code> object with only <code>time</code> field and the <code>_id</code> of first <code>20</code> documents. </p><pre><code>db.infos.find({},{name: 1, \"schedule.time\": 1}) \n</code></pre> <p>It will first search for the documents with <code>genres</code> with <code>Thriller</code> then with <code>projection</code> it will show only the <code>first</code> element of genres array </p><pre><code>db.infos.find({genres: \"Thriller\"},{\"genres.$\": 1}) \n</code></pre> <p>It will first search for the documents with genres array with <code>Drama and Action</code> then with projection it will show only the <code>first</code> element of genres array </p><pre><code>db.infos.find({genres: {$all : [\"Drama\",\"Action\"]}},{\"genres.$\": 1}) \n</code></pre> <p>Here Querying and projecting works independently. First it will search for genres array with <code>Drama and Action</code> then with <code>projection</code> it will show only the array with <code>Horror</code> present or not. </p><pre><code>db.infos.find({genres: {$all : [\"Drama\",\"Action\"]}},{\"genres\" : {$elemMatch: {$eq: \"Horror\"}}}) \n</code></pre> <p><code>$slice</code> only works array while projection. <code>{$slice: 2}</code> will slice the first <code>2</code> elements of the array. </p><pre><code>db.infos.find({}, {genres: {$slice: 2}, name: 1}) \n</code></pre> <p><code>{$slice: [1,3]}</code> will slice the <code>1st</code> to <code>3rd</code> elements of the array. </p><pre><code>db.infos.find({},{genres: {$slice: [1,3]},name: 1}) \n</code></pre>"},{"location":"mongodb/read/#more-examples","title":"More Examples","text":"<p>Find one document: </p><pre><code>db.coll.findOne()\n</code></pre> <p>Find all documents (returns a cursor - show 20 results - \"it\" to display more): </p><pre><code>db.coll.find()\n</code></pre> <p>Find all documents and pretty print: </p><pre><code>db.coll.find().pretty()\n</code></pre> <p>Find documents with specific criteria (implicit logical \"AND\"): </p><pre><code>db.coll.find({name: \"Max\", age: 32})\n</code></pre> <p>Find documents with a specific date: </p><pre><code>db.coll.find({date: ISODate(\"2020-09-25T13:57:17.180Z\")})\n</code></pre> <p>Find documents with specific criteria and explain execution stats: </p><pre><code>db.coll.find({name: \"Max\", age: 32}).explain(\"executionStats\")\n</code></pre> <p>Find distinct values for a field: </p><pre><code>db.coll.distinct(\"name\")\n</code></pre> <p>Count documents with specific criteria (accurate count): </p><pre><code>db.coll.countDocuments({age: 32})\n</code></pre> <p>Estimate document count based on collection metadata: </p><pre><code>db.coll.estimatedDocumentCount()\n</code></pre> <p>Find documents with comparison operators: </p><pre><code>db.coll.find({\"year\": {$gt: 1970}})\ndb.coll.find({\"year\": {$gte: 1970}})\ndb.coll.find({\"year\": {$lt: 1970}})\ndb.coll.find({\"year\": {$lte: 1970}})\ndb.coll.find({\"year\": {$ne: 1970}})\ndb.coll.find({\"year\": {$in: [1958, 1959]}})\ndb.coll.find({\"year\": {$nin: [1958, 1959]}})\n</code></pre> <p>Find documents with logical operators: </p><pre><code>db.coll.find({name: {$not: {$eq: \"Max\"}}})\ndb.coll.find({$or: [{\"year\": 1958}, {\"year\": 1959}]})\ndb.coll.find({$nor: [{price: 1.99}, {sale: true}]})\ndb.coll.find({\n  $and: [\n    {$or: [{qty: {$lt :10}}, {qty :{$gt: 50}}]},\n    {$or: [{sale: true}, {price: {$lt: 5 }}]}\n  ]\n})\n</code></pre> <p>Find documents with element operators: </p><pre><code>db.coll.find({name: {$exists: true}})\ndb.coll.find({\"zipCode\": {$type: 2 }})\ndb.coll.find({\"zipCode\": {$type: \"string\"}})\n</code></pre> <p>Aggregation Pipeline: </p><pre><code>db.coll.aggregate([\n  {$match: {status: \"A\"}},\n  {$group: {_id: \"$cust_id\", total: {$sum: \"$amount\"}}},\n  {$sort: {total: -1}}\n])\n</code></pre> <p>Text search with a \"text\" index: </p><pre><code>db.coll.find({$text: {$search: \"cake\"}}, {score: {$meta: \"textScore\"}}).sort({score: {$meta: \"textScore\"}})\n</code></pre> <p>Find documents with regex: </p><pre><code>db.coll.find({name: /^Max/})   // regex: starts by letter \"M\"\ndb.coll.find({name: /^Max$/i}) // regex case insensitive\n</code></pre> <p>Find documents with array operators: </p><pre><code>db.coll.find({tags: {$all: [\"Realm\", \"Charts\"]}})\ndb.coll.find({field: {$size: 2}}) // impossible to index - prefer storing the size of the array &amp; update it\ndb.coll.find({results: {$elemMatch: {product: \"xyz\", score: {$gte: 8}}}})\n</code></pre> <p>Projections: </p><pre><code>db.coll.find({\"x\": 1}, {\"actors\": 1})               // actors + _id\ndb.coll.find({\"x\": 1}, {\"actors\": 1, \"_id\": 0})     // actors\ndb.coll.find({\"x\": 1}, {\"actors\": 0, \"summary\": 0}) // all but \"actors\" and \"summary\"\n</code></pre> <p>Sort, skip, limit: </p><pre><code>db.coll.find({}).sort({\"year\": 1, \"rating\": -1}).skip(10).limit(3)\n</code></pre> <p>Read Concern: </p><pre><code>db.coll.find().readConcern(\"majority\")\n</code></pre> <p>Since shell is made of JS so we can use JS function For reference: Cursor Methods</p>"},{"location":"mongodb/update/","title":"update","text":""},{"location":"mongodb/update/#introduction","title":"Introduction","text":"<p>In the Users db Info collection all the documents in the following type</p> <p></p><pre><code>{\n  \"_id\": {\n    \"$oid\": \"62ac4ff719cc703713ba43c0\"\n  },\n  \"name\": \"Max\",\n  \"hobbies\": [\n    {\n      \"title\": \"Sports\",\n      \"frequency\": 3\n    },\n    {\n      \"title\": \"Cooking\",\n      \"frequency\": 6\n    }\n  ],\n  \"phone\": 131782734\n}\n</code></pre> But the object with name chris has the different type of <code>hobbies</code> array <p>So, we need to update the <code>hobbies</code>. We have two methods for update any object <code>updateOne</code> and <code>updateMany</code>. The names are self-explanatory.</p> <p>The <code>update</code> method takes two mandatory input one is <code>filter</code> for search and <code>what to update</code>.</p> <p><code>$set</code> keyword is used to set the change the field value. Other fields will be untouched.</p> <pre><code>&gt; db.infos.updateOne(\n    {\"name\" : \"Chris\"},\n    { $set: \n        {\"hobbies\": [\n            {title:\"Sports\",frequency:5},\n            {title:\"Cooking\",frequency:3},\n            {title:\"Hiking\",frequency:1}\n        ]}\n    }\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 } \n\n## If I again run the same query then the modifiedCount will be 0\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 }\n</code></pre> <p>We can update multiple documents at the same time </p><pre><code>&gt; db.infos.updateMany(\n    {\"hobbies.title\": \"Sports\"},\n    {$set: {isSporty: true}}\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 3, \"modifiedCount\" : 3 }\n</code></pre> With <code>$set</code> operator we can change more than one field at a time as well. <p>We also have incrementor or decrement operator as these two are very common operation. </p><pre><code>&gt; db.infos.updateOne(\n    {name: \"Manuel\"}, \n    { $inc : { age : 1 }}\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n</code></pre> <p>With <code>$inc</code> we can also decrement. </p><pre><code>&gt; db.infos.updateOne(\n    { name: \"Manuel\" } , \n    { \n        $inc : { age :  -1 } , \n        $set : { isSporty : false }\n    }\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n</code></pre> We can operate on different field on the same time, but we cannot set age and increment age at the same time. <p>We have other 3 operators like <code>$inc</code>. those are <code>$min</code>, <code>$max</code>, <code>$mul</code>.</p> <p>The field age will be updated as the max value between what we have passed and what is the previous value. If the previous value is 20 then the new value of age will be 31 but if its already 35 then the value will not be changed. </p><pre><code>db.infos.updateOne(\n    { name : \"Manual\" },\n    { $max :{ age : 31 }}\n)\n</code></pre> <p><code>$min</code> and <code>$max</code> is quite similar operation just that one is taking minimum value and another is taking maximum value. The <code>$mul</code> operator will multiply the field with the value that we have passed. If the previous value of age is 30 then the new value will be 30*1.1 = 33 </p><pre><code>db.infos.updateOne(\n    { name : \"Manual\" },\n    { $mul :{ age : 1.1 }}\n)\n</code></pre> <p>We can also drop any field with <code>$unset</code> operator. With this command we can remove the phone field. The value of phone here will be ignored. We can assign any value. </p><pre><code>db.infos.updateOne(\n    { name : \"Manuel\" },\n    { $unset :{ phone : \"\" }}\n)\n</code></pre> <p>We also can rename the field using the <code>$rename</code> operator. The field age will now be converted to <code>totalAge</code> </p><pre><code>db.infos.updateMany(\n    {},\n    { $rename :{ age : \"totalAge\" }}\n)\n\n{ \"acknowledged\" : true, \"matchedCount\" : 4, \"modifiedCount\" : 2 }\n</code></pre> <p>We can do <code>update</code> and <code>insert</code> operation at the same time, and it is called <code>upsert</code>.  Suppose we don\u2019t know we have a document with name as <code>Abhishek</code> or not and we also want to change its value if it is there.  So, we can use <code>upsert</code> here. So, to use <code>upsert</code> we must pass the <code>upsert</code> value in the last parameter.  By default, its value is true. </p><pre><code>&gt; db.infos.updateOne(\n    { name : \"Abhishek\" }, \n    { $set : \n        { \n            \"hobbies\": [\n            { \"title\": \"Sports\", \"frequency\": 3 },\n            { \"title\": \"Cooking\", \"frequency\": 6 }], \n            \"phone\": 131782734, \n            \"isSporty\": true \n        }\n    } , \n    { upsert : true }\n)\n\n\n{\n        \"acknowledged\" : true,\n        \"matchedCount\" : 0,\n        \"modifiedCount\" : 0,\n        \"upsertedId\" : ObjectId(\"62da1c2f60336bad54ef7227\")\n}\n</code></pre> MongoDB is smart enough to determine that if we are querying with equality operator then the name value must be there.  So, in the new object name, hobbies, phone, isSporty all these values will be present. <p>Array update operations:</p> <p>Suppose we have to find the documents where <code>hobbies</code> array has <code>title</code> value of <code>Sports</code> and <code>frequency</code> value greater than 3.  Then the query will be like. </p><pre><code>db.infos.find({ \n    hobbies : { \n        $elemMatch : { \n            title : \"Sports\" , \n            \"frequency\" : { $gte : 3}\n            }\n        }\n    }\n)\n{ \n    \"_id\" : ObjectId(\"62ac4ff719cc703713ba43be\"), \n    \"name\" : \"Chris\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 5 }, \n        { \"title\" : \"Cooking\", \"frequency\" : 3 }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 } \n    ], \n    \"isSporty\" : true \n}\n{ \n    \"_id\" : ObjectId(\"62ac4ff719cc703713ba43c0\"), \n    \"name\" : \"Max\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3 }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6 } \n    ], \n    \"phone\" : 131782734,\n    \"isSporty\" : true \n}\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3 }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6 } \n    ],\n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre> <p>Suppose we want to update the inner array document that we have found and highlighted above. </p><pre><code>&gt; db.infos.updateMany({ \n    hobbies : {\n        $elemMatch : { \n            title : \"Sports\" , \n            \"frequency\" : { $gte : 3}\n        }\n    }}, \n    { $set : { \n        \"hobbies.$.highFrequency\" : true \n    }}\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 3, \"modifiedCount\" : 3 }\n</code></pre> This <code>$</code> represents the same element.  Here we are adding new field that is why we are using <code>\"hobbies.$.highFrequency\"</code> but if we want to override that document then we can simply do this <code>\"hobbies.$\" : {\"title\" : \"Sports\", \"frequency\" : 5}</code> <p>Suppose we want to update all the documents of the array. So, we can use $[] operator it means all the array documents. <code>goodHobby</code> field added for all the inner array documents. </p><pre><code>&gt; db.infos.updateMany({ \n    hobbies : { \n        $elemMatch : { title : \"Sports\" , \"frequency\" : { $gte : 3}}\n    }}, \n    { $set : { \"hobbies.$[].goodHobby\" : true }}\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 3, \"modifiedCount\" : 3 }\n&gt; db.infos.find({ \n        hobbies : { \n            $elemMatch : { title : \"Sports\" , \"frequency\" : { $gte : 3}}\n        }}\n    ).pretty()\n{\n        \"_id\" : ObjectId(\"62ac4ff719cc703713ba43be\"),\n        \"name\" : \"Chris\",\n        \"hobbies\" : [\n                {\n                        \"title\" : \"Sports\",\n                        \"frequency\" : 5,\n                        \"highFrequency\" : true,\n                        \"goodHobby\" : true\n                },\n                {\n                        \"title\" : \"Cooking\",\n                        \"frequency\" : 3,\n                        \"goodHobby\" : true\n                },\n                {\n                        \"title\" : \"Hiking\",\n                        \"frequency\" : 1,\n                        \"goodHobby\" : true\n                }\n        ],\n        \"isSporty\" : true\n}\n</code></pre> <p>Let's say if we have a criterion to upadate only some certain documents then we can <code>$[ el ]</code> and later we will define the <code>el</code> condition in the thir parameter <code>arrayFilers</code> part. In array filter we can pass as many conditions as we want. </p><pre><code>&gt; db.infos.updateOne(\n    { name: \"Abhishek\"}, \n    { $set : { \"hobbies.$[el].goodFrequency\" : true}} , \n    { arrayFilters : [{ \"el.frequency\" :{ $gte : 3}} ]} \n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre> <p>We can also add new element in our array. With <code>$push</code> we can add new element to the existing array. </p><pre><code>&gt; db.infos.updateOne(\n    { name: \"Abhishek\"}, \n    { $push : \n        { hobbies : { title: \"Hiking\" , frequency : 1}}\n    } \n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre> <p>We can also add more than one documents with <code>$each</code> operator. </p><pre><code>&gt; db.infos.updateOne(\n    { name: \"Abhishek\"}, \n    { $push : { \n        hobbies : { \n            $each : [\n                { title: \"Hiking\" , frequency : 1},\n                { title : \"wine\", frequecy: 1}\n            ]\n        }}\n    }\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 }, \n        { \"title\" : \"wine\", \"frequecy\" : 1 } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre> We can also add <code>sor</code>t or <code>slice</code> operator to <code>add</code> the element in sorted order or we can also take only one element. <p>But there is issue with <code>$push</code> operator.  If the values are already existing, then also it will add the value.  We can use <code>$addToSet</code> operator for to add <code>unique</code> element in the array. </p><pre><code>&gt; db.infos.updateOne(\n    { name: \"Abhishek\"}, \n    { $addToSet : { \n        hobbies : { \n            $each : [\n                { title: \"Hiking\" , frequency : 1},\n                {title : \"wine\", frequecy: 1}\n            ]\n        }}\n    }\n)\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 0 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 }, \n        { \"title\" : \"Hiking\", \"frequency\" : 1 }, \n        { \"title\" : \"wine\", \"frequecy\" : 1 } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n\nWe can also pull the element from an array with `$pull` operator.\n```js\n&gt; db.infos.updateOne(\n        { name: \"Abhishek\"},\n        { $pull : { hobbies : { title : \"Hiking\" }}}\n    )\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"wine\", \"frequecy\" : 1 } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre> <code>{ $pull : { hobbies : { title : \"Hiking\" }}}</code> it means pull from the hobbies array where title is Hiking. We can also add other queries. <p>If we want to remove the last element from the array, then we can use <code>$pop</code> operator with value of 1 and if we want to remove the first element then we can assign the value with -1. </p><pre><code>&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"wine\", \"frequecy\" : 1 } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n\n&gt; db.infos.updateOne(\n        { name: \"Abhishek\"}, \n        { $pop : { hobbies : 1}}\n    )\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Sports\", \"frequency\" : 3, \"highFrequency\" : true, \"goodHobby\" : true, \"goodFrequency\" : true }, \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n\n&gt; db.infos.updateOne(\n        { name: \"Abhishek\"}, \n        { $pop : { hobbies : -1}}\n    )\n{ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }\n\n&gt; db.infos.find({name:\"Abhishek\"})\n{ \n    \"_id\" : ObjectId(\"62da1c2f60336bad54ef7227\"), \n    \"name\" : \"Abhishek\", \n    \"hobbies\" : [ \n        { \"title\" : \"Cooking\", \"frequency\" : 6, \"goodHobby\" : true, \"goodFrequency\" : true } \n    ], \n    \"isSporty\" : true, \n    \"phone\" : 131782734 \n}\n</code></pre>"},{"location":"mongodb/update/#more-examples","title":"More examples","text":""},{"location":"mongodb/update/#basic-updates","title":"Basic Updates","text":"<pre><code>db.coll.updateOne({\"_id\": 1}, {$set: {\"year\": 2016, name: \"Max\"}})\ndb.coll.updateOne({\"_id\": 1}, {$unset: {\"year\": 1}})\ndb.coll.updateOne({\"_id\": 1}, {$rename: {\"year\": \"date\"} })\ndb.coll.updateOne({\"_id\": 1}, {$inc: {\"year\": 5}})\ndb.coll.updateOne({\"_id\": 1}, {$mul: {price: NumberDecimal(\"1.25\"), qty: 2}})\ndb.coll.updateOne({\"_id\": 1}, {$min: {\"imdb\": 5}})\ndb.coll.updateOne({\"_id\": 1}, {$max: {\"imdb\": 8}})\ndb.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": true}})\ndb.coll.updateOne({\"_id\": 1}, {$currentDate: {\"lastModified\": {$type: \"timestamp\"}}})\n</code></pre>"},{"location":"mongodb/update/#array-updates","title":"Array Updates","text":"<pre><code>db.coll.updateOne({\"_id\": 1}, {$push :{\"array\": 1}})\ndb.coll.updateOne({\"_id\": 1}, {$pull :{\"array\": 1}})\ndb.coll.updateOne({\"_id\": 1}, {$addToSet :{\"array\": 2}})\ndb.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": 1}})  // last element\ndb.coll.updateOne({\"_id\": 1}, {$pop: {\"array\": -1}}) // first element\ndb.coll.updateOne({\"_id\": 1}, {$pullAll: {\"array\" :[3, 4, 5]}})\ndb.coll.updateOne({\"_id\": 1}, {$push: {\"scores\": {$each: [90, 92]}}})\ndb.coll.updateOne({\"_id\": 2}, {$push: {\"scores\": {$each: [40, 60], $sort: 1}}}) // array sorted\ndb.coll.updateOne({\"_id\": 1, \"grades\": 80}, {$set: {\"grades.$\": 82}})\ndb.coll.updateMany({}, {$inc: {\"grades.$[]\": 10}})\ndb.coll.updateMany({}, {$set: {\"grades.$[element]\": 100}}, {multi: true, arrayFilters: [{\"element\": {$gte: 100}}]})\n</code></pre>"},{"location":"mongodb/update/#findoneandupdate","title":"FindOneAndUpdate","text":"<pre><code>db.coll.findOneAndUpdate({\"name\": \"Max\"}, {$inc: {\"points\": 5}}, {returnNewDocument: true})\n</code></pre>"},{"location":"mongodb/update/#upsert","title":"Upsert","text":"<pre><code>db.coll.updateOne({\"_id\": 1}, {$set: {item: \"apple\"}, $setOnInsert: {defaultQty: 100}}, {upsert: true})\n</code></pre>"},{"location":"mongodb/update/#replace","title":"Replace","text":"<pre><code>db.coll.replaceOne({\"name\": \"Max\"}, {\"firstname\": \"Maxime\", \"surname\": \"Beugnet\"})\n</code></pre>"},{"location":"mongodb/update/#write-concern","title":"Write Concern","text":"<pre><code>db.coll.updateMany({}, {$set: {\"x\": 1}}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\n</code></pre>"},{"location":"neo4j/links/","title":"links","text":""},{"location":"neo4j/links/#neo4j-basics","title":"neo4j basics","text":""},{"location":"neo4j/links/#website","title":"Website","text":""},{"location":"neo4j/links/#official-documentation","title":"Official documentation","text":"<ul> <li>Querying with Cypher</li> <li>Core concepts</li> </ul>"},{"location":"neo4j/links/#neo4j-migrations","title":"neo4j migrations","text":"<ul> <li>official documentation</li> <li>Migrating Neo4J graph schemas in managed Kubernetes</li> </ul>"},{"location":"neo4j/links/#youtube","title":"Youtube","text":"<ul> <li>Intro to Graph Databases Series</li> <li>Neo4j Course for Beginners</li> </ul>"},{"location":"nosql/","title":"NoSQL","text":""},{"location":"nosql/#nosql","title":"NoSQL","text":""},{"location":"nosql/#youtube","title":"Youtube","text":""},{"location":"nosql/#short-videos","title":"Short Videos","text":"<ul> <li>Why does NoSQL exist? (MongoDB, Cassandra) | System Design</li> </ul>"},{"location":"postgres/links/","title":"links","text":""},{"location":"postgres/links/#postgres-basic","title":"POSTGRES basic","text":""},{"location":"postgres/links/#website","title":"Website","text":"<ul> <li>How to Deploy Postgres on Kubernetes for a Scalable Web Application</li> <li>Kubernetes - Configure PostgreSQL Streaming Replication</li> </ul>"},{"location":"postgres/links/#youtube","title":"Youtube","text":"<ul> <li>I replaced my entire tech stack with Postgres...</li> <li>5 Secrets for making PostgreSQL run BLAZING FAST. How to improve database performance.</li> <li>Solving one of PostgreSQL's biggest weaknesses.</li> <li>Wait... PostgreSQL can do WHAT?</li> <li>Learn PostgreSQL Tutorial - Full Course for Beginners</li> <li>PostgresSQL</li> <li> <p>High-Performance Programming</p> <ul> <li>PostgreSQL High-Availability</li> <li>PostgreSQL Perfomance Tuning</li> <li>MySQL Performance Tuning</li> </ul> </li> <li> <p>Why Postgres Is So Popular</p> </li> <li>Postgres Architecture Explained</li> <li>PostgreSQL Perfomance Tuning | High-Performance Programming</li> <li>PostgreSQL High-Availability | High-Performance Programming</li> </ul>"},{"location":"postgres/links/#udemy","title":"Udemy","text":"<ul> <li>SQL and PostgreSQL: The Complete Developer's Guide</li> <li>PostgreSQL Bootcamp : Go From Beginner to Advanced, 60+hours</li> </ul>"},{"location":"queue-system/links/","title":"Queue System","text":""},{"location":"queue-system/links/#queue-system","title":"Queue System","text":""},{"location":"queue-system/links/#youtube","title":"Youtube","text":"<ul> <li>The Best Queue System for Scaling - Not Kafka!</li> <li>What is a Message Queue and When should you use Messaging Queue Systems Like RabbitMQ and Kafka</li> <li>When to Use Kafka or RabbitMQ | System Design</li> <li>Kafka vs. RabbitMQ - who wins and why? | Systems Design Interview 0 to 1 with Ex-Google SWE</li> </ul>"},{"location":"rabbitMQ/links/","title":"RabbitMQ","text":""},{"location":"rabbitMQ/links/#rabbitmq","title":"RabbitMQ","text":""},{"location":"rabbitMQ/links/#blogs","title":"Blogs","text":"<ul> <li>RabbitMQ: A Complete Guide to Message Broker, Performance, and Reliability</li> </ul>"},{"location":"redis/introduction/","title":"introduction","text":""},{"location":"redis/introduction/#redis","title":"Redis","text":"<p>Redis (Remote Dictionary Server) is an open source, in-memory, NoSQL key/value store that is used primarily as an application cache or quick-response database</p>"},{"location":"redis/introduction/#redis-datatypes","title":"Redis datatypes","text":""},{"location":"redis/links/","title":"links","text":""},{"location":"redis/links/#redis-basics","title":"Redis basics","text":""},{"location":"redis/links/#resources","title":"Resources","text":"<ul> <li>introduction</li> </ul>"},{"location":"redis/links/#youtube","title":"Youtube","text":""},{"location":"redis/links/#introduction","title":"Introduction","text":"<ul> <li>System Design: Why is single-threaded Redis so fast?</li> <li>Top 5 Redis Use Cases</li> <li>Can Redis be used as a Primary database?</li> <li>Redis Crash Course - the What, Why and How to use Redis as your primary database</li> <li>I've been using Redis wrong this whole time...</li> <li>Stop Using Redis. Use Open Source Instead Valkey</li> </ul>"},{"location":"redis/links/#usecase","title":"Usecase","text":"<ul> <li>When to Use Redis as a Primary Database - Redis Special Topics (1/4) | System Design</li> <li>Using Redis Streams instead of Kafka - Redis Special Topics (2/4) | System Design</li> <li>Redis - In Practice | Distributed Systems Deep Dives With Ex-Google SWE</li> </ul>"},{"location":"redis/links/#course","title":"Course","text":"<ul> <li>Redis Crash Course</li> <li>Learn Redis in ONE video</li> <li>Redis Enterprise Cloud</li> </ul>"},{"location":"redis/links/#playlist","title":"Playlist","text":"<ul> <li>redis personal</li> </ul>"},{"location":"redis/links/#udemy","title":"Udemy","text":"<ul> <li>Modern Redis Unleashed</li> <li>Redis Beginners to Advance With Free Lab</li> <li>Redis: The Complete Developer's Guide</li> </ul>"},{"location":"sql/leetcode/","title":"leetcode","text":""},{"location":"sql/leetcode/#resources","title":"Resources","text":"<p>Leetcode 50 sql problem link</p> <p>Crack SQL Interview in 50 Qs link</p>"},{"location":"sql/leetcode/#questions","title":"Questions","text":""},{"location":"sql/leetcode/#select","title":"Select","text":"<ul> <li>Recyclable and Low Fat Products</li> <li>Find Customer Referee</li> <li>Big Countries</li> <li>Article Views I</li> <li>Invalid Tweets</li> </ul>"},{"location":"sql/leetcode/#basic-joins","title":"Basic Joins","text":"<ul> <li>Replace Employee ID With The Unique Identifier</li> </ul>"},{"location":"sql/links/","title":"sql-helper","text":""},{"location":"sqlite/links/","title":"Sqlite","text":""},{"location":"sqlite/links/#sqlite","title":"Sqlite","text":""},{"location":"sqlite/links/#youtube","title":"Youtube","text":"<ul> <li> <p>SQLite first impressions</p> </li> <li> <p>The Perfect Dependency - SQLite Case Study</p> </li> </ul>"},{"location":"surrielDB/links/","title":"links","text":""},{"location":"surrielDB/links/#surrieldb-basics","title":"SurrielDB basics","text":""},{"location":"surrielDB/links/#youtube","title":"Youtube","text":""},{"location":"surrielDB/links/#introduction","title":"Introduction","text":"<ul> <li>SurrealDB in 100 Seconds</li> <li>Beyond Surreal? A closer look at NewSQL Relational Data</li> <li>Rust Powered Database SurrealDB (It's Pretty Ambitious)</li> <li>SurrealDB - Rust Embedded Database - Quick Tutorial</li> </ul>"}]}